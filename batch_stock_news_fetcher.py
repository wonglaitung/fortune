#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»è„šæœ¬
ä½œè€…ï¼šAI åŠ©æ‰‹
æ—¥æœŸï¼š2025-10-25
æ›´æ–°ï¼šé›†æˆæƒ…æ„Ÿåˆ†æåŠŸèƒ½
"""

import yfinance as yf
from datetime import datetime, timedelta
import os
import csv
import time
import argparse
import schedule
import pandas as pd

# å¯¼å…¥hk_smart_money_tracker.pyä¸­çš„WATCHLIST
import sys
sys.path.append('/data/fortune')
from hk_smart_money_tracker import WATCHLIST

# å¯¼å…¥æƒ…æ„Ÿåˆ†ææ¨¡å—
from llm_services.sentiment_analyzer import batch_analyze_sentiment, get_sentiment_statistics



def get_stock_news(symbol, stock_name="", size=3):
    """
    é€šè¿‡yfinanceè·å–ä¸ªè‚¡æ–°é—»ï¼Œåªè¿”å›ä¸€ä¸ªæœˆå†…çš„æ–°é—»
    :param symbol: è‚¡ç¥¨ä»£ç  (ä¾‹å¦‚: "0700.HK" for è…¾è®¯æ§è‚¡)
    :param stock_name: è‚¡ç¥¨åç§° (ä¾‹å¦‚: "è…¾è®¯æ§è‚¡")
    :param size: è·å–æ–°é—»æ¡æ•°
    :return: æ–°é—»åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨yfinanceè·å–ä¸ªè‚¡æ–°é—»
        ticker = yf.Ticker(symbol)
        news_data = ticker.news
        
        if not news_data:
            return []
        
        articles = []
        
        # è®¡ç®—ä¸€ä¸ªæœˆå‰çš„æ—¥æœŸ
        one_month_ago = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None) - timedelta(days=30)
        
        # å¤„ç†æ–°é—»æ•°æ®
        for item in news_data:
            # ä»contentå­—æ®µè·å–æ–°é—»æ•°æ®
            content = item.get("content", {})
            
            # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
            pub_time_str = content.get("pubDate", "")
            pub_datetime = None
            pub_time = pub_time_str  # é»˜è®¤ä½¿ç”¨åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
            if pub_time_str:
                try:
                    # å°†ISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                    pub_datetime = datetime.fromisoformat(pub_time_str.replace('Z', '+00:00'))
                    # ç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥é¿å…æ¯”è¾ƒé”™è¯¯
                    pub_datetime = pub_datetime.replace(tzinfo=None)
                    pub_time = pub_datetime.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒä½¿ç”¨åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
                    pass
            
            # åªè·å–ä¸€ä¸ªæœˆå†…çš„æ–°é—»
            if pub_datetime and pub_datetime < one_month_ago:
                continue
            
            title = content.get("title", "").strip()
            summary = content.get("summary", "").strip()
            
            # è·å–æ–°é—»é“¾æ¥
            url = ""
            canonical_url = content.get("canonicalUrl", {})
            click_through_url = content.get("clickThroughUrl", {})
            
            if isinstance(canonical_url, dict):
                url = canonical_url.get("url", "")
            elif isinstance(click_through_url, dict):
                url = click_through_url.get("url", "")
            
            articles.append({
                "title": title[:80] + ("..." if len(title) > 80 else ""),
                "summary": summary[:120] + ("..." if len(summary) > 120 else ""),
                "url": url,
                "publishedAt": pub_time
            })
        
        # æŒ‰æ—¶é—´ç”±è¿‘åˆ°è¿œæ’åºï¼Œç„¶åè¿”å›æŒ‡å®šæ•°é‡çš„æ–°é—»
        sorted_articles = sorted(articles, key=lambda x: x['publishedAt'], reverse=True)
        return sorted_articles[:size]
    except Exception as e:
        print(f"âš ï¸ è·å–ä¸ªè‚¡æ–°é—»å¤±è´¥: {e}")
        return []



def fetch_all_stock_news(analyze_sentiment=True):
    """
    è·å–watch listä¸­æ‰€æœ‰è‚¡ç¥¨çš„æ–°é—»
    
    Args:
        analyze_sentiment (bool): æ˜¯å¦æ‰§è¡Œæƒ…æ„Ÿåˆ†æï¼ˆé»˜è®¤Trueï¼‰
    """
    print("=" * 60)
    print("ğŸ“ˆ æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»")
    print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # è·å–å½“å‰æŸ¥è¯¢æ—¶é—´
    query_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    all_news_data = []
    
    # åˆ é™¤è¶…æ—¶é™åˆ¶ï¼Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨
    stock_count = 0
    
    for code, name in WATCHLIST.items():
        print(f"\nğŸ” æ­£åœ¨è·å– {name} ({code}) çš„æ–°é—»...")
        
        # ä¸ºyfinanceä½¿ç”¨å®Œæ•´çš„è‚¡ç¥¨ä»£ç ï¼ˆåŒ…å«åç¼€å¦‚.HKï¼‰
        symbol_code = code
        
        # è·å–æ–°é—»ï¼Œæ¯åªè‚¡ç¥¨è·å–3æ¡æ–°é—»
        articles = get_stock_news(symbol_code, name, size=3)
        
        if articles:
            print(f"  âœ… è·å–åˆ° {len(articles)} æ¡ç›¸å…³æ–°é—»")
            # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
            for article in articles:
                all_news_data.append({
                    "stock_name": name,
                    "stock_code": code,
                    "publishedAt": article.get("publishedAt", ""),
                    "title": article.get("title", ""),
                    "summary": article.get("summary", ""),
                    "query_time": query_time
                })
        else:
            print(f"  âš ï¸ æœªè·å–åˆ° {name} çš„æ–°é—»")
            
        stock_count += 1
        # çŸ­æš‚ä¼‘çœ ä»¥é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(1)
    
    # ä¿å­˜æ‰€æœ‰æ–°é—»æ•°æ®åˆ°CSVæ–‡ä»¶
    if all_news_data:
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        data_dir = "data"
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
        
        # CSVæ–‡ä»¶è·¯å¾„
        csv_file = os.path.join(data_dir, "all_stock_news_records.csv")
        
        # ä½¿ç”¨pandasä¿å­˜æ•°æ®ï¼ˆæ”¯æŒæƒ…æ„Ÿåˆ†æåˆ—ï¼‰
        df = pd.DataFrame(all_news_data)
        df.columns = ["è‚¡ç¥¨åç§°", "è‚¡ç¥¨ä»£ç ", "æ–°é—»æ—¶é—´", "æ–°é—»æ ‡é¢˜", "ç®€è¦å†…å®¹", "æŸ¥è¯¢æ—¶é—´"]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨æ—§æ•°æ®ï¼Œå¦‚æœå­˜åœ¨åˆ™åˆå¹¶
        if os.path.exists(csv_file):
            try:
                old_df = pd.read_csv(csv_file)
                # åˆå¹¶æ–°æ—§æ•°æ®
                merged_df = pd.concat([old_df, df], ignore_index=True)
                
                # å»é‡é€»è¾‘ï¼šä¿ç•™æœ‰æƒ…æ„Ÿåˆ†æ•°çš„è®°å½•
                # å¦‚æœæ–°æ—§æ•°æ®ä¸­æœ‰ç›¸åŒæ–°é—»ï¼Œä¼˜å…ˆä¿ç•™å·²æœ‰æƒ…æ„Ÿåˆ†æ•°çš„è®°å½•
                def keep_best_record(group):
                    # æŒ‰æƒ…æ„Ÿåˆ†æ•°æ˜¯å¦ä¸ºç©ºæ’åºï¼Œæœ‰æƒ…æ„Ÿåˆ†æ•°çš„ä¼˜å…ˆ
                    group = group.sort_values(
                        by=['æƒ…æ„Ÿåˆ†æ•°'],
                        na_position='last'  # æƒ…æ„Ÿåˆ†æ•°ä¸ºç©ºçš„æ’åœ¨æœ€å
                    )
                    # è¿”å›ç¬¬ä¸€æ¡ï¼ˆæœ‰æƒ…æ„Ÿåˆ†æ•°çš„ï¼‰
                    return group.iloc[[0]]
                
                # æŒ‰è‚¡ç¥¨ä»£ç ã€æ–°é—»æ—¶é—´ã€æ–°é—»æ ‡é¢˜åˆ†ç»„ï¼Œæ¯ç»„ä¿ç•™æœ€å¥½çš„è®°å½•
                merged_df = merged_df.groupby(
                    ['è‚¡ç¥¨ä»£ç ', 'æ–°é—»æ—¶é—´', 'æ–°é—»æ ‡é¢˜'],
                    as_index=False
                ).apply(keep_best_record).reset_index(drop=True)
                
                # æŒ‰æ—¶é—´æ’åº
                merged_df['æ–°é—»æ—¶é—´'] = pd.to_datetime(merged_df['æ–°é—»æ—¶é—´'])
                merged_df = merged_df.sort_values('æ–°é—»æ—¶é—´', ascending=False)
                df = merged_df
            except Exception as e:
                print(f"âš ï¸ åˆå¹¶æ—§æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨æ–°æ•°æ®")
        
        # ä¿å­˜æ•°æ®
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        print(f"\nâœ… æ‰€æœ‰æ–°é—»æ•°æ®å·²ä¿å­˜åˆ° {csv_file}")
        
        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        print("\nğŸ“‹ æ–°é—»æ±‡æ€»:")
        for news in all_news_data:
            print(f"  â€¢ {news['stock_name']} ({news['stock_code']}) | {news['title']}")
        
        # æ‰§è¡Œæƒ…æ„Ÿåˆ†æ
        if analyze_sentiment:
            print("\nğŸ¤– å¼€å§‹æ‰§è¡Œæƒ…æ„Ÿåˆ†æ...")
            try:
                # åªåˆ†ææœ€è¿‘3å¤©çš„æ–°é—»
                df = batch_analyze_sentiment(df, days_limit=3)
                
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                stats = get_sentiment_statistics(df)
                print(f"\nğŸ“Š æƒ…æ„Ÿåˆ†æç»Ÿè®¡:")
                print(f"  æ€»æ–°é—»æ•°: {stats['total']}")
                print(f"  å·²åˆ†æ: {stats['analyzed']}")
                print(f"  æœªåˆ†æ: {stats['unanalyzed']}")
                if stats['analyzed'] > 0:
                    print(f"  å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {stats['sentiment_score_mean']:.2f}")
                    print(f"  æ­£é¢æ–°é—»: {stats['positive_count']}")
                    print(f"  è´Ÿé¢æ–°é—»: {stats['negative_count']}")
                    print(f"  ä¸­æ€§æ–°é—»: {stats['neutral_count']}")
                
                print("\nâœ… æƒ…æ„Ÿåˆ†æå®Œæˆ")
            except Exception as e:
                print(f"âš ï¸ æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
                print("ğŸ’¡ æç¤ºï¼šè¯·æ£€æŸ¥ QWEN_API_KEY ç¯å¢ƒå˜é‡æ˜¯å¦è®¾ç½®")
        
    else:
        print("\nâŒ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
        
    print(f"\nğŸ“Š æ€»å…±å¤„ç†äº† {stock_count} åªè‚¡ç¥¨")
    print("=" * 60)

def run_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
    # è®¾ç½®é¦™æ¸¯æ—¶é—´ä¸Šåˆ9ç‚¹å’Œä¸‹åˆ1ç‚¹åŠè¿è¡Œ
    schedule.every().day.at("09:00").do(fetch_all_stock_news)
    #schedule.every().day.at("13:30").do(fetch_all_stock_news)
    
    print("â° å®šæ—¶ä»»åŠ¡å·²è®¾ç½®å®Œæˆ")
    print("ğŸ“Œ æ¯å¤©é¦™æ¸¯æ—¶é—´ä¸Šåˆ9:00å’Œä¸‹åˆ13:30å°†è‡ªåŠ¨è¿è¡Œ")
    print("ğŸ“Œ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»')
    parser.add_argument('--schedule', '-s', action='store_true', 
                        help='å¯ç”¨å®šæ—¶ä»»åŠ¡æ¨¡å¼ï¼ˆé»˜è®¤ï¼šå•æ¬¡è¿è¡Œï¼‰')
    parser.add_argument('--no-sentiment', action='store_true',
                        help='è·³è¿‡æƒ…æ„Ÿåˆ†æï¼ˆé»˜è®¤ï¼šæ‰§è¡Œæƒ…æ„Ÿåˆ†æï¼‰')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if args.schedule:
        # å¯ç”¨å®šæ—¶ä»»åŠ¡æ¨¡å¼
        run_scheduler()
    else:
        # å•æ¬¡è¿è¡Œæ¨¡å¼
        analyze_sentiment = not args.no_sentiment
        fetch_all_stock_news(analyze_sentiment=analyze_sentiment)
