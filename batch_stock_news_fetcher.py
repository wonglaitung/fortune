#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»è„šæœ¬
ä½œè€…ï¼šAI åŠ©æ‰‹
æ—¥æœŸï¼š2025-10-25
"""

import yfinance as yf
from datetime import datetime, timedelta
import json
import os
import csv
import time
import argparse
import schedule
from llm_services.qwen_engine import chat_with_llm

# å¯¼å…¥hk_smart_money_tracker.pyä¸­çš„WATCHLIST
import sys
sys.path.append('/data/fortune')
from hk_smart_money_tracker import WATCHLIST

def filter_news_with_llm(news_list, stock_name, stock_code, max_news=3):
    """
    ä½¿ç”¨å¤§æ¨¡å‹è¿‡æ»¤æ–°é—»ï¼Œè¯„ä¼°æ–°é—»ä¸è‚¡ç¥¨çš„ç›¸å…³æ€§ï¼Œå¹¶æŒ‰æ—¶é—´æ’åº
    """
    if not news_list:
        return []
    
    # å‡†å¤‡æ–°é—»æ•°æ®ç”¨äºå‘é€ç»™å¤§æ¨¡å‹
    news_texts = []
    for i, news in enumerate(news_list):
        news_text = f"{i+1}. æ ‡é¢˜: {news['title']}\n   å†…å®¹: {news['summary']}\n   å‘å¸ƒæ—¶é—´: {news['publishedAt']}\n"
        news_texts.append(news_text)
    
    news_data = "\n".join(news_texts)
    
    # æ„å»ºå¤§æ¨¡å‹æŸ¥è¯¢
    prompt = f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹æ–°é—»ä¸è‚¡ç¥¨ \"{stock_name}\"ï¼ˆä»£ç ï¼š{stock_code}ï¼‰çš„ç›¸å…³æ€§ï¼Œå¹¶è¿‡æ»¤æ‰ä¸ç›¸å…³çš„æ–°é—»ã€‚

åˆ†æè¦æ±‚ï¼š
1. ç›¸å…³æ€§è¯„ä¼°ï¼šæ–°é—»å†…å®¹æ˜¯å¦ç›´æ¥å½±å“è¯¥è‚¡ç¥¨æˆ–å…¶æ‰€å±è¡Œä¸š
2. æ—¶æ•ˆæ€§è€ƒé‡ï¼šå‘å¸ƒæ—¶é—´è¶Šè¿‘è¶Šé‡è¦ï¼ˆä¸€ä¸ªæœˆå†…çš„æ–°é—»ä¼˜å…ˆï¼‰
3. å½±å“åŠ›åˆ¤æ–­ï¼šæ–°é—»å¯¹è‚¡ä»·çš„æ½œåœ¨å½±å“ç¨‹åº¦
4. è¿‡æ»¤ä¸ç›¸å…³ï¼šè¯·åˆ é™¤ä¸è¯¥è‚¡ç¥¨å®Œå…¨æ— å…³çš„æ–°é—»

è‚¡ç¥¨ä¿¡æ¯ï¼š
- è‚¡ç¥¨åç§°ï¼š{stock_name}
- è‚¡ç¥¨ä»£ç ï¼š{stock_code}

æ–°é—»åˆ—è¡¨ï¼š
{news_data}

è¯·æŒ‰ç»¼åˆè¯„åˆ†ä»é«˜åˆ°ä½æ’åºï¼Œè¿”å›å‰{max_news}æ¡æœ€ç›¸å…³ä¸”é‡è¦çš„æ–°é—»åºå·åˆ—è¡¨ã€‚
å¦‚æœæŸäº›æ–°é—»ä¸è¯¥è‚¡ç¥¨å®Œå…¨æ— å…³ï¼Œè¯·ä¸è¦å°†å…¶åŒ…å«åœ¨è¿”å›çš„åˆ—è¡¨ä¸­ã€‚

è¯·æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    \"relevant_news_indices\": [1, 3, 5]
}}

åªè¿”å›JSONæ ¼å¼ç»“æœï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Šã€‚
"""
    
    try:
        # è°ƒç”¨å¤§æ¨¡å‹ï¼ˆéæ¨ç†æ¨¡å¼ï¼‰
        response = chat_with_llm(prompt, enable_thinking=False)
        
        # è§£æå¤§æ¨¡å‹è¿”å›çš„JSON
        start_idx = response.find('{')
        end_idx = response.rfind('}') + 1
        if start_idx != -1 and end_idx != 0:
            json_str = response[start_idx:end_idx]
            result = json.loads(json_str)
            relevant_indices = result.get("relevant_news_indices", [])
            
            # æ ¹æ®å¤§æ¨¡å‹è¿”å›çš„åºå·è·å–ç›¸å…³æ–°é—»
            filtered_news = []
            for idx in relevant_indices[:max_news]:
                if 1 <= idx <= len(news_list):
                    filtered_news.append(news_list[idx-1])
            
            # æŒ‰æ—¶é—´ç”±è¿‘åˆ°è¿œæ’åº
            filtered_news.sort(key=lambda x: x['publishedAt'], reverse=True)
            
            return filtered_news
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„JSONï¼Œè¿”å›åŸå§‹æ•°æ®å¹¶æŒ‰æ—¶é—´æ’åº
            sorted_news = sorted(news_list[:max_news], key=lambda x: x['publishedAt'], reverse=True)
            return sorted_news
    except Exception as e:
        print(f"âš ï¸ å¤§æ¨¡å‹è¿‡æ»¤å¤±è´¥: {e}")
        # å¦‚æœå¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®å¹¶æŒ‰æ—¶é—´æ’åº
        sorted_news = sorted(news_list[:max_news], key=lambda x: x['publishedAt'], reverse=True)
        return sorted_news

def get_stock_news(symbol, stock_name="", size=3):
    """
    é€šè¿‡yfinanceè·å–ä¸ªè‚¡æ–°é—»ï¼Œå¹¶ä½¿ç”¨å¤§æ¨¡å‹è¿›è¡Œç›¸å…³æ€§è¿‡æ»¤
    :param symbol: è‚¡ç¥¨ä»£ç  (ä¾‹å¦‚: "0700.HK" for è…¾è®¯æ§è‚¡)
    :param stock_name: è‚¡ç¥¨åç§° (ä¾‹å¦‚: "è…¾è®¯æ§è‚¡")
    :param size: è·å–æ–°é—»æ¡æ•°
    :return: æ–°é—»åˆ—è¡¨
    """
    try:
        # ä½¿ç”¨yfinanceè·å–ä¸ªè‚¡æ–°é—»
        ticker = yf.Ticker(symbol)
        news_data = ticker.news
        
        if not news_data:
            return []
        
        articles = []
        
        # è®¡ç®—ä¸€ä¸ªæœˆå‰çš„æ—¥æœŸ
        one_month_ago = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0, tzinfo=None) - timedelta(days=30)
        
        # å¤„ç†æ–°é—»æ•°æ®
        for item in news_data:
            # ä»contentå­—æ®µè·å–æ–°é—»æ•°æ®
            content = item.get("content", {})
            
            # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
            pub_time_str = content.get("pubDate", "")
            pub_datetime = None
            pub_time = pub_time_str  # é»˜è®¤ä½¿ç”¨åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
            if pub_time_str:
                try:
                    # å°†ISOæ ¼å¼æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºdatetimeå¯¹è±¡
                    pub_datetime = datetime.fromisoformat(pub_time_str.replace('Z', '+00:00'))
                    # ç§»é™¤æ—¶åŒºä¿¡æ¯ä»¥é¿å…æ¯”è¾ƒé”™è¯¯
                    pub_datetime = pub_datetime.replace(tzinfo=None)
                    pub_time = pub_datetime.strftime("%Y-%m-%d %H:%M:%S")
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¿æŒä½¿ç”¨åŸå§‹æ—¶é—´å­—ç¬¦ä¸²
                    pass
            
            # åªè·å–ä¸€ä¸ªæœˆå†…çš„æ–°é—»
            if pub_datetime and pub_datetime < one_month_ago:
                continue
            
            title = content.get("title", "").strip()
            summary = content.get("summary", "").strip()
            
            # è·å–æ–°é—»é“¾æ¥
            url = ""
            canonical_url = content.get("canonicalUrl", {})
            click_through_url = content.get("clickThroughUrl", {})
            
            if isinstance(canonical_url, dict):
                url = canonical_url.get("url", "")
            elif isinstance(click_through_url, dict):
                url = click_through_url.get("url", "")
            
            articles.append({
                "title": title[:80] + ("..." if len(title) > 80 else ""),
                "summary": summary[:120] + ("..." if len(summary) > 120 else ""),
                "url": url,
                "publishedAt": pub_time
            })
        
        # å¦‚æœæ²¡æœ‰æŒ‡å®šè‚¡ç¥¨åç§°ï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®
        if not stock_name:
            return articles[:size]
        
        # ä½¿ç”¨å¤§æ¨¡å‹è¿‡æ»¤ç›¸å…³æ–°é—»
        filtered_articles = filter_news_with_llm(articles, stock_name, symbol, size)
        return filtered_articles
    except Exception as e:
        print(f"âš ï¸ è·å–ä¸ªè‚¡æ–°é—»å¤±è´¥: {e}")
        return []



def fetch_all_stock_news():
    """è·å–watch listä¸­æ‰€æœ‰è‚¡ç¥¨çš„æ–°é—»"""
    print("=" * 60)
    print("ğŸ“ˆ æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»")
    print(f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # è·å–å½“å‰æŸ¥è¯¢æ—¶é—´
    query_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    
    all_news_data = []
    
    # åˆ é™¤è¶…æ—¶é™åˆ¶ï¼Œå¤„ç†æ‰€æœ‰è‚¡ç¥¨
    stock_count = 0
    
    for code, name in WATCHLIST.items():
        print(f"\nğŸ” æ­£åœ¨è·å– {name} ({code}) çš„æ–°é—»...")
        
        # ä¸ºyfinanceä½¿ç”¨å®Œæ•´çš„è‚¡ç¥¨ä»£ç ï¼ˆåŒ…å«åç¼€å¦‚.HKï¼‰
        symbol_code = code
        
        # è·å–æ–°é—»ï¼Œæ¯åªè‚¡ç¥¨è·å–3æ¡æ–°é—»
        articles = get_stock_news(symbol_code, name, size=3)
        
        if articles:
            print(f"  âœ… è·å–åˆ° {len(articles)} æ¡ç›¸å…³æ–°é—»")
            # æ·»åŠ åˆ°æ€»æ•°æ®ä¸­
            for article in articles:
                all_news_data.append({
                    "stock_name": name,
                    "stock_code": code,
                    "publishedAt": article.get("publishedAt", ""),
                    "title": article.get("title", ""),
                    "summary": article.get("summary", ""),
                    "query_time": query_time
                })
        else:
            print(f"  âš ï¸ æœªè·å–åˆ° {name} çš„æ–°é—»")
            
        stock_count += 1
        # çŸ­æš‚ä¼‘çœ ä»¥é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
        time.sleep(1)
    
    # ä¿å­˜æ‰€æœ‰æ–°é—»æ•°æ®åˆ°CSVæ–‡ä»¶
    if all_news_data:
        # ç¡®ä¿dataç›®å½•å­˜åœ¨
        data_dir = "data"
        if not os.path.exists(data_dir):
            os.makedirs(data_dir)
        
        # CSVæ–‡ä»¶è·¯å¾„
        csv_file = os.path.join(data_dir, "all_stock_news_records.csv")
        
        with open(csv_file, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            # å†™å…¥è¡¨å¤´
            writer.writerow(["è‚¡ç¥¨åç§°", "è‚¡ç¥¨ä»£ç ", "æ–°é—»æ—¶é—´", "æ–°é—»æ ‡é¢˜", "ç®€è¦å†…å®¹", "æŸ¥è¯¢æ—¶é—´"])
            # å†™å…¥æ•°æ®
            for news in all_news_data:
                writer.writerow([
                    news["stock_name"],
                    news["stock_code"],
                    news["publishedAt"],
                    news["title"],
                    news["summary"],
                    news["query_time"]
                ])
        
        print(f"\nâœ… æ‰€æœ‰æ–°é—»æ•°æ®å·²ä¿å­˜åˆ° {csv_file}")
        
        # æ˜¾ç¤ºæ±‡æ€»ä¿¡æ¯
        print("\nğŸ“‹ æ–°é—»æ±‡æ€»:")
        for news in all_news_data:
            print(f"  â€¢ {news['stock_name']} ({news['stock_code']}) | {news['title']}")
    else:
        print("\nâŒ æœªè·å–åˆ°ä»»ä½•æ–°é—»æ•°æ®")
        
    print(f"\nğŸ“Š æ€»å…±å¤„ç†äº† {stock_count} åªè‚¡ç¥¨")
    print("=" * 60)

def run_scheduler():
    """è®¾ç½®å®šæ—¶ä»»åŠ¡"""
    # è®¾ç½®é¦™æ¸¯æ—¶é—´ä¸Šåˆ9ç‚¹å’Œä¸‹åˆ1ç‚¹åŠè¿è¡Œ
    schedule.every().day.at("09:00").do(fetch_all_stock_news)
    #schedule.every().day.at("13:30").do(fetch_all_stock_news)
    
    print("â° å®šæ—¶ä»»åŠ¡å·²è®¾ç½®å®Œæˆ")
    print("ğŸ“Œ æ¯å¤©é¦™æ¸¯æ—¶é—´ä¸Šåˆ9:00å’Œä¸‹åˆ13:30å°†è‡ªåŠ¨è¿è¡Œ")
    print("ğŸ“Œ æŒ‰ Ctrl+C åœæ­¢ç¨‹åº")
    
    while True:
        schedule.run_pending()
        time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡

if __name__ == "__main__":
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description='æ‰¹é‡è·å–è‡ªé€‰è‚¡æ–°é—»')
    parser.add_argument('--schedule', '-s', action='store_true', 
                        help='å¯ç”¨å®šæ—¶ä»»åŠ¡æ¨¡å¼ï¼ˆé»˜è®¤ï¼šå•æ¬¡è¿è¡Œï¼‰')
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    
    if args.schedule:
        # å¯ç”¨å®šæ—¶ä»»åŠ¡æ¨¡å¼
        run_scheduler()
    else:
        # å•æ¬¡è¿è¡Œæ¨¡å¼
        fetch_all_stock_news()
