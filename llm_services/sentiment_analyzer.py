"""
æƒ…æ„Ÿåˆ†ææ¨¡å—
ä½¿ç”¨å¤§æ¨¡å‹å¯¹æ–°é—»è¿›è¡Œå››ç»´æƒ…æ„Ÿè¯„åˆ†
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from llm_services.qwen_engine import chat_with_llm

def analyze_news_sentiment(stock_name, stock_code, news_title, news_content):
    """
    ä½¿ç”¨å¤§æ¨¡å‹åˆ†ææ–°é—»çš„æƒ…æ„Ÿå½±å“

    Args:
        stock_name (str): è‚¡ç¥¨åç§°
        stock_code (str): è‚¡ç¥¨ä»£ç 
        news_title (str): æ–°é—»æ ‡é¢˜
        news_content (str): æ–°é—»å†…å®¹

    Returns:
        dict: åŒ…å«å››ç»´æƒ…æ„Ÿè¯„åˆ†çš„å­—å…¸
            - relevance: ç›¸å…³æ€§ (0-1)
            - impact: å½±å“åº¦ (0-1)
            - expectation_gap: é¢„æœŸå·® (-1åˆ°1)
            - sentiment_direction: æƒ…æ„Ÿæ–¹å‘ (-1åˆ°1)
            - sentiment_score: ç»¼åˆæƒ…æ„Ÿåˆ†æ•°
    """

    prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„é‡‘èåˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹æ–°é—»å¯¹è‚¡ç¥¨çš„æƒ…æ„Ÿå½±å“ã€‚

è‚¡ç¥¨ï¼š{stock_name} ({stock_code})
æ–°é—»æ ‡é¢˜ï¼š{news_title}
æ–°é—»å†…å®¹ï¼š{news_content}

è¯·ä»ä»¥ä¸‹å››ä¸ªç»´åº¦è¿›è¡Œè¯„åˆ†ï¼ˆæ¯ä¸ªç»´åº¦éƒ½è¦ç»™å‡ºç†ç”±å’Œåˆ†æ•°ï¼‰ï¼š

1. **ç›¸å…³æ€§ (Relevance, 0-1åˆ†)**ï¼šæ–°é—»ä¸è¯¥è‚¡ç¥¨çš„ç›´æ¥ç›¸å…³ç¨‹åº¦
   - 0-0.3: ä½ç›¸å…³æ€§ï¼ˆå¦‚å®è§‚æ–°é—»ã€è¡Œä¸šè¶‹åŠ¿ï¼‰
   - 0.3-0.6: ä¸­ç­‰ç›¸å…³æ€§ï¼ˆå¦‚åŒè¡Œä¸šæ–°é—»ï¼‰
   - 0.6-1.0: é«˜ç›¸å…³æ€§ï¼ˆå¦‚å…¬å¸ä¸“å±æ–°é—»ï¼‰

2. **å½±å“åº¦ (Impact, 0-1åˆ†)**ï¼šæ–°é—»å¯¹è‚¡ä»·çš„æ½œåœ¨å½±å“ç¨‹åº¦
   - 0-0.3: ä½å½±å“ï¼ˆå¦‚å¸¸è§„å…¬å‘Šï¼‰
   - 0.3-0.6: ä¸­ç­‰å½±å“ï¼ˆå¦‚ä¸šç»©å…¬å‘Šï¼‰
   - 0.6-1.0: é«˜å½±å“ï¼ˆå¦‚é‡å¤§åˆ©å¥½/åˆ©ç©ºï¼‰

3. **é¢„æœŸå·® (Expectation Gap, -1åˆ°1åˆ†)**ï¼šæ–°é—»æ˜¯å¦è¶…å‡ºå¸‚åœºé¢„æœŸ
   - -1åˆ°-0.3: ä½äºé¢„æœŸï¼ˆè´Ÿé¢æƒŠå–œï¼‰
   - -0.3åˆ°0.3: ç¬¦åˆé¢„æœŸï¼ˆä¸­æ€§ï¼‰
   - 0.3åˆ°1: è¶…å‡ºé¢„æœŸï¼ˆæ­£é¢æƒŠå–œï¼‰

4. **æƒ…æ„Ÿæ–¹å‘ (Sentiment Direction, -1åˆ°1åˆ†)**ï¼šæ–°é—»çš„æƒ…æ„Ÿå€¾å‘
   - -1åˆ°-0.3: è´Ÿé¢ï¼ˆå¦‚ä¸šç»©ä¸‹æ»‘ã€ç›‘ç®¡å¤„ç½šï¼‰
   - -0.3åˆ°0.3: ä¸­æ€§ï¼ˆå¦‚å¸¸è§„å…¬å‘Šï¼‰
   - 0.3åˆ°1: æ­£é¢ï¼ˆå¦‚ä¸šç»©å¢é•¿ã€é‡å¤§åˆä½œï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "relevance": åˆ†æ•°,
  "impact": åˆ†æ•°,
  "expectation_gap": åˆ†æ•°,
  "sentiment_direction": åˆ†æ•°,
  "reasoning": "ç®€è¦è¯´æ˜åˆ†æç†ç”±"
}}

æ³¨æ„ï¼š
- åˆ†æ•°å¿…é¡»æ˜¯æ•°å€¼ç±»å‹ï¼Œä¸è¦æ˜¯å­—ç¬¦ä¸²
- ç†ç”±è¦ç®€æ´æ˜äº†ï¼Œä¸è¶…è¿‡100å­—
- åªè¿”å›JSONï¼Œä¸è¦æœ‰å…¶ä»–å†…å®¹
"""

    try:
        response = chat_with_llm(prompt, enable_thinking=False)

        # å°è¯•è§£æJSONå“åº”
        result = json.loads(response)

        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ['relevance', 'impact', 'expectation_gap', 'sentiment_direction']
        for field in required_fields:
            if field not in result:
                raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")

        # è®¡ç®—ç»¼åˆæƒ…æ„Ÿåˆ†æ•°
        # å…¬å¼ï¼šç›¸å…³æ€§ Ã— å½±å“åº¦ Ã— (é¢„æœŸå·® + æƒ…æ„Ÿæ–¹å‘) Ã— 5
        # ç»“æœæˆªæ–­åˆ° [-5, +5] èŒƒå›´ï¼Œé¿å…æç«¯å€¼
        raw_score = (
            result['relevance'] *
            result['impact'] *
            (result['expectation_gap'] + result['sentiment_direction']) *
            5
        )
        
        # æˆªæ–­åˆ° [-5, +5] èŒƒå›´
        result['sentiment_score'] = max(-5.0, min(5.0, raw_score))

        return result

    except json.JSONDecodeError as e:
        print(f"âš ï¸ JSONè§£æå¤±è´¥: {e}")
        print(f"å“åº”å†…å®¹: {response}")
        return None
    except Exception as e:
        print(f"âš ï¸ æƒ…æ„Ÿåˆ†æå¤±è´¥: {e}")
        return None


def batch_analyze_sentiment(news_df, days_limit=3):
    """
    æ‰¹é‡åˆ†ææ–°é—»æƒ…æ„Ÿ

    Args:
        news_df (DataFrame): æ–°é—»æ•°æ®
        days_limit (int): åªåˆ†ææœ€è¿‘Nå¤©æœªåˆ†æçš„æ–°é—»ï¼ˆé»˜è®¤3å¤©ï¼‰

    Returns:
        DataFrame: åŒ…å«æƒ…æ„Ÿåˆ†æçš„æ–°é—»æ•°æ®
    """
    # è½¬æ¢æ—¥æœŸ
    news_df['æ–°é—»æ—¶é—´'] = pd.to_datetime(news_df['æ–°é—»æ—¶é—´'])
    news_df['æ—¥æœŸ'] = news_df['æ–°é—»æ—¶é—´'].dt.date

    # åªåˆ†ææœ€è¿‘Nå¤©çš„æœªåˆ†ææ–°é—»
    cutoff_date = (datetime.now() - timedelta(days=days_limit)).date()
    recent_news = news_df[news_df['æ—¥æœŸ'] >= cutoff_date].copy()

    # ç­›é€‰æœªåˆ†æçš„æ–°é—»
    unanalyzed = recent_news[
        recent_news['æƒ…æ„Ÿåˆ†æ•°'].isna() |
        recent_news['æƒ…æ„Ÿåˆ†æ•°'].isnull()
    ].copy()

    if len(unanalyzed) == 0:
        print(f"âœ… æ‰€æœ‰æœ€è¿‘{days_limit}å¤©çš„æ–°é—»å·²åˆ†æ")
        # ç»Ÿè®¡å·²åˆ†æçš„æ–°é—»æ€»æ•°
        analyzed_count = len(news_df[news_df['æƒ…æ„Ÿåˆ†æ•°'].notna()])
        print(f"ğŸ“Š å·²æœ‰ {analyzed_count} æ¡æ–°é—»å®Œæˆæƒ…æ„Ÿåˆ†æ")
        return news_df

    print(f"ğŸ“Š å¼€å§‹åˆ†æ {len(unanalyzed)} æ¡æ–°é—»çš„æƒ…æ„Ÿ...")

    # é€æ¡åˆ†æ
    for idx, row in unanalyzed.iterrows():
        try:
            result = analyze_news_sentiment(
                row['è‚¡ç¥¨åç§°'],
                row['è‚¡ç¥¨ä»£ç '],
                row['æ–°é—»æ ‡é¢˜'],
                row['ç®€è¦å†…å®¹']
            )

            if result:
                # æ›´æ–°æ•°æ®
                news_df.loc[idx, 'æƒ…æ„Ÿåˆ†æ•°'] = result['sentiment_score']
                news_df.loc[idx, 'ç›¸å…³æ€§'] = result['relevance']
                news_df.loc[idx, 'å½±å“åº¦'] = result['impact']
                news_df.loc[idx, 'é¢„æœŸå·®'] = result['expectation_gap']
                news_df.loc[idx, 'æƒ…æ„Ÿæ–¹å‘'] = result['sentiment_direction']
                news_df.loc[idx, 'æƒ…æ„Ÿåˆ†ææ—¶é—´'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

                print(f"âœ… [{row['è‚¡ç¥¨ä»£ç ']}] æƒ…æ„Ÿåˆ†æ•°: {result['sentiment_score']:.2f}")
            else:
                print(f"âš ï¸ [{row['è‚¡ç¥¨ä»£ç ']}] åˆ†æå¤±è´¥")

        except Exception as e:
            print(f"âŒ [{row['è‚¡ç¥¨ä»£ç ']}] åˆ†æå¼‚å¸¸: {e}")
            continue

    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
    news_df.to_csv('data/all_stock_news_records.csv', index=False, encoding='utf-8-sig')
    print(f"âœ… æƒ…æ„Ÿåˆ†æå®Œæˆï¼Œæ•°æ®å·²ä¿å­˜")

    return news_df


def get_sentiment_statistics(news_df):
    """
    è·å–æƒ…æ„Ÿåˆ†æç»Ÿè®¡ä¿¡æ¯

    Args:
        news_df (DataFrame): æ–°é—»æ•°æ®

    Returns:
        dict: ç»Ÿè®¡ä¿¡æ¯
    """
    # ç­›é€‰å·²åˆ†æçš„æ–°é—»
    analyzed = news_df[news_df['æƒ…æ„Ÿåˆ†æ•°'].notna()].copy()

    if len(analyzed) == 0:
        return {
            'total': 0,
            'analyzed': 0,
            'unanalyzed': len(news_df)
        }

    stats = {
        'total': len(news_df),
        'analyzed': len(analyzed),
        'unanalyzed': len(news_df) - len(analyzed),
        'sentiment_score_mean': analyzed['æƒ…æ„Ÿåˆ†æ•°'].mean(),
        'sentiment_score_std': analyzed['æƒ…æ„Ÿåˆ†æ•°'].std(),
        'sentiment_score_min': analyzed['æƒ…æ„Ÿåˆ†æ•°'].min(),
        'sentiment_score_max': analyzed['æƒ…æ„Ÿåˆ†æ•°'].max(),
        'positive_count': len(analyzed[analyzed['æƒ…æ„Ÿåˆ†æ•°'] > 0]),
        'negative_count': len(analyzed[analyzed['æƒ…æ„Ÿåˆ†æ•°'] < 0]),
        'neutral_count': len(analyzed[analyzed['æƒ…æ„Ÿåˆ†æ•°'] == 0])
    }

    return stats


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    print("=== æƒ…æ„Ÿåˆ†ææµ‹è¯• ===")

    # è¯»å–æ–°é—»æ•°æ®
    df = pd.read_csv('data/all_stock_news_records.csv')

    # æ‰¹é‡åˆ†ææœ€è¿‘3å¤©çš„æ–°é—»
    result_df = batch_analyze_sentiment(df, days_limit=3)

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = get_sentiment_statistics(result_df)
    print("\n=== æƒ…æ„Ÿåˆ†æç»Ÿè®¡ ===")
    print(f"æ€»æ–°é—»æ•°: {stats['total']}")
    print(f"å·²åˆ†æ: {stats['analyzed']}")
    print(f"æœªåˆ†æ: {stats['unanalyzed']}")
    if stats['analyzed'] > 0:
        print(f"å¹³å‡æƒ…æ„Ÿåˆ†æ•°: {stats['sentiment_score_mean']:.2f}")
        print(f"æ­£é¢æ–°é—»: {stats['positive_count']}")
        print(f"è´Ÿé¢æ–°é—»: {stats['negative_count']}")
        print(f"ä¸­æ€§æ–°é—»: {stats['neutral_count']}")