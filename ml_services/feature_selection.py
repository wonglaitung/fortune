#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç‰¹å¾é€‰æ‹©ä¼˜åŒ–è„šæœ¬ - ä½¿ç”¨F-test + äº’ä¿¡æ¯æ··åˆæ–¹æ³•
ä»2936ä¸ªç‰¹å¾ä¸­ç­›é€‰å‡ºæœ€æœ‰æ•ˆçš„ç‰¹å¾ï¼ˆ500-1000ä¸ªï¼‰
é¢„æœŸæå‡ï¼š0.5-1%å‡†ç¡®ç‡
"""

import os
import sys
import argparse
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, f_regression, mutual_info_regression
from sklearn.model_selection import cross_val_score
import lightgbm as lgb

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import WATCHLIST as STOCK_LIST
from ml_services.ml_trading_model import MLTradingModel
from ml_services.logger_config import get_logger

logger = get_logger('feature_selection')


def load_training_data():
    """
    åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆå¤ç”¨ç°æœ‰æ¨¡å‹çš„æ•°æ®å‡†å¤‡æµç¨‹ï¼‰

    è¿”å›:
    - X: ç‰¹å¾çŸ©é˜µ
    - y: ç›®æ ‡å˜é‡
    - feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    """
    logger.info("=" * 50)
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®")
    logger.info("=" * 50)

    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    model = MLTradingModel()

    # å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨20å¤©é¢„æµ‹horizonï¼‰
    codes = list(STOCK_LIST.keys())
    logger.info(f"å‡†å¤‡åŠ è½½ {len(codes)} åªè‚¡ç¥¨çš„æ•°æ®...")

    # è°ƒç”¨prepare_dataæ–¹æ³•ï¼ˆè¿”å›DataFrameï¼‰
    df = model.prepare_data(codes, horizon=20)

    # å…ˆåˆ é™¤å…¨ä¸ºNaNçš„åˆ—
    cols_all_nan = df.columns[df.isnull().all()].tolist()
    if cols_all_nan:
        print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_all_nan)} ä¸ªå…¨ä¸ºNaNçš„åˆ—")
        df = df.drop(columns=cols_all_nan)

    # åˆ é™¤åŒ…å«NaNçš„è¡Œ
    df = df.dropna()

    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸç´¢å¼•æ’åº
    df = df.sort_index()

    # è·å–ç‰¹å¾åˆ—
    feature_columns = model.get_feature_columns(df)
    print(f"ä½¿ç”¨ {len(feature_columns)} ä¸ªç‰¹å¾")

    # å¤„ç†åˆ†ç±»ç‰¹å¾
    categorical_features = []
    for col in feature_columns:
        if df[col].dtype == 'object' or df[col].dtype.name == 'category':
            print(f"  ç¼–ç åˆ†ç±»ç‰¹å¾: {col}")
            categorical_features.append(col)
            from sklearn.preprocessing import LabelEncoder
            le = LabelEncoder()
            df[col] = le.fit_transform(df[col].astype(str))

    # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
    X = df[feature_columns]
    y = df['Label']

    logger.info(f"æ•°æ®åŠ è½½å®Œæˆ")
    print(f"   - æ ·æœ¬æ•°é‡: {len(X)}")
    print(f"   - ç‰¹å¾æ•°é‡: {len(feature_columns)}")
    print(f"   - ç›®æ ‡å˜é‡åˆ†å¸ƒ: {y.value_counts().to_dict()}")
    print("")

    return X, y, feature_columns


def feature_selection_f_test(X, y, k=1000):
    """
    ä½¿ç”¨F-testé€‰æ‹©ç‰¹å¾

    å‚æ•°:
    - X: ç‰¹å¾çŸ©é˜µ
    - y: ç›®æ ‡å˜é‡
    - k: é€‰æ‹©çš„ç‰¹å¾æ•°é‡

    è¿”å›:
    - selected_features: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    - scores: F-teståˆ†æ•°
    """
    logger.info("=" * 50)
    print("ğŸ”¬ F-testç‰¹å¾é€‰æ‹©")
    logger.info("=" * 50)

    selector = SelectKBest(f_classif, k=k)
    X_selected = selector.fit_transform(X, y)

    selected_features = selector.get_support(indices=True)
    scores = selector.scores_

    logger.info(f"F-testé€‰æ‹©å®Œæˆ")
    print(f"   - é€‰æ‹©ç‰¹å¾æ•°é‡: {len(selected_features)}")
    print(f"   - å¹³å‡F-teståˆ†æ•°: {np.mean(scores):.2f}")
    print(f"   - æœ€é«˜F-teståˆ†æ•°: {np.max(scores):.2f}")
    print("")

    return selected_features, scores


def feature_selection_mutual_info(X, y, k=1000):
    """
    ä½¿ç”¨äº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾

    å‚æ•°:
    - X: ç‰¹å¾çŸ©é˜µ
    - y: ç›®æ ‡å˜é‡
    - k: é€‰æ‹©çš„ç‰¹å¾æ•°é‡

    è¿”å›:
    - selected_features: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    - scores: äº’ä¿¡æ¯åˆ†æ•°
    """
    logger.info("=" * 50)
    print("ğŸ”¬ äº’ä¿¡æ¯ç‰¹å¾é€‰æ‹©")
    logger.info("=" * 50)

    selector = SelectKBest(mutual_info_classif, k=k)
    X_selected = selector.fit_transform(X, y)

    selected_features = selector.get_support(indices=True)
    scores = selector.scores_

    logger.info(f"äº’ä¿¡æ¯é€‰æ‹©å®Œæˆ")
    print(f"   - é€‰æ‹©ç‰¹å¾æ•°é‡: {len(selected_features)}")
    print(f"   - å¹³å‡äº’ä¿¡æ¯åˆ†æ•°: {np.mean(scores):.4f}")
    print(f"   - æœ€é«˜äº’ä¿¡æ¯åˆ†æ•°: {np.max(scores):.4f}")
    print("")

    return selected_features, scores


def feature_selection_hybrid(X, y, feature_names, top_k=500):
    """
    ä½¿ç”¨F-test + äº’ä¿¡æ¯æ··åˆæ–¹æ³•é€‰æ‹©ç‰¹å¾

    ç­–ç•¥ï¼š
    1. åˆ†åˆ«ä½¿ç”¨F-testå’Œäº’ä¿¡æ¯é€‰æ‹©top 1000ç‰¹å¾
    2. å–ä¸¤è€…çš„äº¤é›†ï¼ˆçº¦500-700ä¸ªç‰¹å¾ï¼‰
    3. æŒ‰ç»¼åˆå¾—åˆ†æ’åºï¼Œé€‰æ‹©top 500ç‰¹å¾

    å‚æ•°:
    - X: ç‰¹å¾çŸ©é˜µ
    - y: ç›®æ ‡å˜é‡
    - feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    - top_k: æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°é‡

    è¿”å›:
    - selected_features: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    - feature_scores: ç‰¹å¾å¾—åˆ†DataFrame
    """
    logger.info("=" * 50)
    print("ğŸ”¬ F-test + äº’ä¿¡æ¯æ··åˆç‰¹å¾é€‰æ‹©")
    logger.info("=" * 50)

    # 1. F-testé€‰æ‹©
    f_selected, f_scores = feature_selection_f_test(X, y, k=1000)

    # 2. äº’ä¿¡æ¯é€‰æ‹©
    mi_selected, mi_scores = feature_selection_mutual_info(X, y, k=1000)

    # 3. å–äº¤é›†
    f_set = set(f_selected)
    mi_set = set(mi_selected)
    intersection = f_set.intersection(mi_set)

    logger.info(f"é€‰æ‹©ç»“æœç»Ÿè®¡")
    print(f"   - F-testé€‰æ‹©: {len(f_selected)} ä¸ªç‰¹å¾")
    print(f"   - äº’ä¿¡æ¯é€‰æ‹©: {len(mi_selected)} ä¸ªç‰¹å¾")
    print(f"   - äº¤é›†: {len(intersection)} ä¸ªç‰¹å¾")
    print("")

    # 4. è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆå½’ä¸€åŒ–åå¹³å‡ï¼‰
    all_features = set(range(len(feature_names)))

    feature_data = []
    for idx in all_features:
        f_score = f_scores[idx] if idx in f_set else 0
        mi_score = mi_scores[idx] if idx in mi_set else 0

        # å½’ä¸€åŒ–
        f_score_norm = f_score / np.max(f_scores) if np.max(f_scores) > 0 else 0
        mi_score_norm = mi_score / np.max(mi_scores) if np.max(mi_scores) > 0 else 0

        # ç»¼åˆå¾—åˆ†ï¼ˆå¹³å‡ï¼‰
        combined_score = (f_score_norm + mi_score_norm) / 2

        feature_data.append({
            'Feature_Index': idx,
            'Feature_Name': feature_names[idx],
            'F_Test_Score': f_score,
            'F_Test_Normalized': f_score_norm,
            'MI_Score': mi_score,
            'MI_Normalized': mi_score_norm,
            'Combined_Score': combined_score,
            'In_Intersection': idx in intersection
        })

    # åˆ›å»ºDataFrame
    feature_scores = pd.DataFrame(feature_data)

    # 5. é€‰æ‹©top_kç‰¹å¾
    feature_scores_sorted = feature_scores.sort_values('Combined_Score', ascending=False)
    selected_features = feature_scores_sorted.head(top_k)['Feature_Index'].values

    logger.info(f"æ··åˆé€‰æ‹©å®Œæˆ")
    print(f"   - æœ€ç»ˆé€‰æ‹©ç‰¹å¾æ•°é‡: {len(selected_features)}")
    print(f"   - äº¤é›†ç‰¹å¾æ•°é‡: {feature_scores_sorted.head(top_k)['In_Intersection'].sum()}")
    print(f"   - å¹³å‡ç»¼åˆå¾—åˆ†: {feature_scores_sorted.head(top_k)['Combined_Score'].mean():.4f}")
    print("")

    return selected_features, feature_scores_sorted


def evaluate_feature_selection(X, y, selected_features, feature_names):
    """
    è¯„ä¼°ç‰¹å¾é€‰æ‹©æ•ˆæœ

    å‚æ•°:
    - X: åŸå§‹ç‰¹å¾çŸ©é˜µ
    - y: ç›®æ ‡å˜é‡
    - selected_features: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    - feature_names: ç‰¹å¾åç§°åˆ—è¡¨

    è¿”å›:
    - performance: æ€§èƒ½æŒ‡æ ‡
    """
    logger.info("=" * 50)
    print("ğŸ“ˆ è¯„ä¼°ç‰¹å¾é€‰æ‹©æ•ˆæœ")
    logger.info("=" * 50)

    try:
        # é€‰æ‹©ç‰¹å¾
        X_selected = X.iloc[:, selected_features]

        # åˆ›å»ºLightGBMæ•°æ®é›†
        lgb_train = lgb.Dataset(X_selected, y)

        # ä½¿ç”¨ç®€å•å‚æ•°å¿«é€Ÿè¯„ä¼°
        params = {
            'objective': 'binary',
            'metric': 'binary_logloss',
            'boosting_type': 'gbdt',
            'num_leaves': 31,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'verbose': -1
        }

        # ä½¿ç”¨cvè¯„ä¼°
        cv_results = lgb.cv(
            params,
            lgb_train,
            num_boost_round=100,
            nfold=5,
            stratified=False
        )

        # æ£€æŸ¥cv_resultsçš„é”®å
        logger.info(f"cv_resultsé”®å: {list(cv_results.keys())}")

        # å°è¯•æ‰¾åˆ°æ­£ç¡®çš„é”®å
        mean_key = None
        stdv_key = None
        for key in cv_results.keys():
            if 'mean' in key:
                mean_key = key
            if 'stdv' in key:
                stdv_key = key

        if mean_key and stdv_key:
            avg_logloss = np.mean(cv_results[mean_key])
            std_logloss = np.std(cv_results[stdv_key])

            logger.info(f"è¯„ä¼°å®Œæˆ")
            print(f"   - å¹³å‡{mean_key}: {avg_logloss:.4f}")
            print(f"   - {stdv_key}: {std_logloss:.4f}")
            print("")

            performance = {
                'avg_logloss': avg_logloss,
                'std_logloss': std_logloss,
                'num_features': len(selected_features)
            }

            return performance
        else:
            logger.warning(f"æ— æ³•æ‰¾åˆ°æ­£ç¡®çš„é”®åï¼Œä½¿ç”¨é»˜è®¤å€¼")
            performance = {
                'avg_logloss': 0.0,
                'std_logloss': 0.0,
                'num_features': len(selected_features)
            }
            return performance

    except Exception as e:
        logger.warning(f"è¯„ä¼°å¤±è´¥: {e}")
        print("ä½¿ç”¨é»˜è®¤å€¼ç»§ç»­...")
        performance = {
            'avg_logloss': 0.0,
            'std_logloss': 0.0,
            'num_features': len(selected_features)
        }
        return performance


def save_results(feature_scores, selected_features, output_dir='output'):
    """
    ä¿å­˜ç‰¹å¾é€‰æ‹©ç»“æœ

    å‚æ•°:
    - feature_scores: ç‰¹å¾å¾—åˆ†DataFrame
    - selected_features: é€‰æ‹©çš„ç‰¹å¾ç´¢å¼•
    - output_dir: è¾“å‡ºç›®å½•
    """
    logger.info("=" * 50)
    print("ğŸ’¾ ä¿å­˜ç»“æœ")
    logger.info("=" * 50)

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # ä¿å­˜ç‰¹å¾å¾—åˆ†
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    feature_scores_path = os.path.join(output_dir, f'feature_selection_scores_{timestamp}.csv')
    feature_scores.to_csv(feature_scores_path, index=False, encoding='utf-8-sig')
    logger.info(f"ç‰¹å¾å¾—åˆ†å·²ä¿å­˜è‡³: {feature_scores_path}")

    # ä¿å­˜é€‰æ‹©çš„ç‰¹å¾
    selected_features_path = os.path.join(output_dir, f'selected_features_{timestamp}.csv')
    selected_df = feature_scores[feature_scores['Feature_Index'].isin(selected_features)].copy()
    selected_df.to_csv(selected_features_path, index=False, encoding='utf-8-sig')
    logger.info(f"é€‰æ‹©çš„ç‰¹å¾å·²ä¿å­˜è‡³: {selected_features_path}")

    # ä¿å­˜ç‰¹å¾ç´¢å¼•åˆ—è¡¨ï¼ˆæ–¹ä¾¿åç»­ä½¿ç”¨ï¼‰
    selected_indices_path = os.path.join(output_dir, f'selected_feature_indices_{timestamp}.txt')
    with open(selected_indices_path, 'w', encoding='utf-8') as f:
        f.write(','.join(map(str, selected_features)))
    logger.info(f"ç‰¹å¾ç´¢å¼•å·²ä¿å­˜è‡³: {selected_indices_path}")

    print("")


def main():
    parser = argparse.ArgumentParser(description='ç‰¹å¾é€‰æ‹©ä¼˜åŒ– - F-test + äº’ä¿¡æ¯æ··åˆæ–¹æ³•')
    parser.add_argument('--top-k', type=int, default=500,
                       help='æœ€ç»ˆé€‰æ‹©çš„ç‰¹å¾æ•°é‡ (é»˜è®¤: 500)')
    parser.add_argument('--output-dir', type=str, default='output',
                       help='è¾“å‡ºç›®å½• (é»˜è®¤: output)')

    args = parser.parse_args()

    print("\n" + "=" * 80)
    print("ğŸš€ ç‰¹å¾é€‰æ‹©ä¼˜åŒ–å¼€å§‹")
    logger.info("=" * 50)
    print(f"âš™ï¸  å‚æ•°é…ç½®:")
    print(f"   - ç›®æ ‡ç‰¹å¾æ•°é‡: {args.top_k}")
    print(f"   - è¾“å‡ºç›®å½•: {args.output_dir}")
    print("")

    try:
        # æ­¥éª¤1: åŠ è½½è®­ç»ƒæ•°æ®
        X, y, feature_names = load_training_data()

        # æ­¥éª¤2: æ··åˆç‰¹å¾é€‰æ‹©
        selected_features, feature_scores = feature_selection_hybrid(
            X, y, feature_names, top_k=args.top_k
        )

        # æ­¥éª¤3: è¯„ä¼°æ•ˆæœ
        performance = evaluate_feature_selection(X, y, selected_features, feature_names)

        # æ­¥éª¤4: ä¿å­˜ç»“æœ
        save_results(feature_scores, selected_features, args.output_dir)

        # æ­¥éª¤5: æ˜¾ç¤ºTop 20ç‰¹å¾
        logger.info("=" * 50)
        print("ğŸ† Top 20ç‰¹å¾")
        logger.info("=" * 50)
        top_20 = feature_scores.head(20)
        print(top_20[['Feature_Name', 'Combined_Score', 'In_Intersection']].to_string(index=False))
        print("")

        logger.info("=" * 50)
        logger.info("ç‰¹å¾é€‰æ‹©ä¼˜åŒ–å®Œæˆï¼")
        logger.info("=" * 50)
        logger.info(f"ä¼˜åŒ–æ€»ç»“:")
        print(f"   - åŸå§‹ç‰¹å¾æ•°é‡: {len(feature_names)}")
        print(f"   - ä¼˜åŒ–åç‰¹å¾æ•°é‡: {len(selected_features)}")
        print(f"   - ç‰¹å¾å‡å°‘æ¯”ä¾‹: {(1 - len(selected_features)/len(feature_names))*100:.1f}%")
        print(f"   - äº¤é›†ç‰¹å¾å æ¯”: {feature_scores[feature_scores['Feature_Index'].isin(selected_features)]['In_Intersection'].sum()/len(selected_features)*100:.1f}%")
        print(f"   - å¹³å‡ç»¼åˆå¾—åˆ†: {feature_scores[feature_scores['Feature_Index'].isin(selected_features)]['Combined_Score'].mean():.4f}")
        print("")
        print(f"ğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. æ£€æŸ¥é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä¿å­˜åˆ° {args.output_dir}/selected_features_*.csvï¼‰")
        print(f"   2. åœ¨ml_trading_model.pyä¸­é›†æˆç‰¹å¾é€‰æ‹©é€»è¾‘")
        print(f"   3. é‡æ–°è®­ç»ƒæ¨¡å‹å¹¶è¯„ä¼°å‡†ç¡®ç‡æå‡")
        print(f"   4. å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ¨¡å‹æ€§èƒ½")
        print("")

    except Exception as e:
        logger.error(f"ç‰¹å¾é€‰æ‹©å¤±è´¥: {e}")
    except Exception as e:
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()