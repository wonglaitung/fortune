#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹ - äºŒåˆ†ç±»æ¨¡å‹é¢„æµ‹æ¬¡æ—¥æ¶¨è·Œ
æ•´åˆæŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢ã€èµ„é‡‘æµå‘ç­‰ç‰¹å¾ï¼Œä½¿ç”¨LightGBMè¿›è¡Œè®­ç»ƒ
"""

import warnings
import os
import sys
import argparse
from datetime import datetime, timedelta
import pickle
import hashlib
import json
import random
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

# ç¼“å­˜é…ç½®
CACHE_DIR = 'data/stock_cache'
STOCK_DATA_CACHE_DAYS = 7  # è‚¡ç¥¨å†å²æ•°æ®ç¼“å­˜7å¤©
HSI_DATA_CACHE_HOURS = 1   # æ’ç”ŸæŒ‡æ•°æ•°æ®ç¼“å­˜1å°æ—¶

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data_services.tencent_finance import get_hk_stock_data_tencent, get_hsi_data_tencent
from data_services.technical_analysis import TechnicalAnalyzer
from data_services.fundamental_data import get_comprehensive_fundamental_data
from ml_services.base_model_processor import BaseModelProcessor
from ml_services.us_market_data import us_market_data
from ml_services.logger_config import get_logger
from config import WATCHLIST as STOCK_LIST

# è‚¡ç¥¨åç§°æ˜ å°„
STOCK_NAMES = STOCK_LIST

# è‡ªé€‰è‚¡åˆ—è¡¨ï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼‰
WATCHLIST = list(STOCK_NAMES.keys())

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger('ml_trading_model')


# ========== ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ ==========
def save_predictions_to_text(predictions_df, predict_date=None):
    """
    ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­æå–å’Œå¯¹æ¯”

    å‚æ•°:
    - predictions_df: é¢„æµ‹ç»“æœDataFrame
    - predict_date: é¢„æµ‹æ—¥æœŸ
    """
    try:
        from datetime import datetime

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¥æœŸï¼‰
        if predict_date:
            date_str = predict_date
        else:
            date_str = datetime.now().strftime('%Y-%m-%d')

        # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists('data'):
            os.makedirs('data')

        # æ–‡ä»¶è·¯å¾„
        filepath = f'data/ml_predictions_20d_{date_str}.txt'

        # æ„å»ºå†…å®¹
        content = f"{'=' * 80}\n"
        content += f"æœºå™¨å­¦ä¹ 20å¤©é¢„æµ‹ç»“æœ\n"
        content += f"é¢„æµ‹æ—¥æœŸ: {date_str}\n"
        content += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += f"{'=' * 80}\n\n"

        # æ·»åŠ é¢„æµ‹ç»“æœ
        content += "ã€é¢„æµ‹ç»“æœã€‘\n"
        content += "-" * 80 + "\n"
        content += f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹æ–¹å‘':<10} {'ä¸Šæ¶¨æ¦‚ç‡':<12} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡æ—¥æœŸ':<15}\n"
        content += "-" * 80 + "\n"

        # æŒ‰ä¸€è‡´æ€§æ’åºï¼ˆå¦‚æœæœ‰consistentåˆ—ï¼‰
        if 'consistent' in predictions_df.columns:
            predictions_df_sorted = predictions_df.sort_values(by=['consistent', 'avg_probability'], ascending=[False, False])
        else:
            predictions_df_sorted = predictions_df.sort_values(by='probability', ascending=False)

        for _, row in predictions_df_sorted.iterrows():
            code = row.get('code', 'N/A')
            name = row.get('name', 'N/A')
            current_price = row.get('current_price', None)
            data_date = row.get('data_date', 'N/A')
            target_date = row.get('target_date', 'N/A')
            
            # å°è¯•è·å–é¢„æµ‹å’Œæ¦‚ç‡ï¼ˆæ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼‰
            prediction = None
            probability = None
            
            # ä¼˜å…ˆä½¿ç”¨å¹³å‡æ¦‚ç‡å’Œä¸€è‡´æ€§åˆ¤æ–­
            if 'avg_probability' in row and 'consistent' in row:
                if row['consistent']:
                    # ä¸¤ä¸ªæ¨¡å‹ä¸€è‡´ï¼Œä½¿ç”¨å¹³å‡æ¦‚ç‡
                    probability = row['avg_probability']
                    prediction = 1 if probability >= 0.5 else 0
            elif 'prediction' in row:
                prediction = row.get('prediction', None)
                probability = row.get('probability', None)
            elif 'prediction_LGBM' in row:
                # ä½¿ç”¨LGBMçš„é¢„æµ‹
                prediction = row.get('prediction_LGBM', None)
                probability = row.get('probability_LGBM', None)

            if prediction is not None:
                pred_label = "ä¸Šæ¶¨" if prediction == 1 else "ä¸‹è·Œ"
                prob_str = f"{probability:.4f}" if probability is not None else "N/A"
                price_str = f"{current_price:.2f}" if current_price is not None else "N/A"
            else:
                pred_label = "N/A"
                prob_str = "N/A"
                price_str = "N/A"

            content += f"{code:<10} {name:<12} {pred_label:<10} {prob_str:<12} {price_str:<12} {data_date:<15} {target_date:<15}\n"

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        content += "\n" + "-" * 80 + "\n"
        content += "ã€ç»Ÿè®¡ä¿¡æ¯ã€‘\n"
        content += "-" * 80 + "\n"

        # åˆå§‹åŒ–å˜é‡
        total_count = 0
        up_count = 0
        down_count = 0
        consistent_count = 0
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_count = len(predictions_df)
        
        # è®¡ç®—ä¸Šæ¶¨å’Œä¸‹è·Œæ•°é‡
        if 'avg_probability' in predictions_df.columns:
            up_count = (predictions_df['avg_probability'] >= 0.5).sum()
            down_count = total_count - up_count
        elif 'prediction' in predictions_df.columns:
            up_count = (predictions_df['prediction'] == 1).sum()
            down_count = (predictions_df['prediction'] == 0).sum()
        elif 'prediction_LGBM' in predictions_df.columns:
            up_count = (predictions_df['prediction_LGBM'] == 1).sum()
            down_count = total_count - up_count
        
        if total_count > 0:
            content += f"é¢„æµ‹ä¸Šæ¶¨: {up_count} åª\n"
            content += f"é¢„æµ‹ä¸‹è·Œ: {down_count} åª\n"
            content += f"æ€»è®¡: {total_count} åª\n"
            content += f"ä¸Šæ¶¨æ¯”ä¾‹: {up_count/total_count*100:.1f}%\n"

        if 'consistent' in predictions_df.columns:
            consistent_count = predictions_df['consistent'].sum()
            content += f"\nä¸¤ä¸ªæ¨¡å‹ä¸€è‡´æ€§: {consistent_count}/{total_count} ({consistent_count/total_count*100:.1f}%)\n"

        if 'avg_probability' in predictions_df.columns:
            avg_prob = predictions_df['avg_probability'].mean()
            content += f"å¹³å‡ä¸Šæ¶¨æ¦‚ç‡: {avg_prob:.4f}\n"

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        logger.info(f"20å¤©é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {filepath}")
        return filepath

    except Exception as e:
        logger.error(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        import traceback
        logger.debug(traceback.format_exc())
        return None


def get_target_date(date, horizon):
    """è®¡ç®—ç›®æ ‡æ—¥æœŸï¼ˆæ•°æ®æ—¥æœŸ + é¢„æµ‹å‘¨æœŸï¼‰"""
    if isinstance(date, str):
        date = datetime.strptime(date, '%Y-%m-%d')
    target_date = date + timedelta(days=horizon)
    return target_date.strftime('%Y-%m-%d')


# ========== ç¼“å­˜è¾…åŠ©å‡½æ•° ==========
def _get_cache_key(stock_code, period_days):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"{stock_code}_{period_days}d"

def _get_cache_file_path(cache_key):
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)
    return os.path.join(CACHE_DIR, f"{cache_key}.pkl")

def _is_cache_valid(cache_file_path, cache_hours):
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    if not os.path.exists(cache_file_path):
        return False
    cache_time = os.path.getmtime(cache_file_path)
    current_time = datetime.now().timestamp()
    age_hours = (current_time - cache_time) / 3600
    return age_hours < cache_hours

def _save_cache(cache_file_path, data):
    """ä¿å­˜ç¼“å­˜"""
    try:
        with open(cache_file_path, 'wb') as f:
            pickle.dump({
                'data': data,
                'timestamp': datetime.now().isoformat()
            }, f)
    except Exception as e:
        logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def _load_cache(cache_file_path):
    """åŠ è½½ç¼“å­˜"""
    try:
        with open(cache_file_path, 'rb') as f:
            cache = pickle.load(f)
            return cache['data']
    except Exception as e:
        logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return None

def get_stock_data_with_cache(stock_code, period_days=730):
    """è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = _get_cache_key(stock_code, period_days)
    cache_file_path = _get_cache_file_path(cache_key)
    
    # æ£€æŸ¥ç¼“å­˜
    if _is_cache_valid(cache_file_path, STOCK_DATA_CACHE_DAYS * 24):
        logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„è‚¡ç¥¨æ•°æ® {stock_code}")
        cached_data = _load_cache(cache_file_path)
        if cached_data is not None:
            return cached_data

    # ä»ç½‘ç»œè·å–
        logger.debug(f"ä¸‹è½½è‚¡ç¥¨æ•°æ® {stock_code}")
    stock_df = get_hk_stock_data_tencent(stock_code, period_days)
    
    # ä¿å­˜ç¼“å­˜
    if stock_df is not None and not stock_df.empty:
        _save_cache(cache_file_path, stock_df)
    
    return stock_df

def get_hsi_data_with_cache(period_days=730):
    """è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = _get_cache_key("HSI", period_days)
    cache_file_path = _get_cache_file_path(cache_key)
    
    # æ£€æŸ¥ç¼“å­˜
    if _is_cache_valid(cache_file_path, HSI_DATA_CACHE_HOURS):
        logger.debug("ä½¿ç”¨ç¼“å­˜çš„æ’ç”ŸæŒ‡æ•°æ•°æ®")
        cached_data = _load_cache(cache_file_path)
        if cached_data is not None:
            return cached_data

    # ä»ç½‘ç»œè·å–
        logger.debug("ä¸‹è½½æ’ç”ŸæŒ‡æ•°æ•°æ®")
    hsi_df = get_hsi_data_tencent(period_days)
    
    # ä¿å­˜ç¼“å­˜
    if hsi_df is not None and not hsi_df.empty:
        _save_cache(cache_file_path, hsi_df)
    
    return hsi_df


class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹ç±»"""

    def __init__(self):
        self.tech_analyzer = TechnicalAnalyzer()
        # æ¿å—åˆ†æç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self._sector_analyzer = None
        self._sector_performance_cache = {}
        # æ–°é—»æ•°æ®ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
        self._news_data_cache = None
        self._news_data_days = 30

    def _get_sector_analyzer(self):
        """è·å–æ¿å—åˆ†æå™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
        if self._sector_analyzer is None:
            try:
                from data_services.hk_sector_analysis import SectorAnalyzer
                self._sector_analyzer = SectorAnalyzer()
                logger.debug("æ¿å—åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except ImportError:
                logger.warning("æ¿å—åˆ†ææ¨¡å—ä¸å¯ç”¨")
                return None
        return self._sector_analyzer

    def _get_sector_performance(self, period):
        """è·å–æ¿å—è¡¨ç°æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f'period_{period}'
        
        if cache_key not in self._sector_performance_cache:
            analyzer = self._get_sector_analyzer()
            if analyzer is None:
                return None
            
            try:
                perf_df = analyzer.calculate_sector_performance(period)
                self._sector_performance_cache[cache_key] = perf_df
            except Exception as e:
                print(f"  âš ï¸ è·å–æ¿å—è¡¨ç°å¤±è´¥ (period={period}): {e}")
                return None
        
        return self._sector_performance_cache[cache_key]

    def calculate_technical_features(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆæ‰©å±•ç‰ˆï¼š80ä¸ªæŒ‡æ ‡ï¼‰"""
        if df.empty or len(df) < 200:
            return df

        # ========== åŸºç¡€ç§»åŠ¨å¹³å‡çº¿ ==========
        df = self.tech_analyzer.calculate_moving_averages(df, periods=[5, 10, 20, 50, 100, 200])

        # ========== RSI (Wilder å¹³æ»‘) ==========
        df = self.tech_analyzer.calculate_rsi(df, period=14)
        # RSI å˜åŒ–ç‡
        df['RSI_ROC'] = df['RSI'].pct_change()

        # ========== MACD ==========
        df = self.tech_analyzer.calculate_macd(df)
        # MACD æŸ±çŠ¶å›¾
        df['MACD_Hist'] = df['MACD'] - df['MACD_signal']
        # MACD æŸ±çŠ¶å›¾å˜åŒ–ç‡
        df['MACD_Hist_ROC'] = df['MACD_Hist'].pct_change()

        # ========== å¸ƒæ—å¸¦ ==========
        df = self.tech_analyzer.calculate_bollinger_bands(df, period=20, std_dev=2)
        # å¸ƒæ—å¸¦å®½åº¦
        df['BB_Width'] = (df['BB_upper'] - df['BB_lower']) / df['BB_middle']
        # å¸ƒæ—å¸¦çªç ´
        df['BB_Breakout'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

        # ========== ATR ==========
        df = self.tech_analyzer.calculate_atr(df, period=14)
        # ATR æ¯”ç‡ï¼ˆATRç›¸å¯¹äº10æ—¥å‡çº¿çš„æ¯”ç‡ï¼‰
        df['ATR_MA'] = df['ATR'].rolling(window=10, min_periods=1).mean()
        df['ATR_Ratio'] = df['ATR'] / df['ATR_MA']

        # ========== æˆäº¤é‡ç›¸å…³ ==========
        df['Vol_MA20'] = df['Volume'].rolling(window=20, min_periods=1).mean()
        df['Vol_Ratio'] = df['Volume'] / df['Vol_MA20']
        # æˆäº¤é‡ z-score
        df['Vol_Mean_20'] = df['Volume'].rolling(20, min_periods=1).mean()
        df['Vol_Std_20'] = df['Volume'].rolling(20, min_periods=1).std()
        df['Vol_Z_Score'] = (df['Volume'] - df['Vol_Mean_20']) / df['Vol_Std_20']
        # æˆäº¤é¢
        df['Turnover'] = df['Close'] * df['Volume']
        # æˆäº¤é¢ z-score
        df['Turnover_Mean_20'] = df['Turnover'].rolling(20, min_periods=1).mean()
        df['Turnover_Std_20'] = df['Turnover'].rolling(20, min_periods=1).std()
        df['Turnover_Z_Score'] = (df['Turnover'] - df['Turnover_Mean_20']) / df['Turnover_Std_20']
        # æˆäº¤é¢å˜åŒ–ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Turnover_Change_1d'] = df['Turnover'].pct_change()
        df['Turnover_Change_5d'] = df['Turnover'].pct_change(5)
        df['Turnover_Change_10d'] = df['Turnover'].pct_change(10)
        df['Turnover_Change_20d'] = df['Turnover'].pct_change(20)
        # æ¢æ‰‹ç‡ï¼ˆå‡è®¾æ€»è‚¡æœ¬ä¸ºå¸¸æ•°ï¼Œè¿™é‡Œä½¿ç”¨æˆäº¤é¢/ä»·æ ¼ä½œä¸ºè¿‘ä¼¼ï¼‰
        df['Turnover_Rate'] = (df['Turnover'] / (df['Close'] * 1000000)) * 100
        # æ¢æ‰‹ç‡å˜åŒ–ç‡
        df['Turnover_Rate_Change_5d'] = df['Turnover_Rate'].pct_change(5)
        df['Turnover_Rate_Change_20d'] = df['Turnover_Rate'].pct_change(20)

        # ========== VWAP (æˆäº¤é‡åŠ æƒå¹³å‡ä»·) ==========
        df['TP'] = (df['High'] + df['Low'] + df['Close']) / 3
        df['VWAP'] = (df['TP'] * df['Volume']).rolling(window=20, min_periods=1).sum() / df['Volume'].rolling(window=20, min_periods=1).sum()

        # ========== OBV (èƒ½é‡æ½®) ==========
        df['OBV'] = 0.0
        for i in range(1, len(df)):
            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1] + df['Volume'].iloc[i]
            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1] - df['Volume'].iloc[i]
            else:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1]

        # ========== CMF (Chaikin Money Flow) ==========
        df['MF_Multiplier'] = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])
        df['MF_Volume'] = df['MF_Multiplier'] * df['Volume']
        df['CMF'] = df['MF_Volume'].rolling(20, min_periods=1).sum() / df['Volume'].rolling(20, min_periods=1).sum()
        # CMF ä¿¡å·çº¿
        df['CMF_Signal'] = df['CMF'].rolling(5, min_periods=1).mean()

        # ========== ADX (å¹³å‡è¶‹å‘æŒ‡æ•°) ==========
        # +DM and -DM
        up_move = df['High'].diff()
        down_move = -df['Low'].diff()
        df['+DM'] = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
        df['-DM'] = np.where((down_move > up_move) & (down_move > 0), down_move, 0)
        # +DI and -DI
        df['+DI'] = 100 * (df['+DM'].ewm(alpha=1/14, adjust=False).mean() / df['ATR'])
        df['-DI'] = 100 * (df['-DM'].ewm(alpha=1/14, adjust=False).mean() / df['ATR'])
        # ADX
        dx = 100 * (np.abs(df['+DI'] - df['-DI']) / (df['+DI'] + df['-DI']))
        df['ADX'] = dx.ewm(alpha=1/14, adjust=False).mean()

        # ========== éšæœºæŒ¯è¡å™¨ (Stochastic Oscillator) ==========
        K_Period = 14
        D_Period = 3
        df['Low_Min'] = df['Low'].rolling(window=K_Period, min_periods=1).min()
        df['High_Max'] = df['High'].rolling(window=K_Period, min_periods=1).max()
        df['Stoch_K'] = 100 * (df['Close'] - df['Low_Min']) / (df['High_Max'] - df['Low_Min'])
        df['Stoch_D'] = df['Stoch_K'].rolling(window=D_Period, min_periods=1).mean()

        # ========== Williams %R ==========
        df['Williams_R'] = (df['High_Max'] - df['Close']) / (df['High_Max'] - df['Low_Min']) * -100

        # ========== ROC (ä»·æ ¼å˜åŒ–ç‡) ==========
        df['ROC'] = df['Close'].pct_change(periods=12)

        # ========== æ³¢åŠ¨ç‡ï¼ˆå¹´åŒ–ï¼‰ ==========
        df['Returns'] = df['Close'].pct_change()
        df['Volatility'] = df['Returns'].rolling(20, min_periods=10).std() * np.sqrt(252)

        # ========== ä»·æ ¼ä½ç½®ç‰¹å¾ ==========
        # ä»·æ ¼ç›¸å¯¹äºå‡çº¿çš„åç¦»
        df['MA5_Deviation'] = (df['Close'] - df['MA5']) / df['MA5'] * 100
        df['MA10_Deviation'] = (df['Close'] - df['MA10']) / df['MA10'] * 100
        # ä»·æ ¼ç™¾åˆ†ä½ï¼ˆç›¸å¯¹äº60æ—¥çª—å£ï¼‰
        df['Price_Percentile'] = df['Close'].rolling(window=60, min_periods=1).apply(
            lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()) * 100
        )
        # å¸ƒæ—å¸¦ä½ç½®
        df['BB_Position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

        # ========== å¤šå‘¨æœŸæ”¶ç›Šç‡ ==========
        df['Return_1d'] = df['Close'].pct_change()
        df['Return_3d'] = df['Close'].pct_change(3)
        df['Return_5d'] = df['Close'].pct_change(5)
        df['Return_10d'] = df['Close'].pct_change(10)
        df['Return_20d'] = df['Close'].pct_change(20)
        df['Return_60d'] = df['Close'].pct_change(60)

        # ========== ä»·æ ¼ç›¸å¯¹äºå‡çº¿çš„æ¯”ç‡ ==========
        df['Price_Ratio_MA5'] = df['Close'] / df['MA5']
        df['Price_Ratio_MA20'] = df['Close'] / df['MA20']
        df['Price_Ratio_MA50'] = df['Close'] / df['MA50']

        # ========== é«˜ä¼˜å…ˆçº§ï¼šæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ ==========
        # å‡çº¿åç¦»åº¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
        df['MA5_Deviation_Std'] = (df['Close'] - df['MA5']) / df['Close'].rolling(5).std()
        df['MA20_Deviation_Std'] = (df['Close'] - df['MA20']) / df['Close'].rolling(20).std()

        # æ»šåŠ¨æ³¢åŠ¨ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Volatility_5d'] = df['Close'].pct_change().rolling(5).std()
        df['Volatility_10d'] = df['Close'].pct_change().rolling(10).std()
        df['Volatility_20d'] = df['Close'].pct_change().rolling(20).std()

        # æ»šåŠ¨ååº¦/å³°åº¦ï¼ˆä¸šç•Œå¸¸ç”¨ï¼‰
        df['Skewness_20d'] = df['Close'].pct_change().rolling(20).skew()
        df['Kurtosis_20d'] = df['Close'].pct_change().rolling(20).kurt()

        # åŠ¨é‡åŠ é€Ÿåº¦ï¼ˆä¸šç•Œé‡è¦ç‰¹å¾ï¼‰
        df['Momentum_Accel_5d'] = df['Return_5d'] - df['Return_5d'].shift(5)
        df['Momentum_Accel_10d'] = df['Return_10d'] - df['Return_10d'].shift(5)

        # ========== é«˜ä¼˜å…ˆçº§ï¼šä»·æ ¼å½¢æ€ç‰¹å¾ ==========
        # Næ—¥é«˜ä½ç‚¹ä½ç½®ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºåœ¨æœ€é«˜ç‚¹ï¼‰
        df['High_Position_20d'] = (df['Close'] - df['Low'].rolling(20).min()) / (df['High'].rolling(20).max() - df['Low'].rolling(20).min())
        df['High_Position_60d'] = (df['Close'] - df['Low'].rolling(60).min()) / (df['High'].rolling(60).max() - df['Low'].rolling(60).min())

        # è·ç¦»è¿‘æœŸé«˜ç‚¹/ä½ç‚¹çš„å¤©æ•°ï¼ˆä¸šç•Œå¸¸ç”¨ï¼‰
        df['Days_Since_High_20d'] = df['Close'].rolling(20).apply(lambda x: 20 - np.argmax(x), raw=False)
        df['Days_Since_Low_20d'] = df['Close'].rolling(20).apply(lambda x: 20 - np.argmin(x), raw=False)

        # æ—¥å†…ç‰¹å¾ï¼ˆä¸šç•Œæ ¸å¿ƒä¿¡å·ï¼‰
        df['Intraday_Range'] = (df['High'] - df['Low']) / df['Close']
        df['Intraday_Range_MA5'] = df['Intraday_Range'].rolling(5).mean()
        df['Intraday_Range_MA20'] = df['Intraday_Range'].rolling(20).mean()

        # æ”¶ç›˜ä½ç½®ï¼ˆé˜³çº¿/é˜´çº¿å¼ºåº¦ï¼Œ0-1ä¹‹é—´ï¼‰
        df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
        # ä¸Šå½±çº¿/ä¸‹å½±çº¿æ¯”ä¾‹
        df['Upper_Shadow'] = (df['High'] - df[['Close', 'Open']].max(axis=1)) / (df['High'] - df['Low'] + 1e-10)
        df['Lower_Shadow'] = (df[['Close', 'Open']].min(axis=1) - df['Low']) / (df['High'] - df['Low'] + 1e-10)

        # å¼€ç›˜ç¼ºå£
        df['Gap_Size'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)
        df['Gap_Up'] = (df['Gap_Size'] > 0.01).astype(int)  # è·³ç©ºé«˜å¼€ >1%
        df['Gap_Down'] = (df['Gap_Size'] < -0.01).astype(int)  # è·³ç©ºä½å¼€ >1%

        # ========== ä¸­ä¼˜å…ˆçº§ï¼šé‡ä»·å…³ç³»ç‰¹å¾ ==========
        # é‡ä»·èƒŒç¦»ï¼ˆä¸šç•Œé‡è¦ä¿¡å·ï¼‰
        df['Price_Up_Volume_Down'] = ((df['Return_1d'] > 0) & (df['Turnover'].pct_change() < 0)).astype(int)
        df['Price_Down_Volume_Up'] = ((df['Return_1d'] < 0) & (df['Turnover'].pct_change() > 0)).astype(int)

        # OBV è¶‹åŠ¿
        df['OBV_MA5'] = df['OBV'].rolling(5).mean()
        df['OBV_Trend'] = (df['OBV'] > df['OBV_MA5']).astype(int)

        # æˆäº¤é‡æ³¢åŠ¨ç‡
        df['Volume_Volatility'] = df['Turnover'].rolling(20).std() / (df['Turnover'].rolling(20).mean() + 1e-10)

        # æˆäº¤é‡æ¯”ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Volume_Ratio_5d'] = df['Volume'] / df['Volume'].rolling(5).mean()
        df['Volume_Ratio_20d'] = df['Volume'] / df['Volume'].rolling(20).mean()

        # ========== é•¿æœŸè¶‹åŠ¿ç‰¹å¾ï¼ˆä¸“é—¨ä¼˜åŒ–ä¸€ä¸ªæœˆæ¨¡å‹ï¼‰ ==========
        # é•¿æœŸå‡çº¿ï¼ˆ120æ—¥åŠå¹´çº¿ã€250æ—¥å¹´çº¿ï¼‰
        df['MA120'] = df['Close'].rolling(window=120, min_periods=1).mean()
        df['MA250'] = df['Close'].rolling(window=250, min_periods=1).mean()

        # ä»·æ ¼ç›¸å¯¹é•¿æœŸå‡çº¿çš„æ¯”ç‡ï¼ˆä¸šç•Œé•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼‰
        df['Price_Ratio_MA120'] = df['Close'] / df['MA120']
        df['Price_Ratio_MA250'] = df['Close'] / df['MA250']

        # é•¿æœŸæ”¶ç›Šç‡ï¼ˆä¸šç•Œæ ¸å¿ƒé•¿æœŸç‰¹å¾ï¼‰
        df['Return_120d'] = df['Close'].pct_change(120)
        df['Return_250d'] = df['Close'].pct_change(250)

        # é•¿æœŸåŠ¨é‡ï¼ˆMomentum = å½“å‰ä»·æ ¼ / Næ—¥å‰ä»·æ ¼ - 1ï¼‰
        df['Momentum_120d'] = df['Close'] / df['Close'].shift(120) - 1
        df['Momentum_250d'] = df['Close'] / df['Close'].shift(250) - 1

        # é•¿æœŸåŠ¨é‡åŠ é€Ÿåº¦ï¼ˆè¶‹åŠ¿å˜åŒ–çš„äºŒé˜¶å¯¼æ•°ï¼‰
        df['Momentum_Accel_120d'] = df['Return_120d'] - df['Return_120d'].shift(30)

        # é•¿æœŸå‡çº¿æ–œç‡ï¼ˆè¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡ï¼‰
        df['MA120_Slope'] = (df['MA120'] - df['MA120'].shift(10)) / df['MA120'].shift(10)
        df['MA250_Slope'] = (df['MA250'] - df['MA250'].shift(20)) / df['MA250'].shift(20)

        # é•¿æœŸå‡çº¿æ’åˆ—ï¼ˆå¤šå¤´/ç©ºå¤´/æ··ä¹±ï¼‰
        df['MA_Alignment_Long'] = np.where(
            (df['MA50'] > df['MA120']) & (df['MA120'] > df['MA250']), 1,  # å¤šå¤´æ’åˆ—
            np.where(
                (df['MA50'] < df['MA120']) & (df['MA120'] < df['MA250']), -1,  # ç©ºå¤´æ’åˆ—
                0  # æ··ä¹±æ’åˆ—
            )
        )

        # é•¿æœŸå‡çº¿ä¹–ç¦»ç‡ï¼ˆä»·æ ¼åç¦»é•¿æœŸå‡çº¿çš„ç¨‹åº¦ï¼‰
        df['MA120_Deviation'] = (df['Close'] - df['MA120']) / df['MA120'] * 100
        df['MA250_Deviation'] = (df['Close'] - df['MA250']) / df['MA250'] * 100

        # é•¿æœŸæ³¢åŠ¨ç‡ï¼ˆé£é™©æŒ‡æ ‡ï¼‰
        df['Volatility_60d'] = df['Close'].pct_change().rolling(60).std()
        df['Volatility_120d'] = df['Close'].pct_change().rolling(120).std()

        # é•¿æœŸATRï¼ˆé•¿æœŸé£é™©ï¼‰
        df['ATR_MA60'] = df['ATR'].rolling(60, min_periods=1).mean()
        df['ATR_MA120'] = df['ATR'].rolling(120, min_periods=1).mean()
        df['ATR_Ratio_60d'] = df['ATR'] / df['ATR_MA60']
        df['ATR_Ratio_120d'] = df['ATR'] / df['ATR_MA120']

        # é•¿æœŸæˆäº¤é‡è¶‹åŠ¿
        df['Volume_MA120'] = df['Volume'].rolling(120, min_periods=1).mean()
        df['Volume_MA250'] = df['Volume'].rolling(250, min_periods=1).mean()
        df['Volume_Ratio_120d'] = df['Volume'] / df['Volume_MA120']
        df['Volume_Trend_Long'] = np.where(
            df['Volume_MA120'] > df['Volume_MA250'], 1, -1
        )

        # é•¿æœŸæ”¯æ’‘é˜»åŠ›ä½ï¼ˆåŸºäº120æ—¥é«˜ä½ç‚¹ï¼‰
        df['Support_120d'] = df['Low'].rolling(120, min_periods=1).min()
        df['Resistance_120d'] = df['High'].rolling(120, min_periods=1).max()
        df['Distance_Support_120d'] = (df['Close'] - df['Support_120d']) / df['Close']
        df['Distance_Resistance_120d'] = (df['Resistance_120d'] - df['Close']) / df['Close']

        # é•¿æœŸRSIï¼ˆåŸºäº120æ—¥ï¼‰
        df['RSI_120'] = self.tech_analyzer.calculate_rsi(df.copy(), period=120)['RSI']

        return df

    def create_fundamental_features(self, code):
        """åˆ›å»ºåŸºæœ¬é¢ç‰¹å¾ï¼ˆåªä½¿ç”¨å®é™…å¯ç”¨çš„æ•°æ®ï¼‰"""
        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            fundamental_data = get_comprehensive_fundamental_data(stock_code)
            if fundamental_data:
                # åªä½¿ç”¨å®é™…å¯ç”¨çš„åŸºæœ¬é¢æ•°æ®
                return {
                    'PE': fundamental_data.get('fi_pe_ratio', np.nan),
                    'PB': fundamental_data.get('fi_pb_ratio', np.nan),
                    'Market_Cap': fundamental_data.get('fi_market_cap', np.nan),
                    'ROE': np.nan,  # æš‚ä¸å¯ç”¨
                    'ROA': np.nan,  # æš‚ä¸å¯ç”¨
                    'Dividend_Yield': np.nan,  # æš‚ä¸å¯ç”¨
                    'EPS': np.nan,  # æš‚ä¸å¯ç”¨
                    'Net_Margin': np.nan,  # æš‚ä¸å¯ç”¨
                    'Gross_Margin': np.nan  # æš‚ä¸å¯ç”¨
                }
        except Exception as e:
            print(f"è·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥ {code}: {e}")
        return {}

    def create_smart_money_features(self, df):
        """åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾"""
        if df.empty or len(df) < 50:
            return df

        # ä»·æ ¼ç›¸å¯¹ä½ç½®
        df['Price_Pct_20d'] = df['Close'].rolling(window=20).apply(lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()))

        # æ”¾é‡ä¸Šæ¶¨ä¿¡å·
        df['Strong_Volume_Up'] = (df['Close'] > df['Open']) & (df['Vol_Ratio'] > 1.5)

        # ç¼©é‡å›è°ƒä¿¡å·
        df['Prev_Close'] = df['Close'].shift(1)
        df['Weak_Volume_Down'] = (df['Close'] < df['Prev_Close']) & (df['Vol_Ratio'] < 1.0) & ((df['Prev_Close'] - df['Close']) / df['Prev_Close'] < 0.02)

        # åŠ¨é‡ä¿¡å·
        df['Momentum_5d'] = df['Close'] / df['Close'].shift(5) - 1
        df['Momentum_10d'] = df['Close'] / df['Close'].shift(10) - 1

        return df

    def create_stock_type_features(self, code, df):
        """åˆ›å»ºè‚¡ç¥¨ç±»å‹ç‰¹å¾ï¼ˆåŸºäºä¸šç•Œæƒ¯ä¾‹ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆç”¨äºè®¡ç®—æµåŠ¨æ€§ç­‰åŠ¨æ€ç‰¹å¾ï¼‰

        Returns:
            dict: è‚¡ç¥¨ç±»å‹ç‰¹å¾å­—å…¸
        """
        # è‚¡ç¥¨ç±»å‹åˆ†ç±»ï¼ˆåŸºäºä¸åŒè‚¡ç¥¨ç±»å‹åˆ†ææ¡†æ¶å¯¹æ¯”.mdï¼‰
        stock_type_mapping = {
            # é“¶è¡Œè‚¡
            '0005.HK': {'type': 'bank', 'name': 'æ±‡ä¸°é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 70, 'risk': 20},
            '0939.HK': {'type': 'bank', 'name': 'å»ºè®¾é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 80, 'risk': 20},
            '1288.HK': {'type': 'bank', 'name': 'å†œä¸šé“¶è¡Œ', 'defensive': 95, 'growth': 25, 'cyclical': 20, 'liquidity': 85, 'risk': 15},
            '1398.HK': {'type': 'bank', 'name': 'å·¥å•†é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 85, 'risk': 20},
            '3968.HK': {'type': 'bank', 'name': 'æ‹›å•†é“¶è¡Œ', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 25},

            # å…¬ç”¨äº‹ä¸šè‚¡
            '0728.HK': {'type': 'utility', 'name': 'ä¸­å›½ç”µä¿¡', 'defensive': 90, 'growth': 25, 'cyclical': 15, 'liquidity': 70, 'risk': 20},
            '0941.HK': {'type': 'utility', 'name': 'ä¸­å›½ç§»åŠ¨', 'defensive': 95, 'growth': 30, 'cyclical': 15, 'liquidity': 80, 'risk': 15},

            # ç§‘æŠ€è‚¡
            '0700.HK': {'type': 'tech', 'name': 'è…¾è®¯æ§è‚¡', 'defensive': 40, 'growth': 85, 'cyclical': 30, 'liquidity': 90, 'risk': 60},
            '9988.HK': {'type': 'tech', 'name': 'é˜¿é‡Œå·´å·´-SW', 'defensive': 35, 'growth': 85, 'cyclical': 35, 'liquidity': 85, 'risk': 65},
            '3690.HK': {'type': 'tech', 'name': 'ç¾å›¢-W', 'defensive': 30, 'growth': 80, 'cyclical': 40, 'liquidity': 85, 'risk': 70},
            '1810.HK': {'type': 'tech', 'name': 'å°ç±³é›†å›¢-W', 'defensive': 35, 'growth': 75, 'cyclical': 45, 'liquidity': 80, 'risk': 65},

            # åŠå¯¼ä½“è‚¡
            '0981.HK': {'type': 'semiconductor', 'name': 'ä¸­èŠ¯å›½é™…', 'defensive': 25, 'growth': 80, 'cyclical': 70, 'liquidity': 75, 'risk': 75},
            '1347.HK': {'type': 'semiconductor', 'name': 'åè™¹åŠå¯¼ä½“', 'defensive': 20, 'growth': 85, 'cyclical': 75, 'liquidity': 70, 'risk': 80},

            # äººå·¥æ™ºèƒ½è‚¡
            '6682.HK': {'type': 'ai', 'name': 'ç¬¬å››èŒƒå¼', 'defensive': 20, 'growth': 90, 'cyclical': 50, 'liquidity': 60, 'risk': 85},
            '9660.HK': {'type': 'ai', 'name': 'åœ°å¹³çº¿æœºå™¨äºº', 'defensive': 15, 'growth': 95, 'cyclical': 60, 'liquidity': 55, 'risk': 90},
            '2533.HK': {'type': 'ai', 'name': 'é»‘èŠéº»æ™ºèƒ½', 'defensive': 15, 'growth': 95, 'cyclical': 65, 'liquidity': 50, 'risk': 90},

            # æ–°èƒ½æºè‚¡
            '1211.HK': {'type': 'new_energy', 'name': 'æ¯”äºšè¿ªè‚¡ä»½', 'defensive': 30, 'growth': 85, 'cyclical': 60, 'liquidity': 80, 'risk': 70},
            '1330.HK': {'type': 'environmental', 'name': 'ç»¿è‰²åŠ¨åŠ›ç¯ä¿', 'defensive': 25, 'growth': 75, 'cyclical': 80, 'liquidity': 60, 'risk': 80},

            # èƒ½æº/å‘¨æœŸè‚¡
            '0883.HK': {'type': 'energy', 'name': 'ä¸­å›½æµ·æ´‹çŸ³æ²¹', 'defensive': 30, 'growth': 50, 'cyclical': 90, 'liquidity': 75, 'risk': 75},
            '1088.HK': {'type': 'energy', 'name': 'ä¸­å›½ç¥å', 'defensive': 40, 'growth': 45, 'cyclical': 85, 'liquidity': 70, 'risk': 70},
            '1138.HK': {'type': 'shipping', 'name': 'ä¸­è¿œæµ·èƒ½', 'defensive': 25, 'growth': 45, 'cyclical': 95, 'liquidity': 65, 'risk': 80},
            '0388.HK': {'type': 'exchange', 'name': 'é¦™æ¸¯äº¤æ˜“æ‰€', 'defensive': 25, 'growth': 50, 'cyclical': 90, 'liquidity': 70, 'risk': 75},

            # ä¿é™©è‚¡
            '1299.HK': {'type': 'insurance', 'name': 'å‹é‚¦ä¿é™©', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 30},

            # ç”Ÿç‰©åŒ»è¯è‚¡
            '2269.HK': {'type': 'biotech', 'name': 'è¯æ˜ç”Ÿç‰©', 'defensive': 30, 'growth': 80, 'cyclical': 55, 'liquidity': 70, 'risk': 70},

            # æˆ¿åœ°äº§è‚¡
            '0012.HK': {'type': 'real_estate', 'name': 'æ’åŸºåœ°äº§', 'defensive': 20, 'growth': 30, 'cyclical': 95, 'liquidity': 50, 'risk': 85},
            '0016.HK': {'type': 'real_estate', 'name': 'æ–°é¸¿åŸºåœ°äº§', 'defensive': 25, 'growth': 35, 'cyclical': 90, 'liquidity': 55, 'risk': 80},
            '1109.HK': {'type': 'real_estate', 'name': 'åæ¶¦ç½®åœ°', 'defensive': 30, 'growth': 40, 'cyclical': 85, 'liquidity': 60, 'risk': 75},

            # æŒ‡æ•°åŸºé‡‘
            '2800.HK': {'type': 'index', 'name': 'ç›ˆå¯ŒåŸºé‡‘', 'defensive': 80, 'growth': 40, 'cyclical': 30, 'liquidity': 90, 'risk': 25},
        }

        # è·å–è‚¡ç¥¨ç±»å‹ä¿¡æ¯
        stock_info_mapping = {
            # é“¶è¡Œè‚¡
            '0005.HK': {'type': 'bank', 'name': 'æ±‡ä¸°é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 70, 'risk': 20},
            '0388.HK': {'type': 'exchange', 'name': 'é¦™æ¸¯äº¤æ˜“æ‰€', 'defensive': 25, 'growth': 50, 'cyclical': 90, 'liquidity': 70, 'risk': 75},
            '0700.HK': {'type': 'tech', 'name': 'è…¾è®¯æ§è‚¡', 'defensive': 40, 'growth': 85, 'cyclical': 30, 'liquidity': 90, 'risk': 60},
            '0728.HK': {'type': 'utility', 'name': 'ä¸­å›½ç”µä¿¡', 'defensive': 90, 'growth': 25, 'cyclical': 15, 'liquidity': 70, 'risk': 20},
            '0883.HK': {'type': 'energy', 'name': 'ä¸­å›½æµ·æ´‹çŸ³æ²¹', 'defensive': 30, 'growth': 50, 'cyclical': 90, 'liquidity': 75, 'risk': 75},
            '0939.HK': {'type': 'bank', 'name': 'å»ºè®¾é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 80, 'risk': 20},
            '0941.HK': {'type': 'utility', 'name': 'ä¸­å›½ç§»åŠ¨', 'defensive': 95, 'growth': 30, 'cyclical': 15, 'liquidity': 80, 'risk': 15},
            '0981.HK': {'type': 'semiconductor', 'name': 'ä¸­èŠ¯å›½é™…', 'defensive': 25, 'growth': 80, 'cyclical': 70, 'liquidity': 75, 'risk': 75},
            '1088.HK': {'type': 'energy', 'name': 'ä¸­å›½ç¥å', 'defensive': 40, 'growth': 45, 'cyclical': 85, 'liquidity': 70, 'risk': 70},
            '1138.HK': {'type': 'shipping', 'name': 'ä¸­è¿œæµ·èƒ½', 'defensive': 25, 'growth': 45, 'cyclical': 95, 'liquidity': 65, 'risk': 80},
            '1211.HK': {'type': 'new_energy', 'name': 'æ¯”äºšè¿ªè‚¡ä»½', 'defensive': 30, 'growth': 85, 'cyclical': 60, 'liquidity': 80, 'risk': 70},
            '1288.HK': {'type': 'bank', 'name': 'å†œä¸šé“¶è¡Œ', 'defensive': 95, 'growth': 25, 'cyclical': 20, 'liquidity': 85, 'risk': 15},
            '1299.HK': {'type': 'insurance', 'name': 'å‹é‚¦ä¿é™©', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 30},
            '1330.HK': {'type': 'environmental', 'name': 'ç»¿è‰²åŠ¨åŠ›ç¯ä¿', 'defensive': 25, 'growth': 75, 'cyclical': 80, 'liquidity': 60, 'risk': 80},
            '1347.HK': {'type': 'semiconductor', 'name': 'åè™¹åŠå¯¼ä½“', 'defensive': 20, 'growth': 85, 'cyclical': 75, 'liquidity': 70, 'risk': 80},
            '1398.HK': {'type': 'bank', 'name': 'å·¥å•†é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 85, 'risk': 20},
            '1810.HK': {'type': 'tech', 'name': 'å°ç±³é›†å›¢-W', 'defensive': 35, 'growth': 75, 'cyclical': 45, 'liquidity': 80, 'risk': 65},
            '2269.HK': {'type': 'biotech', 'name': 'è¯æ˜ç”Ÿç‰©', 'defensive': 30, 'growth': 80, 'cyclical': 55, 'liquidity': 70, 'risk': 70},
            '2533.HK': {'type': 'ai', 'name': 'é»‘èŠéº»æ™ºèƒ½', 'defensive': 15, 'growth': 95, 'cyclical': 65, 'liquidity': 50, 'risk': 90},
            '2800.HK': {'type': 'index', 'name': 'ç›ˆå¯ŒåŸºé‡‘', 'defensive': 80, 'growth': 40, 'cyclical': 30, 'liquidity': 90, 'risk': 25},
            '3690.HK': {'type': 'tech', 'name': 'ç¾å›¢-W', 'defensive': 30, 'growth': 80, 'cyclical': 40, 'liquidity': 85, 'risk': 70},
            '3968.HK': {'type': 'bank', 'name': 'æ‹›å•†é“¶è¡Œ', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 25},
            '6682.HK': {'type': 'ai', 'name': 'ç¬¬å››èŒƒå¼', 'defensive': 20, 'growth': 90, 'cyclical': 50, 'liquidity': 60, 'risk': 85},
            '9660.HK': {'type': 'ai', 'name': 'åœ°å¹³çº¿æœºå™¨äºº', 'defensive': 15, 'growth': 95, 'cyclical': 60, 'liquidity': 55, 'risk': 90},
            '9988.HK': {'type': 'tech', 'name': 'é˜¿é‡Œå·´å·´-SW', 'defensive': 35, 'growth': 85, 'cyclical': 35, 'liquidity': 85, 'risk': 65},
            # æˆ¿åœ°äº§è‚¡
            '0012.HK': {'type': 'real_estate', 'name': 'æ’åŸºåœ°äº§', 'defensive': 20, 'growth': 30, 'cyclical': 95, 'liquidity': 50, 'risk': 85},
            '0016.HK': {'type': 'real_estate', 'name': 'æ–°é¸¿åŸºåœ°äº§', 'defensive': 25, 'growth': 35, 'cyclical': 90, 'liquidity': 55, 'risk': 80},
            '1109.HK': {'type': 'real_estate', 'name': 'åæ¶¦ç½®åœ°', 'defensive': 30, 'growth': 40, 'cyclical': 85, 'liquidity': 60, 'risk': 75},
        }

        # è·å–è‚¡ç¥¨ç±»å‹ä¿¡æ¯
        stock_info = stock_info_mapping.get(code, None)
        if not stock_info:
            logger.warning(f"æœªæ‰¾åˆ°è‚¡ç¥¨ {code} çš„ç±»å‹ä¿¡æ¯")
            return {}

        features = {
            # è‚¡ç¥¨ç±»å‹ç‰¹å¾ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰
            'Stock_Type': stock_info['type'],

            # ç»¼åˆè¯„åˆ†ç‰¹å¾ï¼ˆåŸºäºä¸šç•Œæƒ¯ä¾‹ï¼‰
            'Stock_Defensive_Score': stock_info['defensive'] / 100.0,  # é˜²å¾¡æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Growth_Score': stock_info['growth'] / 100.0,          # æˆé•¿æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Cyclical_Score': stock_info['cyclical'] / 100.0,        # å‘¨æœŸæ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Liquidity_Score': stock_info['liquidity'] / 100.0,      # æµåŠ¨æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Risk_Score': stock_info['risk'] / 100.0,                # é£é™©è¯„åˆ†ï¼ˆ0-1ï¼‰

            # è¡ç”Ÿç‰¹å¾ï¼ˆåŸºäºä¸šç•Œåˆ†ææƒé‡ï¼‰
            # é“¶è¡Œè‚¡ï¼šåŸºæœ¬é¢æƒé‡70%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡30%
            'Bank_Style_Fundamental_Weight': 0.7 if stock_info['type'] == 'bank' else 0.0,
            'Bank_Style_Technical_Weight': 0.3 if stock_info['type'] == 'bank' else 0.0,

            # ç§‘æŠ€è‚¡ï¼šåŸºæœ¬é¢æƒé‡40%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡60%
            'Tech_Style_Fundamental_Weight': 0.4 if stock_info['type'] == 'tech' else 0.0,
            'Tech_Style_Technical_Weight': 0.6 if stock_info['type'] == 'tech' else 0.0,

            # å‘¨æœŸè‚¡ï¼šåŸºæœ¬é¢æƒé‡10%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡70%ï¼Œèµ„é‡‘æµå‘æƒé‡20%
            'Cyclical_Style_Fundamental_Weight': 0.1 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,
            'Cyclical_Style_Technical_Weight': 0.7 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,
            'Cyclical_Style_Flow_Weight': 0.2 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,

            # æˆ¿åœ°äº§è‚¡ï¼šåŸºæœ¬é¢æƒé‡20%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡60%ï¼Œèµ„é‡‘æµå‘æƒé‡20%
            'RealEstate_Style_Fundamental_Weight': 0.2 if stock_info['type'] == 'real_estate' else 0.0,
            'RealEstate_Style_Technical_Weight': 0.6 if stock_info['type'] == 'real_estate' else 0.0,
            'RealEstate_Style_Flow_Weight': 0.2 if stock_info['type'] == 'real_estate' else 0.0,
        }

        # åŠ¨æ€ç‰¹å¾ï¼ˆåŸºäºå†å²æ•°æ®è®¡ç®—ï¼‰
        if df is not None and not df.empty and len(df) >= 60:
            # å†å²æ³¢åŠ¨ç‡ï¼ˆåŸºäº60æ—¥æ•°æ®ï¼‰
            returns = df['Close'].pct_change().dropna()
            if len(returns) >= 30:
                historical_volatility = returns.rolling(window=30, min_periods=10).std().iloc[-1]
                features['Stock_Historical_Volatility'] = historical_volatility

                # å®é™…æµåŠ¨æ€§è¯„åˆ†ï¼ˆåŸºäºæˆäº¤é¢æ³¢åŠ¨ï¼‰
                turnover_volatility = df['Turnover'].rolling(window=20, min_periods=10).std().iloc[-1] / df['Turnover'].rolling(window=20, min_periods=10).mean().iloc[-1]
                features['Stock_Actual_Liquidity_Score'] = max(0, min(1, 1 - turnover_volatility))

                # ä»·æ ¼ç¨³å®šæ€§è¯„åˆ†ï¼ˆåŸºäºä»·æ ¼æ³¢åŠ¨ï¼‰
                price_volatility = df['Close'].rolling(window=20, min_periods=10).std().iloc[-1] / df['Close'].rolling(window=20, min_periods=10).mean().iloc[-1]
                features['Stock_Price_Stability_Score'] = max(0, min(1, 1 - price_volatility))

        return features

    def calculate_multi_period_metrics(self, df):
        """è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡ï¼ˆè¶‹åŠ¿å’Œç›¸å¯¹å¼ºåº¦ï¼‰"""
        if df.empty or len(df) < 60:
            return df

        periods = [3, 5, 10, 20, 60]

        for period in periods:
            if len(df) < period:
                continue

            # è®¡ç®—æ”¶ç›Šç‡
            return_col = f'Return_{period}d'
            if return_col in df.columns:
                # è®¡ç®—è¶‹åŠ¿æ–¹å‘ï¼ˆ1=ä¸Šæ¶¨ï¼Œ0=ä¸‹è·Œï¼‰
                trend_col = f'{period}d_Trend'
                df[trend_col] = (df[return_col] > 0).astype(int)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦ä¿¡å·ï¼ˆåŸºäºæ”¶ç›Šç‡ï¼‰
                rs_signal_col = f'{period}d_RS_Signal'
                df[rs_signal_col] = (df[return_col] > 0).astype(int)

        # è®¡ç®—å¤šå‘¨æœŸè¶‹åŠ¿è¯„åˆ†
        trend_cols = [f'{p}d_Trend' for p in periods]
        if all(col in df.columns for col in trend_cols):
            df['Multi_Period_Trend_Score'] = df[trend_cols].sum(axis=1)

        # è®¡ç®—å¤šå‘¨æœŸç›¸å¯¹å¼ºåº¦è¯„åˆ†
        rs_cols = [f'{p}d_RS_Signal' for p in periods]
        if all(col in df.columns for col in rs_cols):
            df['Multi_Period_RS_Score'] = df[rs_cols].sum(axis=1)

        return df

    def calculate_relative_strength(self, stock_df, hsi_df):
        """è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡ï¼ˆç›¸å¯¹äºæ’ç”ŸæŒ‡æ•°ï¼‰"""
        if stock_df.empty or hsi_df.empty:
            return stock_df

        # ç¡®ä¿ç´¢å¼•å¯¹é½
        stock_df = stock_df.copy()
        hsi_df = hsi_df.copy()

        # è®¡ç®—æ’ç”ŸæŒ‡æ•°æ”¶ç›Šç‡
        hsi_df['HSI_Return_1d'] = hsi_df['Close'].pct_change()
        hsi_df['HSI_Return_3d'] = hsi_df['Close'].pct_change(3)
        hsi_df['HSI_Return_5d'] = hsi_df['Close'].pct_change(5)
        hsi_df['HSI_Return_10d'] = hsi_df['Close'].pct_change(10)
        hsi_df['HSI_Return_20d'] = hsi_df['Close'].pct_change(20)
        hsi_df['HSI_Return_60d'] = hsi_df['Close'].pct_change(60)

        # åˆå¹¶æ’ç”ŸæŒ‡æ•°æ•°æ®
        hsi_cols = ['HSI_Return_1d', 'HSI_Return_3d', 'HSI_Return_5d', 'HSI_Return_10d', 'HSI_Return_20d', 'HSI_Return_60d']
        stock_df = stock_df.merge(hsi_df[hsi_cols], left_index=True, right_index=True, how='left')

        # è®¡ç®—ç›¸å¯¹å¼ºåº¦ï¼ˆRS_ratio = (1+stock_ret)/(1+hsi_ret)-1ï¼‰
        periods = [1, 3, 5, 10, 20, 60]
        for period in periods:
            stock_ret_col = f'Return_{period}d'
            hsi_ret_col = f'HSI_Return_{period}d'

            if stock_ret_col in stock_df.columns and hsi_ret_col in stock_df.columns:
                # RS_ratioï¼ˆå¤åˆæ”¶ç›Šæ¯”ï¼‰
                rs_ratio_col = f'RS_Ratio_{period}d'
                stock_df[rs_ratio_col] = (1 + stock_df[stock_ret_col]) / (1 + stock_df[hsi_ret_col]) - 1

                # RS_diffï¼ˆæ”¶ç›Šå·®å€¼ï¼‰
                rs_diff_col = f'RS_Diff_{period}d'
                stock_df[rs_diff_col] = stock_df[stock_ret_col] - stock_df[hsi_ret_col]

        # è·‘èµ¢æ’æŒ‡ï¼ˆåŸºäº5æ—¥ç›¸å¯¹å¼ºåº¦ï¼‰
        if 'RS_Ratio_5d' in stock_df.columns:
            stock_df['Outperforms_HSI'] = (stock_df['RS_Ratio_5d'] > 0).astype(int)

        return stock_df

    def create_market_environment_features(self, stock_df, hsi_df, us_market_df=None):
        """åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰

        Args:
            stock_df: è‚¡ç¥¨æ•°æ®
            hsi_df: æ’ç”ŸæŒ‡æ•°æ•°æ®
            us_market_df: ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        if stock_df.empty or hsi_df.empty:
            return stock_df

        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ HSI_Return_5d åˆ—ï¼ˆç”± calculate_relative_strength åˆ›å»ºï¼‰
        if 'HSI_Return_5d' not in stock_df.columns:
            # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå¹¶åˆå¹¶
            hsi_df = hsi_df.copy()
            hsi_df['HSI_Return'] = hsi_df['Close'].pct_change()
            hsi_df['HSI_Return_5d'] = hsi_df['Close'].pct_change(5)
            stock_df = stock_df.merge(hsi_df[['HSI_Return', 'HSI_Return_5d']], left_index=True, right_index=True, how='left')

        # ç›¸å¯¹è¡¨ç°ï¼ˆç›¸å¯¹äºæ’ç”ŸæŒ‡æ•°ï¼‰
        stock_df['Relative_Return'] = stock_df['Return_5d'] - stock_df['HSI_Return_5d']

        # å¦‚æœæä¾›äº†ç¾è‚¡æ•°æ®ï¼Œåˆå¹¶ç¾è‚¡ç‰¹å¾
        if us_market_df is not None and not us_market_df.empty:
            # ç¾è‚¡ç‰¹å¾åˆ—
            us_features = [
                'SP500_Return', 'SP500_Return_5d', 'SP500_Return_20d',
                'NASDAQ_Return', 'NASDAQ_Return_5d', 'NASDAQ_Return_20d',
                'VIX_Change', 'VIX_Ratio_MA20', 'VIX_Level',
                'US_10Y_Yield', 'US_10Y_Yield_Change'
            ]

            # åªåˆå¹¶å­˜åœ¨çš„ç‰¹å¾
            existing_us_features = [f for f in us_features if f in us_market_df.columns]
            if existing_us_features:
                # å¯¹ç¾è‚¡ç‰¹å¾è¿›è¡Œ shift(1)ï¼Œç¡®ä¿ä¸åŒ…å«æœªæ¥ä¿¡æ¯
                # å› ä¸ºç¾è‚¡æ•°æ®æ¯”æ¸¯è‚¡æ™š15å°æ—¶å¼€ç›˜ï¼Œæ‰€ä»¥åœ¨é¢„æµ‹æ¸¯è‚¡ T+1 æ—¥æ¶¨è·Œæ—¶ï¼Œ
                # åªèƒ½ä½¿ç”¨ T æ—¥åŠä¹‹å‰çš„ç¾è‚¡æ•°æ®
                us_market_df_shifted = us_market_df[existing_us_features].shift(1)

                stock_df = stock_df.merge(
                    us_market_df_shifted,
                    left_index=True, right_index=True, how='left'
                )

        return stock_df

    def create_label(self, df, horizon, for_backtest=False):
        """åˆ›å»ºæ ‡ç­¾ï¼šæ¬¡æ—¥æ¶¨è·Œ
        
        Args:
            df: è‚¡ç¥¨æ•°æ®
            horizon: é¢„æµ‹å‘¨æœŸ
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸ç§»é™¤æœ€åhorizonè¡Œï¼‰
        """
        if df.empty or len(df) < horizon + 1:
            return df

        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
        df['Future_Return'] = df['Close'].shift(-horizon) / df['Close'] - 1

        # äºŒåˆ†ç±»æ ‡ç­¾ï¼š1=ä¸Šæ¶¨ï¼Œ0=ä¸‹è·Œ
        df['Label'] = (df['Future_Return'] > 0).astype(int)

        # å¦‚æœä¸æ˜¯å›æµ‹æ¨¡å¼ï¼Œç§»é™¤æœ€åhorizonè¡Œï¼ˆæ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ï¼‰
        if not for_backtest:
            df = df.iloc[:-horizon]

        return df

    def create_technical_fundamental_interactions(self, df):
        """åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢çš„äº¤äº’ç‰¹å¾

        æ ¹æ®ä¸šç•Œæœ€ä½³å®è·µï¼ŒæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢çš„äº¤äº’èƒ½å¤Ÿæ•æ‰éçº¿æ€§å…³ç³»ï¼Œ
        æé«˜æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡ã€‚å‚è€ƒï¼šarXiv 2025è®ºæ–‡ã€é‡åŒ–äº¤æ˜“æœ€ä½³å®è·µã€‚

        äº¤äº’ç‰¹å¾åˆ—è¡¨ï¼š
        1. RSI Ã— PEï¼šè¶…å–+ä½ä¼°=å¼ºåŠ›ä¹°å…¥ï¼Œè¶…ä¹°+é«˜ä¼°=å¼ºåŠ›å–å‡º
        2. RSI Ã— PBï¼šè¶…å–+ä½ä¼°å€¼=ä»·å€¼æœºä¼š
        3. MACD Ã— ROEï¼šè¶‹åŠ¿å‘ä¸Š+é«˜ç›ˆåˆ©èƒ½åŠ›=å¼ºåŠ²å¢é•¿
        4. MACD_Hist Ã— ROEï¼šåŠ¨èƒ½å¢å¼º+ç›ˆåˆ©èƒ½åŠ›å¼º=åŠ é€Ÿä¸Šæ¶¨
        5. BB_Position Ã— Dividend_Yieldï¼šä¸‹è½¨é™„è¿‘+é«˜è‚¡æ¯=é˜²å®ˆä»·å€¼
        6. Price_Pct_20d Ã— PEï¼šä½ä½+ä½ä¼°=è¶…è·Œåå¼¹
        7. Price_Pct_20d Ã— PBï¼šä½ä½+ä½ä¼°å€¼=ä»·å€¼ä¿®å¤
        8. Price_Pct_20d Ã— ROEï¼šä½ä½+é«˜ç›ˆåˆ©=é”™æ€æœºä¼š
        9. ATR Ã— PEï¼šé«˜æ³¢åŠ¨+ä½ä¼°=é«˜é£é™©é«˜å›æŠ¥
        10. ATR Ã— ROEï¼šé«˜æ³¢åŠ¨+é«˜ç›ˆåˆ©=æˆé•¿æ½œåŠ›
        11. Vol_Ratio Ã— PEï¼šæ”¾é‡+ä½ä¼°=èµ„é‡‘æµå…¥ä»·å€¼è‚¡
        12. OBV_Slope Ã— ROEï¼šèµ„é‡‘æµå…¥+é«˜ç›ˆåˆ©=åŸºæœ¬é¢é©±åŠ¨ä¸Šæ¶¨
        13. CMF Ã— Dividend_Yieldï¼šèµ„é‡‘æµå…¥+é«˜è‚¡æ¯=é˜²å¾¡æ€§ä¹°å…¥
        14. Return_5d Ã— PEï¼šçŸ­æœŸä¸Šæ¶¨+ä½ä¼°å€¼=å¯æŒç»­ä¸Šæ¶¨
        15. Return_5d Ã— ROEï¼šçŸ­æœŸä¸Šæ¶¨+é«˜ç›ˆåˆ©=ç›ˆåˆ©ç¡®è®¤
        """
        if df.empty:
            return df

        # åŸºæœ¬é¢ç‰¹å¾åˆ—è¡¨ï¼ˆåªä½¿ç”¨å®é™…å¯ç”¨çš„ï¼‰
        fundamental_features = ['PE', 'PB']  # ç›®å‰åªæ”¯æŒPEå’ŒPB

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—åï¼‰
        technical_features = ['RSI', 'RSI_ROC', 'MACD', 'MACD_Hist', 'MACD_Hist_ROC',
                             'BB_Position', 'ATR', 'Vol_Ratio', 'CMF',
                             'Return_5d', 'Price_Pct_20d', 'Momentum_5d']

        # é¢„å®šä¹‰çš„é«˜ä»·å€¼äº¤äº’ç»„åˆï¼ˆåŸºäºä¸šç•Œå®è·µï¼Œåªä½¿ç”¨å®é™…å¯ç”¨çš„åŸºæœ¬é¢ç‰¹å¾ï¼‰
        high_value_interactions = [
            # è¶…ä¹°è¶…å–ä¸ä¼°å€¼çš„äº¤äº’
            ('RSI', 'PE'),           # RSI Ã— PE
            ('RSI', 'PB'),           # RSI Ã— PB
            # è¶‹åŠ¿ä¸ä¼°å€¼çš„äº¤äº’
            ('MACD', 'PE'),         # MACD Ã— PE
            ('MACD', 'PB'),         # MACD Ã— PB
            ('MACD_Hist', 'PE'),    # MACDæŸ±çŠ¶å›¾ Ã— PE
            ('MACD_Hist', 'PB'),    # MACDæŸ±çŠ¶å›¾ Ã— PB
            # ä½ç½®ä¸ä¼°å€¼çš„äº¤äº’
            ('Price_Pct_20d', 'PE'), # ä»·æ ¼ä½ç½® Ã— PE
            ('Price_Pct_20d', 'PB'), # ä»·æ ¼ä½ç½® Ã— PB
            # æ³¢åŠ¨ä¸ä¼°å€¼çš„äº¤äº’
            ('ATR', 'PE'),           # ATR Ã— PE
            ('ATR', 'PB'),           # ATR Ã— PB
            # æˆäº¤é‡ä¸ä¼°å€¼çš„äº¤äº’
            ('Vol_Ratio', 'PE'),     # æˆäº¤é‡æ¯”ç‡ Ã— PE
            ('Vol_Ratio', 'PB'),     # æˆäº¤é‡æ¯”ç‡ Ã— PB
            # èµ„é‡‘æµä¸ä¼°å€¼çš„äº¤äº’
            ('CMF', 'PE'),           # CMF Ã— PE
            ('CMF', 'PB'),           # CMF Ã— PB
            # æ”¶ç›Šä¸ä¼°å€¼çš„äº¤äº’
            ('Return_5d', 'PE'),     # 5æ—¥æ”¶ç›Š Ã— PE
            ('Return_5d', 'PB'),     # 5æ—¥æ”¶ç›Š Ã— PB
            # åŠ¨é‡ä¸ä¼°å€¼çš„äº¤äº’
            ('Momentum_5d', 'PE'),   # 5æ—¥åŠ¨é‡ Ã— PE
            ('Momentum_5d', 'PB'),   # 5æ—¥åŠ¨é‡ Ã— PB
        ]

        print(f"ğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")

        interaction_count = 0
        for tech_feat, fund_feat in high_value_interactions:
            if tech_feat in df.columns and fund_feat in df.columns:
                # äº¤äº’ç‰¹å¾å‘½åï¼šæŠ€æœ¯_åŸºæœ¬é¢
                interaction_name = f"{tech_feat}_{fund_feat}"
                df[interaction_name] = df[tech_feat] * df[fund_feat]
                interaction_count += 1

        logger.info(f"æˆåŠŸç”Ÿæˆ {interaction_count} ä¸ªæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾")

        # åˆ é™¤æ‰€æœ‰å€¼å…¨ä¸ºNaNçš„äº¤äº’ç‰¹å¾ï¼ˆåŸºæœ¬é¢æ•°æ®ä¸å¯ç”¨å¯¼è‡´çš„ï¼‰
        interaction_cols = [col for col in df.columns if any(sub in col for sub in ['_PE', '_PB', '_ROE', '_ROA', '_Dividend_Yield', '_EPS', '_Net_Margin', '_Gross_Margin'])]
        cols_to_drop = [col for col in interaction_cols if df[col].isnull().all()]
        if cols_to_drop:
            df = df.drop(columns=cols_to_drop)
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_to_drop)} ä¸ªå…¨ä¸ºNaNçš„äº¤äº’ç‰¹å¾")

        return df

    def create_sentiment_features(self, code, df):
        """åˆ›å»ºæƒ…æ„ŸæŒ‡æ ‡ç‰¹å¾ï¼ˆå‚è€ƒ hk_smart_money_tracker.pyï¼‰

        ä»æ–°é—»æ•°æ®ä¸­è®¡ç®—æƒ…æ„Ÿè¶‹åŠ¿ç‰¹å¾ï¼š
        - sentiment_ma3: 3æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆçŸ­æœŸæƒ…ç»ªï¼‰
        - sentiment_ma7: 7æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆä¸­æœŸæƒ…ç»ªï¼‰
        - sentiment_ma14: 14æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆé•¿æœŸæƒ…ç»ªï¼‰
        - sentiment_volatility: æƒ…æ„Ÿæ³¢åŠ¨ç‡ï¼ˆæƒ…ç»ªç¨³å®šæ€§ï¼‰
        - sentiment_change_rate: æƒ…æ„Ÿå˜åŒ–ç‡ï¼ˆæƒ…ç»ªå˜åŒ–æ–¹å‘ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«æƒ…æ„Ÿç‰¹å¾çš„å­—å…¸
        """
        try:
            # è¯»å–æ–°é—»æ•°æ®
            news_file_path = 'data/all_stock_news_records.csv'
            if not os.path.exists(news_file_path):
                # æ²¡æœ‰æ–°é—»æ–‡ä»¶ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if self._news_data_cache is None:
                self._news_data_cache = pd.read_csv(news_file_path)
                # åˆ›å»º'æ–‡æœ¬'åˆ—ï¼ˆåˆå¹¶æ ‡é¢˜å’Œå†…å®¹ï¼‰
                self._news_data_cache['æ–‡æœ¬'] = self._news_data_cache['æ–°é—»æ ‡é¢˜'].astype(str) + ' ' + self._news_data_cache['ç®€è¦å†…å®¹'].astype(str)
            
            news_df = self._news_data_cache
            if news_df.empty:
                # æ–°é—»æ–‡ä»¶ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # ç­›é€‰è¯¥è‚¡ç¥¨çš„æ–°é—»
            stock_news = news_df[news_df['è‚¡ç¥¨ä»£ç '] == code].copy()
            if stock_news.empty:
                # è¯¥è‚¡ç¥¨æ²¡æœ‰æ–°é—»ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            stock_news['æ–°é—»æ—¶é—´'] = pd.to_datetime(stock_news['æ–°é—»æ—¶é—´'])

            # åªä½¿ç”¨å·²åˆ†ææƒ…æ„Ÿåˆ†æ•°çš„æ–°é—»
            stock_news = stock_news[stock_news['æƒ…æ„Ÿåˆ†æ•°'].notna()].copy()
            if stock_news.empty:
                # æ²¡æœ‰æƒ…æ„Ÿåˆ†æ•°æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            stock_news = stock_news.sort_values('æ–°é—»æ—¶é—´')

            # æŒ‰æ—¥æœŸèšåˆæƒ…æ„Ÿåˆ†æ•°ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼‰
            sentiment_by_date = stock_news.groupby('æ–°é—»æ—¶é—´')['æƒ…æ„Ÿåˆ†æ•°'].mean()

            # è·å–å®é™…æ•°æ®å¤©æ•°
            actual_days = len(sentiment_by_date)

            # åŠ¨æ€è°ƒæ•´ç§»åŠ¨å¹³å‡çª—å£
            window_ma3 = min(3, actual_days)
            window_ma7 = min(7, actual_days)
            window_ma14 = min(14, actual_days)
            window_volatility = min(14, actual_days)

            # è®¡ç®—ç§»åŠ¨å¹³å‡
            sentiment_ma3 = sentiment_by_date.rolling(window=window_ma3, min_periods=1).mean().iloc[-1]
            sentiment_ma7 = sentiment_by_date.rolling(window=window_ma7, min_periods=1).mean().iloc[-1]
            sentiment_ma14 = sentiment_by_date.rolling(window=window_ma14, min_periods=1).mean().iloc[-1]

            # è®¡ç®—æ³¢åŠ¨ç‡
            sentiment_volatility = sentiment_by_date.rolling(window=window_volatility, min_periods=2).std().iloc[-1] if actual_days >= 2 else np.nan

            # è®¡ç®—å˜åŒ–ç‡
            if actual_days >= 2:
                latest_sentiment = sentiment_by_date.iloc[-1]
                prev_sentiment = sentiment_by_date.iloc[-2]
                sentiment_change_rate = (latest_sentiment - prev_sentiment) / abs(prev_sentiment) if prev_sentiment != 0 else np.nan
            else:
                sentiment_change_rate = np.nan

            return {
                'sentiment_ma3': sentiment_ma3,
                'sentiment_ma7': sentiment_ma7,
                'sentiment_ma14': sentiment_ma14,
                'sentiment_volatility': sentiment_volatility,
                'sentiment_change_rate': sentiment_change_rate,
                'sentiment_days': actual_days
            }

        except Exception as e:
            logger.warning(f"è®¡ç®—æƒ…æ„Ÿç‰¹å¾å¤±è´¥ {code}: {e}")
            # å¼‚å¸¸æƒ…å†µè¿”å›é»˜è®¤å€¼
            return {
                'sentiment_ma3': 0.0,
                'sentiment_ma7': 0.0,
                'sentiment_ma14': 0.0,
                'sentiment_volatility': 0.0,
                'sentiment_change_rate': 0.0,
                'sentiment_days': 0
            }

    def create_topic_features(self, code, df):
        """åˆ›å»ºä¸»é¢˜åˆ†å¸ƒç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰

        ä»æ–°é—»æ•°æ®ä¸­æå–ä¸»é¢˜åˆ†å¸ƒç‰¹å¾ï¼š
        - Topic_1 ~ Topic_10: 10ä¸ªä¸»é¢˜çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆ0-1ä¹‹é—´ï¼Œæ€»å’Œä¸º1ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«ä¸»é¢˜ç‰¹å¾çš„å­—å…¸
        """
        try:
            from ml_services.topic_modeling import TopicModeler

            # åˆ›å»ºä¸»é¢˜å»ºæ¨¡å™¨
            topic_modeler = TopicModeler(n_topics=10, language='mixed')

            # å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
            model_path = 'data/lda_topic_model.pkl'

            if os.path.exists(model_path):
                topic_modeler.load_model(model_path)

                # ä½¿ç”¨ç¼“å­˜çš„æ–°é—»æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if self._news_data_cache is None:
                    self._news_data_cache = topic_modeler.load_news_data(days=self._news_data_days)
                
                # æ£€æŸ¥æ–°é—»æ•°æ®æ˜¯å¦æœ‰æ•ˆ
                if self._news_data_cache is None:
                    logger.warning(f" æ–°é—»æ•°æ®åŠ è½½å¤±è´¥ï¼ˆè¿”å›Noneï¼‰")
                    return {f'Topic_{i+1}': 0.0 for i in range(10)}
                
                if len(self._news_data_cache) == 0:
                    logger.warning(f" æ–°é—»æ•°æ®ä¸ºç©º")
                    return {f'Topic_{i+1}': 0.0 for i in range(10)}
                
                if 'æ–‡æœ¬' not in self._news_data_cache.columns:
                    logger.warning(f" æ–°é—»æ•°æ®ç¼ºå°‘'æ–‡æœ¬'åˆ—ï¼Œå¯ç”¨åˆ—: {self._news_data_cache.columns.tolist()}")
                    return {f'Topic_{i+1}': 0.0 for i in range(10)}
                
                # è·å–è‚¡ç¥¨ä¸»é¢˜ç‰¹å¾
                topic_features = topic_modeler.get_stock_topic_features(code, self._news_data_cache)

                if topic_features:
                    return topic_features
                else:
                    return {f'Topic_{i+1}': 0.0 for i in range(10)}
            else:
                logger.warning(f" ä¸»é¢˜æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ: python ml_services/topic_modeling.py")
                return {f'Topic_{i+1}': 0.0 for i in range(10)}

        except Exception as e:
            import traceback
            logger.error(f"åˆ›å»ºä¸»é¢˜ç‰¹å¾å¤±è´¥ {code}: {e}")
            print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯:\n{traceback.format_exc()}")
            return {f'Topic_{i+1}': 0.0 for i in range(10)}

    def create_topic_sentiment_interaction_features(self, code, df):
        """åˆ›å»ºä¸»é¢˜ä¸æƒ…æ„Ÿäº¤äº’ç‰¹å¾

        å°†ä¸»é¢˜åˆ†å¸ƒä¸æƒ…æ„Ÿè¯„åˆ†è¿›è¡Œäº¤äº’ï¼Œæ•æ‰"æŸä¸ªä¸»é¢˜çš„æ–°é—»å¸¦æœ‰æŸç§æƒ…æ„Ÿæ—¶"çš„ç‰¹å®šæ•ˆæœï¼š
        - Topic_1 Ã— sentiment_ma3: ä¸»é¢˜1ä¸3æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_ma7: ä¸»é¢˜1ä¸7æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_ma14: ä¸»é¢˜1ä¸14æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_volatility: ä¸»é¢˜1ä¸æƒ…æ„Ÿæ³¢åŠ¨ç‡çš„äº¤äº’
        - Topic_1 Ã— sentiment_change_rate: ä¸»é¢˜1ä¸æƒ…æ„Ÿå˜åŒ–ç‡çš„äº¤äº’
        - ... å…±10ä¸ªä¸»é¢˜ Ã— 5ä¸ªæƒ…æ„ŸæŒ‡æ ‡ = 50ä¸ªäº¤äº’ç‰¹å¾

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–ä¸»é¢˜ç‰¹å¾
            topic_features = self.create_topic_features(code, df)

            # è·å–æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.create_sentiment_features(code, df)

            # åˆ›å»ºäº¤äº’ç‰¹å¾
            interaction_features = {}

            # æƒ…æ„ŸæŒ‡æ ‡åˆ—è¡¨
            sentiment_keys = ['sentiment_ma3', 'sentiment_ma7', 'sentiment_ma14',
                            'sentiment_volatility', 'sentiment_change_rate']

            # ä¸ºæ¯ä¸ªä¸»é¢˜ä¸æ¯ä¸ªæƒ…æ„ŸæŒ‡æ ‡åˆ›å»ºäº¤äº’ç‰¹å¾
            for topic_idx in range(10):
                topic_key = f'Topic_{topic_idx + 1}'
                topic_prob = topic_features.get(topic_key, 0.0)

                for sentiment_key in sentiment_keys:
                    sentiment_value = sentiment_features.get(sentiment_key, 0.0)

                    # äº¤äº’ç‰¹å¾ = ä¸»é¢˜æ¦‚ç‡ Ã— æƒ…æ„Ÿå€¼
                    interaction_key = f'{topic_key}_x_{sentiment_key}'
                    interaction_features[interaction_key] = topic_prob * sentiment_value

            if interaction_features:
                logger.info(f"è·å–ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾: {code} (å…±{len(interaction_features)}ä¸ª)")
                return interaction_features
            else:
                logger.warning(f" æ— æ³•åˆ›å»ºä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾: {code}")
                return {}

        except Exception as e:
            logger.error(f"åˆ›å»ºä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾å¤±è´¥ {code}: {e}")
            return {}

    def create_expectation_gap_features(self, code, df):
        """åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾

        è®¡ç®—æ–°é—»æƒ…æ„Ÿç›¸å¯¹äºå¸‚åœºé¢„æœŸçš„å·®è·ï¼š
        - Sentiment_Gap_MA7: å½“å‰æƒ…æ„Ÿä¸7æ—¥ç§»åŠ¨å¹³å‡çš„å·®è·
        - Sentiment_Gap_MA14: å½“å‰æƒ…æ„Ÿä¸14æ—¥ç§»åŠ¨å¹³å‡çš„å·®è·
        - Positive_Surprise: æ­£å‘æ„å¤–ï¼ˆæƒ…æ„Ÿè¶…è¿‡é¢„æœŸçš„ç¨‹åº¦ï¼‰
        - Negative_Surprise: è´Ÿå‘æ„å¤–ï¼ˆæƒ…æ„Ÿä½äºé¢„æœŸçš„ç¨‹åº¦ï¼‰
        - Expectation_Change_Strength: é¢„æœŸå˜åŒ–å¼ºåº¦

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«é¢„æœŸå·®è·ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.create_sentiment_features(code, df)

            # åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾
            expectation_gap_features = {}

            # è·å–å½“å‰æƒ…æ„Ÿå€¼ï¼ˆä½¿ç”¨æœ€æ–°çš„æƒ…æ„Ÿå€¼ï¼‰
            current_sentiment = sentiment_features.get('sentiment_ma3', 0.0)

            # è®¡ç®—ä¸ä¸åŒå‘¨æœŸç§»åŠ¨å¹³å‡çš„å·®è·
            ma7 = sentiment_features.get('sentiment_ma7', 0.0)
            ma14 = sentiment_features.get('sentiment_ma14', 0.0)

            # é¢„æœŸå·®è· = å½“å‰æƒ…æ„Ÿ - é•¿æœŸç§»åŠ¨å¹³å‡
            expectation_gap_features['Sentiment_Gap_MA7'] = current_sentiment - ma7
            expectation_gap_features['Sentiment_Gap_MA14'] = current_sentiment - ma14

            # æ­£å‘æ„å¤–ï¼ˆæƒ…æ„Ÿè¶…é¢„æœŸï¼Œå·®è·ä¸ºæ­£ï¼‰
            expectation_gap_features['Positive_Surprise'] = max(0, current_sentiment - ma14)

            # è´Ÿå‘æ„å¤–ï¼ˆæƒ…æ„Ÿä¸åŠé¢„æœŸï¼Œå·®è·ä¸ºè´Ÿï¼Œå–ç»å¯¹å€¼ï¼‰
            expectation_gap_features['Negative_Surprise'] = max(0, ma14 - current_sentiment)

            # ä½¿ç”¨æƒ…æ„Ÿå˜åŒ–ç‡æ¥è¡¡é‡é¢„æœŸå·®è·çš„å¼ºåº¦
            sentiment_change_rate = sentiment_features.get('sentiment_change_rate', 0.0)
            expectation_gap_features['Expectation_Change_Strength'] = abs(sentiment_change_rate)

            if expectation_gap_features:
                logger.info(f"è·å–é¢„æœŸå·®è·ç‰¹å¾: {code} (å…±{len(expectation_gap_features)}ä¸ª)")
                return expectation_gap_features
            else:
                logger.warning(f" æ— æ³•åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾: {code}")
                return {}

        except Exception as e:
            logger.error(f"åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾å¤±è´¥ {code}: {e}")
            return {}

    def create_sector_features(self, code, df):
        """åˆ›å»ºæ¿å—åˆ†æç‰¹å¾ï¼ˆä¼˜åŒ–ç‰ˆï¼Œä½¿ç”¨ç¼“å­˜ï¼‰

        ä»æ¿å—åˆ†æä¸­æå–æ¿å—æ¶¨è·Œå¹…ã€æ¿å—æ’åã€æ¿å—è¶‹åŠ¿ç­‰ç‰¹å¾ï¼š
        - sector_avg_change: æ¿å—å¹³å‡æ¶¨è·Œå¹…ï¼ˆ1æ—¥/5æ—¥/20æ—¥ï¼‰
        - sector_rank: æ¿å—æ¶¨è·Œå¹…æ’åï¼ˆ1æ—¥/5æ—¥/20æ—¥ï¼‰
        - sector_rising_ratio: æ¿å—ä¸Šæ¶¨è‚¡ç¥¨æ¯”ä¾‹
        - sector_total_volume: æ¿å—æ€»æˆäº¤é‡
        - sector_stock_count: æ¿å—è‚¡ç¥¨æ•°é‡
        - sector_trend: æ¿å—è¶‹åŠ¿ï¼ˆé‡åŒ–ä¸ºæ•°å€¼ï¼‰
        - sector_flow_score: æ¿å—èµ„é‡‘æµå‘è¯„åˆ†
        - is_sector_leader: æ˜¯å¦ä¸ºæ¿å—é¾™å¤´
        - sector_best_stock_change: æ¿å—æœ€ä½³è‚¡ç¥¨æ¶¨è·Œå¹…
        - sector_worst_stock_change: æ¿å—æœ€å·®è‚¡ç¥¨æ¶¨è·Œå¹…

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«æ¿å—ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–æ¿å—åˆ†æå™¨ï¼ˆå•ä¾‹ï¼‰
            sector_analyzer = self._get_sector_analyzer()
            if sector_analyzer is None:
                # æ¨¡å—ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sector_avg_change_1d': 0.0,
                    'sector_avg_change_5d': 0.0,
                    'sector_avg_change_20d': 0.0,
                    'sector_rank_1d': 0,
                    'sector_rank_5d': 0,
                    'sector_rank_20d': 0,
                    'sector_rising_ratio_1d': 0.5,
                    'sector_rising_ratio_5d': 0.5,
                    'sector_rising_ratio_20d': 0.5,
                    'sector_total_volume': 0.0,
                    'sector_stock_count': 0,
                    'sector_trend_score': 0.0,
                    'sector_flow_score': 0.0,
                    'is_sector_leader': 0,
                    'sector_best_stock_change': 0.0,
                    'sector_worst_stock_change': 0.0,
                    'sector_outperform_hsi': 0
                }

            # è·å–è‚¡ç¥¨æ‰€å±æ¿å—
            sector_info = sector_analyzer.stock_mapping.get(code)
            if not sector_info:
                # æœªæ‰¾åˆ°æ¿å—ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sector_avg_change_1d': 0.0,
                    'sector_avg_change_5d': 0.0,
                    'sector_avg_change_20d': 0.0,
                    'sector_rank_1d': 0,
                    'sector_rank_5d': 0,
                    'sector_rank_20d': 0,
                    'sector_rising_ratio_1d': 0.5,
                    'sector_rising_ratio_5d': 0.5,
                    'sector_rising_ratio_20d': 0.5,
                    'sector_total_volume': 0.0,
                    'sector_stock_count': 0,
                    'sector_trend_score': 0.0,
                    'sector_flow_score': 0.0,
                    'is_sector_leader': 0,
                    'sector_best_stock_change': 0.0,
                    'sector_worst_stock_change': 0.0,
                    'sector_outperform_hsi': 0
                }

            sector_code = sector_info['sector']

            features = {}

            # è®¡ç®—ä¸åŒå‘¨æœŸçš„æ¿å—è¡¨ç°ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            for period in [1, 5, 20]:
                try:
                    perf_df = self._get_sector_performance(period)

                    if perf_df is not None and not perf_df.empty:
                        # æ‰¾åˆ°è¯¥æ¿å—çš„æ’å
                        sector_row = perf_df[perf_df['sector_code'] == sector_code]

                        if not sector_row.empty:
                            sector_data = sector_row.iloc[0]

                            # æ¿å—å¹³å‡æ¶¨è·Œå¹…
                            features[f'sector_avg_change_{period}d'] = sector_data['avg_change_pct']

                            # æ¿å—æ’å
                            sector_rank = perf_df[perf_df['sector_code'] == sector_code].index[0] + 1
                            features[f'sector_rank_{period}d'] = sector_rank

                            # æ¿å—ä¸Šæ¶¨è‚¡ç¥¨æ¯”ä¾‹
                            rising_count = sum(1 for s in sector_data['stocks'] if s['change_pct'] > 0)
                            total_count = len(sector_data['stocks'])
                            features[f'sector_rising_ratio_{period}d'] = rising_count / total_count if total_count > 0 else 0.5

                            # æ¿å—æ€»æˆäº¤é‡
                            features['sector_total_volume'] = sector_data['total_volume']

                            # æ¿å—è‚¡ç¥¨æ•°é‡
                            features['sector_stock_count'] = sector_data['stock_count']

                            # æœ€ä½³å’Œæœ€å·®è‚¡ç¥¨è¡¨ç°
                            if sector_data['best_stock']:
                                features['sector_best_stock_change'] = sector_data['best_stock']['change_pct']
                            if sector_data['worst_stock']:
                                features['sector_worst_stock_change'] = sector_data['worst_stock']['change_pct']

                            # æ˜¯å¦ä¸ºæ¿å—é¾™å¤´ï¼ˆå‰3åï¼‰
                            features['is_sector_leader'] = 1 if sector_rank <= 3 else 0
                        else:
                            # æ¿å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            features[f'sector_avg_change_{period}d'] = 0.0
                            features[f'sector_rank_{period}d'] = 0
                            features[f'sector_rising_ratio_{period}d'] = 0.5
                    else:
                        # æ— æ³•è·å–æ¿å—æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        features[f'sector_avg_change_{period}d'] = 0.0
                        features[f'sector_rank_{period}d'] = 0
                        features[f'sector_rising_ratio_{period}d'] = 0.5

                except Exception as e:
                    logger.warning(f"è®¡ç®—æ¿å—è¡¨ç°å¤±è´¥ (period={period}): {e}")
                    features[f'sector_avg_change_{period}d'] = 0.0
                    features[f'sector_rank_{period}d'] = 0
                    features[f'sector_rising_ratio_{period}d'] = 0.5

            # è®¡ç®—æ¿å—è¶‹åŠ¿
            try:
                trend_result = sector_analyzer.analyze_sector_trend(sector_code, days=20)

                if 'trend' in trend_result:
                    # å°†è¶‹åŠ¿é‡åŒ–ä¸ºæ•°å€¼
                    trend_mapping = {
                        'å¼ºåŠ¿ä¸Šæ¶¨': 2.0,
                        'æ¸©å’Œä¸Šæ¶¨': 1.0,
                        'éœ‡è¡æ•´ç†': 0.0,
                        'æ¸©å’Œä¸‹è·Œ': -1.0,
                        'å¼ºåŠ¿ä¸‹è·Œ': -2.0
                    }
                    features['sector_trend_score'] = trend_mapping.get(trend_result['trend'], 0.0)
                else:
                    features['sector_trend_score'] = 0.0
            except Exception as e:
                logger.warning(f"è®¡ç®—æ¿å—è¶‹åŠ¿å¤±è´¥: {e}")
                features['sector_trend_score'] = 0.0

            # è®¡ç®—æ¿å—èµ„é‡‘æµå‘
            try:
                flow_result = sector_analyzer.analyze_sector_fund_flow(sector_code, days=5)

                if 'avg_flow_score' in flow_result:
                    features['sector_flow_score'] = flow_result['avg_flow_score']
                else:
                    features['sector_flow_score'] = 0.0
            except Exception as e:
                logger.warning(f"è®¡ç®—æ¿å—èµ„é‡‘æµå‘å¤±è´¥: {e}")
                features['sector_flow_score'] = 0.0

            # åˆ¤æ–­æ¿å—æ˜¯å¦è·‘èµ¢æ’æŒ‡ï¼ˆåŸºäºæ¿å—å¹³å‡æ¶¨è·Œå¹…ï¼‰
            if 'sector_avg_change_1d' in features and 'sector_avg_change_5d' in features:
                # ç®€åŒ–å¤„ç†ï¼šå‡è®¾æ’æŒ‡æ¶¨è·Œå¹…ä¸º0ï¼ˆå®é™…åº”è¯¥ä»æ’æŒ‡æ•°æ®ä¸­è·å–ï¼‰
                # è¿™é‡Œä½¿ç”¨æ¿å—è‡ªèº«çš„æ¶¨è·Œå¹…ä½œä¸ºå‚è€ƒ
                features['sector_outperform_hsi'] = 1 if features['sector_avg_change_5d'] > 0 else 0

            return features

        except Exception as e:
            logger.warning(f"è®¡ç®—æ¿å—ç‰¹å¾å¤±è´¥ {code}: {e}")
            # å¼‚å¸¸æƒ…å†µè¿”å›é»˜è®¤å€¼
            return {
                'sector_avg_change_1d': 0.0,
                'sector_avg_change_5d': 0.0,
                'sector_avg_change_20d': 0.0,
                'sector_rank_1d': 0,
                'sector_rank_5d': 0,
                'sector_rank_20d': 0,
                'sector_rising_ratio_1d': 0.5,
                'sector_rising_ratio_5d': 0.5,
                'sector_rising_ratio_20d': 0.5,
                'sector_total_volume': 0.0,
                'sector_stock_count': 0,
                'sector_trend_score': 0.0,
                'sector_flow_score': 0.0,
                'is_sector_leader': 0,
                'sector_best_stock_change': 0.0,
                'sector_worst_stock_change': 0.0,
                'sector_outperform_hsi': 0
            }

    def create_interaction_features(self, df):
        """åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„äº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰

        ç”Ÿæˆç­–ç•¥ï¼šå°†æ‰€æœ‰ç±»åˆ«å‹ç‰¹å¾ï¼ˆ13ä¸ªï¼‰ä¸æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾ï¼ˆ90ä¸ªï¼‰è¿›è¡Œäº¤å‰ï¼Œ
        å½¢æˆ 1170 ä¸ªäº¤å‰ç‰¹å¾ã€‚GBDT+LR ç®—æ³•ä¼šè‡ªåŠ¨è¿‡æ»¤æ— ç”¨ç‰¹å¾ã€‚
        """
        if df.empty:
            return df

        # ç±»åˆ«å‹ç‰¹å¾ï¼ˆ13ä¸ªï¼‰
        categorical_features = [
            'Outperforms_HSI',
            'Strong_Volume_Up',
            'Weak_Volume_Down',
            '3d_Trend', '5d_Trend', '10d_Trend', '20d_Trend', '60d_Trend',
            '3d_RS_Signal', '5d_RS_Signal', '10d_RS_Signal', '20d_RS_Signal', '60d_RS_Signal'
        ]

        # æ•°å€¼å‹ç‰¹å¾ï¼ˆæ’é™¤ç±»åˆ«å‹ç‰¹å¾ã€æ ‡ç­¾å’ŒåŸå§‹ä»·æ ¼æ•°æ®ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Returns', 'TP', 'MF_Multiplier', 'MF_Volume',
                          'High_Max', 'Low_Min'] + categorical_features

        numeric_features = [col for col in df.columns if col not in exclude_columns]

        print(f"ç”Ÿæˆäº¤å‰ç‰¹å¾: {len(categorical_features)} ä¸ªç±»åˆ« Ã— {len(numeric_features)} ä¸ªæ•°å€¼ = {len(categorical_features) * len(numeric_features)} ä¸ªäº¤å‰ç‰¹å¾")

        # ç”Ÿæˆæ‰€æœ‰äº¤å‰ç‰¹å¾
        interaction_count = 0
        for cat_feat in categorical_features:
            if cat_feat not in df.columns:
                continue

            for num_feat in numeric_features:
                if num_feat not in df.columns:
                    continue

                # äº¤å‰ç‰¹å¾å‘½åï¼šç±»åˆ«_æ•°å€¼
                interaction_name = f"{cat_feat}_{num_feat}"
                df[interaction_name] = df[cat_feat] * df[num_feat]
                interaction_count += 1

        logger.info(f"æˆåŠŸç”Ÿæˆ {interaction_count} ä¸ªäº¤å‰ç‰¹å¾")
        return df


class BaseTradingModel:
    """äº¤æ˜“æ¨¡å‹åŸºç±» - æä¾›å…¬å…±æ–¹æ³•å’Œå±æ€§"""

    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.processor = BaseModelProcessor()
        self.feature_columns = []
        self.horizon = 1  # é»˜è®¤é¢„æµ‹å‘¨æœŸ
        self.model_type = None  # å­ç±»å¿…é¡»è®¾ç½®
        self.categorical_encoders = {}

    def load_selected_features(self, filepath=None, current_feature_names=None):
        """åŠ è½½é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨ç‰¹å¾åç§°äº¤é›†ï¼Œç¡®ä¿ç‰¹å¾å­˜åœ¨ï¼‰

        Args:
            filepath: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„ï¼‰
            current_feature_names: å½“å‰æ•°æ®é›†çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        import os
        import glob

        if filepath is None:
            # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾åç§°æ–‡ä»¶
            # ä¼˜å…ˆæŸ¥æ‰¾ model_importance_selected_*.csvï¼ˆæ¨¡å‹é‡è¦æ€§æ³•ï¼‰
            pattern = 'output/model_importance_selected_*.csv'
            files = glob.glob(pattern)
            if not files:
                # å›é€€åˆ° selected_features_*.csvï¼ˆç»Ÿè®¡æ–¹æ³•ï¼‰
                pattern = 'output/selected_features_*.csv'
                files = glob.glob(pattern)
            
            if not files:
                return None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            filepath = max(files, key=os.path.getmtime)

        try:
            import pandas as pd
            # è¯»å–ç‰¹å¾åç§°
            df = pd.read_csv(filepath)
            selected_names = df['Feature_Name'].tolist()

            logger.debug(f"åŠ è½½ç‰¹å¾åˆ—è¡¨æ–‡ä»¶: {filepath}")
            logger.info(f"åŠ è½½äº† {len(selected_names)} ä¸ªé€‰æ‹©çš„ç‰¹å¾")

            # å¦‚æœæä¾›äº†å½“å‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨äº¤é›†
            if current_feature_names is not None:
                current_set = set(current_feature_names)
                selected_set = set(selected_names)
                available_set = current_set & selected_set
                
                available_names = list(available_set)
                logger.info(f"å½“å‰æ•°æ®é›†ç‰¹å¾æ•°é‡: {len(current_feature_names)}")
                logger.info(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_names)}")
                logger.info(f"å®é™…å¯ç”¨çš„ç‰¹å¾æ•°é‡: {len(available_names)}")
                logger.warning(f" {len(selected_set) - len(available_names)} ä¸ªç‰¹å¾åœ¨å½“å‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
                
                return available_names
            else:
                return selected_names

        except Exception as e:
            logger.warning(f"åŠ è½½ç‰¹å¾åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—ï¼ˆæ’é™¤ä¸­é—´è®¡ç®—åˆ—ï¼‰"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns


class LightGBMModel(BaseTradingModel):
    """LightGBM æ¨¡å‹ - åŸºäº LightGBM æ¢¯åº¦æå‡ç®—æ³•çš„å•ä¸€æ¨¡å‹"""

    def __init__(self):
        super().__init__()  # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        self.model = None
        self.scaler = StandardScaler()
        self.model_type = 'lgbm'  # æ¨¡å‹ç±»å‹æ ‡è¯†

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1, for_backtest=False):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼Œä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸åº”ç”¨horizonè¿‡æ»¤ï¼‰
        """
        self.horizon = horizon
        all_data = []

        # ========== æ­¥éª¤1ï¼šè·å–å…±äº«æ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰ ==========
        logger.info("è·å–å…±äº«æ•°æ®...")
        
        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            logger.info(f"æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            logger.warning(r"æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼Œæ‰€æœ‰è‚¡ç¥¨å…±äº«ï¼‰
        hsi_df = get_hsi_data_with_cache(period_days=730)
        if hsi_df is None or hsi_df.empty:
            raise ValueError("æ— æ³•è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®")

        # ========== æ­¥éª¤2ï¼šå¹¶è¡Œä¸‹è½½è‚¡ç¥¨æ•°æ® ==========
        print(f"\nğŸš€ å¹¶è¡Œä¸‹è½½ {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
        
        def fetch_single_stock_data(code):
            """è·å–å•åªè‚¡ç¥¨æ•°æ®"""
            try:
                stock_code = code.replace('.HK', '')
                stock_df = get_stock_data_with_cache(stock_code, period_days=730)
                if stock_df is not None and not stock_df.empty:
                    return (code, stock_df)
                return None
            except Exception as e:
                logger.warning(f"ä¸‹è½½è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                return None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½ï¼ˆæœ€å¤š8ä¸ªå¹¶å‘ï¼‰
        stock_data_list = []
        with ThreadPoolExecutor(max_workers=8) as executor:
            future_to_code = {executor.submit(fetch_single_stock_data, code): code for code in codes}
            
            for i, future in enumerate(as_completed(future_to_code), 1):
                result = future.result()
                if result is not None:
                    stock_data_list.append(result)
                    print(f"  âœ… [{i}/{len(codes)}] {result[0]}")

        logger.info(f"æˆåŠŸä¸‹è½½ {len(stock_data_list)} åªè‚¡ç¥¨æ•°æ®")

        # ========== æ­¥éª¤3ï¼šè®¡ç®—ç‰¹å¾ ==========
        print(f"\nğŸ”§ è®¡ç®—ç‰¹å¾...")
        
        for i, (code, stock_df) in enumerate(stock_data_list, 1):
            try:
                print(f"  [{i}/{len(stock_data_list)}] å¤„ç†è‚¡ç¥¨: {code}")

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡ï¼ˆä½¿ç”¨å…±äº«çš„æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼‰
                stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon, for_backtest=for_backtest)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value

                # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
                sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
                for key, value in sentiment_features.items():
                    stock_df[key] = value

                # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
                topic_features = self.feature_engineer.create_topic_features(code, stock_df)
                for key, value in topic_features.items():
                    stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

                # æ·»åŠ æ¿å—ç‰¹å¾
                sector_features = self.feature_engineer.create_sector_features(code, stock_df)
                for key, value in sector_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆä¿ç•™æ—¥æœŸç´¢å¼•ï¼Œä¸é‡ç½®ç´¢å¼•ï¼‰
        df = pd.concat(all_data, ignore_index=False)

        # æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        df = df.sort_index()

        # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆå…ˆæ‰§è¡Œï¼Œå› ä¸ºè¿™æ˜¯é«˜ä»·å€¼ç‰¹å¾ï¼‰
        print("\nğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")
        df = self.feature_engineer.create_technical_fundamental_interactions(df)

        # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰
        print("\nğŸ”— ç”Ÿæˆäº¤å‰ç‰¹å¾...")
        df = self.feature_engineer.create_interaction_features(df)

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1, use_feature_selection=False):
        """è®­ç»ƒæ¨¡å‹

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
        """
        print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # å…ˆåˆ é™¤å…¨ä¸ºNaNçš„åˆ—ï¼ˆé¿å…dropnaåˆ é™¤æ‰€æœ‰è¡Œï¼‰
        cols_all_nan = df.columns[df.isnull().all()].tolist()
        if cols_all_nan:
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_all_nan)} ä¸ªå…¨ä¸ºNaNçš„åˆ—")
            df = df.drop(columns=cols_all_nan)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼ˆdropna å¯èƒ½ä¼šæ”¹å˜é¡ºåºï¼‰
        df = df.sort_index()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        print(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        if use_feature_selection:
            print("\nğŸ¯ åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆLightGBMï¼‰...")
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)
            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                logger.info(f"ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                logger.warning(r"æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")

        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç¼–ç ï¼‰
        categorical_features = []
        self.categorical_encoders = {}  # å­˜å‚¨ç¼–ç å™¨ï¼Œç”¨äºé¢„æµ‹æ—¶è§£ç 

        for col in self.feature_columns:
            if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                print(f"  ç¼–ç åˆ†ç±»ç‰¹å¾: {col}")
                categorical_features.append(col)
                # ä½¿ç”¨LabelEncoderè¿›è¡Œç¼–ç 
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.categorical_encoders[col] = le

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=5)

        # æ ¹æ®é¢„æµ‹å‘¨æœŸè°ƒæ•´æ­£åˆ™åŒ–å‚æ•°ï¼ˆåˆ†å‘¨æœŸä¼˜åŒ–ç­–ç•¥ï¼‰
        # æ¬¡æ—¥æ¨¡å‹ï¼šæœ€å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        # ä¸€å‘¨æ¨¡å‹ï¼šé€‚åº¦æ­£åˆ™åŒ–ä¿æŒå­¦ä¹ èƒ½åŠ›
        # ä¸€ä¸ªæœˆæ¨¡å‹ï¼šå¢å¼ºæ­£åˆ™åŒ–ï¼ˆç‰¹å¾æ•°é‡å¤šï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
        if horizon == 1:
            # æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆæœ€å¼ºæ­£åˆ™åŒ–ï¼‰
            print("ä½¿ç”¨æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆå¼ºæ­£åˆ™åŒ–ï¼‰...")
            lgb_params = {
                'n_estimators': 40,           # å‡å°‘æ ‘æ•°é‡ï¼ˆ50â†’40ï¼‰
                'learning_rate': 0.02,         # é™ä½å­¦ä¹ ç‡ï¼ˆ0.03â†’0.02ï¼‰
                'max_depth': 3,                # é™ä½æ·±åº¦ï¼ˆ4â†’3ï¼‰
                'num_leaves': 12,              # å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ15â†’12ï¼‰
                'min_child_samples': 40,       # å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ30â†’40ï¼‰
                'subsample': 0.65,             # å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'colsample_bytree': 0.65,      # å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'reg_alpha': 0.2,              # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.1â†’0.2ï¼‰
                'reg_lambda': 0.2,             # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.1â†’0.2ï¼‰
                'min_split_gain': 0.15,        # å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.1â†’0.15ï¼‰
                'feature_fraction': 0.65,      # å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'bagging_fraction': 0.65,      # å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }
        elif horizon == 5:
            # ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰
            print("ä½¿ç”¨5å¤©æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰...")
            lgb_params = {
                'n_estimators': 50,           # ä¿æŒ50
                'learning_rate': 0.03,         # ä¿æŒ0.03
                'max_depth': 4,                # ä¿æŒ4
                'num_leaves': 15,              # ä¿æŒ15
                'min_child_samples': 30,       # ä¿æŒ30
                'subsample': 0.7,              # ä¿æŒ0.7
                'colsample_bytree': 0.7,       # ä¿æŒ0.7
                'reg_alpha': 0.1,              # ä¿æŒ0.1
                'reg_lambda': 0.1,             # ä¿æŒ0.1
                'min_split_gain': 0.1,         # ä¿æŒ0.1
                'feature_fraction': 0.7,       # ä¿æŒ0.7
                'bagging_fraction': 0.7,       # ä¿æŒ0.7
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }
        else:  # horizon == 20
            # ä¸€ä¸ªæœˆæ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ– - 2026-02-16ä¼˜åŒ–ï¼‰
            # åŸå› ï¼šç‰¹å¾æ•°é‡ä»2530å¢è‡³2936ï¼ˆ+16%ï¼‰ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
            # ä¼˜åŒ–ç›®æ ‡ï¼šå°†è®­ç»ƒ/éªŒè¯å·®è·ä»Â±7.07%é™è‡³<5%
            print("ä½¿ç”¨20å¤©æ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½è¿‡æ‹Ÿåˆï¼‰...")
            lgb_params = {
                'n_estimators': 40,           # è¿›ä¸€æ­¥å‡å°‘æ ‘æ•°é‡ï¼ˆ45â†’40ï¼‰
                'learning_rate': 0.02,         # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼ˆ0.025â†’0.02ï¼‰
                'max_depth': 3,                # é™ä½æ·±åº¦ï¼ˆ4â†’3ï¼‰å‡å°‘è¿‡æ‹Ÿåˆ
                'num_leaves': 11,              # è¿›ä¸€æ­¥å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ13â†’11ï¼‰
                'min_child_samples': 40,       # è¿›ä¸€æ­¥å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ35â†’40ï¼‰
                'subsample': 0.6,              # è¿›ä¸€æ­¥å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'colsample_bytree': 0.6,       # è¿›ä¸€æ­¥å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'reg_alpha': 0.25,             # è¶…å¢å¼ºL1æ­£åˆ™ï¼ˆ0.18â†’0.25ï¼‰
                'reg_lambda': 0.25,            # è¶…å¢å¼ºL2æ­£åˆ™ï¼ˆ0.18â†’0.25ï¼‰
                'min_split_gain': 0.15,        # è¿›ä¸€æ­¥å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.12â†’0.15ï¼‰
                'feature_fraction': 0.6,       # è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'bagging_fraction': 0.6,       # è¿›ä¸€æ­¥å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }

        # è®­ç»ƒæ¨¡å‹ï¼ˆå¢åŠ æ­£åˆ™åŒ–ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼‰
        print("è®­ç»ƒLightGBMæ¨¡å‹...")
        self.model = lgb.LGBMClassifier(**lgb_params)

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        scores = []
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # æ·»åŠ early_stoppingä»¥å‡å°‘è¿‡æ‹Ÿåˆ
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(stopping_rounds=15, verbose=False)  # å¢åŠ patienceï¼ˆ10â†’15ï¼‰
                ]
            )
            y_pred = self.model.predict(X_val)
            score = accuracy_score(y_val, y_pred)
            scores.append(score)
            print(f"éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.model.fit(X, y)

        mean_accuracy = np.mean(scores)
        std_accuracy = np.std(scores)
        print(f"\nå¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})")

        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶ï¼ˆä¾›ç»¼åˆåˆ†æä½¿ç”¨ï¼‰
        accuracy_info = {
            'model_type': 'lgbm',
            'horizon': horizon,
            'accuracy': float(mean_accuracy),
            'std': float(std_accuracy),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        import json
        accuracy_file = 'data/model_accuracy.json'
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
            
            # æ›´æ–°å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
            key = f'lgbm_{horizon}d'
            existing_data[key] = accuracy_info
            
            # ä¿å­˜å›æ–‡ä»¶
            with open(accuracy_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            logger.info(f"å‡†ç¡®ç‡å·²ä¿å­˜åˆ° {accuracy_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜å‡†ç¡®ç‡å¤±è´¥: {e}")

        # ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨ BaseModelProcessor ç»Ÿä¸€æ ¼å¼ï¼‰
        feat_imp = self.processor.analyze_feature_importance(
            self.model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            contrib_values = self.model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )
        except Exception as e:
            logger.warning(f"ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(10))

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    logger.warning(f"è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)

            # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

            # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

            # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)

            # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
            stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
            for key, value in stock_type_features.items():
                stock_df[key] = value

            # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
            for key, value in sentiment_features.items():
                stock_df[key] = value

            # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
            topic_features = self.feature_engineer.create_topic_features(code, stock_df)
            for key, value in topic_features.items():
                stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

            # æ·»åŠ æ¿å—ç‰¹å¾
            sector_features = self.feature_engineer.create_sector_features(code, stock_df)
            for key, value in sector_features.items():
                stock_df[key] = value

            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

            # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_interaction_features(stock_df)

            # è·å–æœ€æ–°æ•°æ®ï¼ˆæˆ–æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼‰
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨ï¼‰
            for col, encoder in self.categorical_encoders.items():
                if col in latest_data.columns:
                    # å¦‚æœé‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œæ˜ å°„åˆ°0
                    try:
                        latest_data[col] = encoder.transform(latest_data[col].astype(str))
                    except ValueError:
                        # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
                        logger.warning(f"è­¦å‘Š: åˆ†ç±»ç‰¹å¾ {col} åŒ…å«è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        latest_data[col] = 0

            X = latest_data[self.feature_columns].values

            # é¢„æµ‹
            proba = self.model.predict_proba(X)[0]
            prediction = self.model.predict(X)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'categorical_encoders': self.categorical_encoders
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_columns = model_data['feature_columns']
        self.categorical_encoders = model_data.get('categorical_encoders', {})
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class GBDTModel(BaseTradingModel):
    """GBDT æ¨¡å‹ - åŸºäºæ¢¯åº¦æå‡å†³ç­–æ ‘çš„å•ä¸€æ¨¡å‹"""

    def __init__(self):
        super().__init__()  # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        self.gbdt_model = None
        self.actual_n_estimators = 0
        self.model_type = 'gbdt'  # æ¨¡å‹ç±»å‹æ ‡è¯†

    def load_selected_features(self, filepath=None, current_feature_names=None):
        """åŠ è½½é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨ç‰¹å¾åç§°äº¤é›†ï¼Œç¡®ä¿ç‰¹å¾å­˜åœ¨ï¼‰

        Args:
            filepath: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„ï¼‰
            current_feature_names: å½“å‰æ•°æ®é›†çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        import os
        import glob

        if filepath is None:
            # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾åç§°æ–‡ä»¶
            pattern = 'output/selected_features_*.csv'
            files = glob.glob(pattern)
            if not files:
                return None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            filepath = max(files, key=os.path.getmtime)

        try:
            import pandas as pd
            # è¯»å–ç‰¹å¾åç§°
            df = pd.read_csv(filepath)
            selected_names = df['Feature_Name'].tolist()

            logger.debug(f"åŠ è½½ç‰¹å¾åˆ—è¡¨æ–‡ä»¶: {filepath}")
            logger.info(f"åŠ è½½äº† {len(selected_names)} ä¸ªé€‰æ‹©çš„ç‰¹å¾")

            # å¦‚æœæä¾›äº†å½“å‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨äº¤é›†
            if current_feature_names is not None:
                current_set = set(current_feature_names)
                selected_set = set(selected_names)
                available_set = current_set & selected_set
                
                available_names = list(available_set)
                logger.info(f"å½“å‰æ•°æ®é›†ç‰¹å¾æ•°é‡: {len(current_feature_names)}")
                logger.info(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_names)}")
                logger.info(f"å®é™…å¯ç”¨çš„ç‰¹å¾æ•°é‡: {len(available_names)}")
                logger.warning(f" {len(selected_set) - len(available_set)} ä¸ªç‰¹å¾åœ¨å½“å‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
                
                return available_names
            else:
                return selected_names

        except Exception as e:
            logger.warning(f"åŠ è½½ç‰¹å¾åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1, for_backtest=False):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸åº”ç”¨horizonè¿‡æ»¤ï¼‰
        """
        self.horizon = horizon
        all_data = []

        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        logger.info("è·å–ç¾è‚¡å¸‚åœºæ•°æ®...")
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            logger.info(f"æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            logger.warning(r"æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        for code in codes:
            try:
                print(f"å¤„ç†è‚¡ç¥¨: {code}")

                # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€ï¼Œè…¾è®¯è´¢ç»æ¥å£ä¸éœ€è¦
                stock_code = code.replace('.HK', '')

                # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
                if stock_df is None or stock_df.empty:
                    continue

                # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                hsi_df = get_hsi_data_tencent(period_days=730)
                if hsi_df is None or hsi_df.empty:
                    continue

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                
                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon, for_backtest=for_backtest)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value

                # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
                sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
                for key, value in sentiment_features.items():
                    stock_df[key] = value

                # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
                topic_features = self.feature_engineer.create_topic_features(code, stock_df)
                for key, value in topic_features.items():
                    stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

                # æ·»åŠ æ¿å—ç‰¹å¾
                sector_features = self.feature_engineer.create_sector_features(code, stock_df)
                for key, value in sector_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆä¿ç•™æ—¥æœŸç´¢å¼•ï¼Œä¸é‡ç½®ç´¢å¼•ï¼‰
        df = pd.concat(all_data, ignore_index=False)

        # æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        df = df.sort_index()

        # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆå…ˆæ‰§è¡Œï¼Œå› ä¸ºè¿™æ˜¯é«˜ä»·å€¼ç‰¹å¾ï¼‰
        print("\nğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")
        df = self.feature_engineer.create_technical_fundamental_interactions(df)

        # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰
        print("\nğŸ”— ç”Ÿæˆäº¤å‰ç‰¹å¾...")
        df = self.feature_engineer.create_interaction_features(df)

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1, use_feature_selection=False):
        """è®­ç»ƒ GBDT æ¨¡å‹

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
        """
        print("="*70)
        logger.info("å¼€å§‹è®­ç»ƒ GBDT æ¨¡å‹")
        print("="*70)

        # å‡†å¤‡æ•°æ®
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # å…ˆåˆ é™¤å…¨ä¸ºNaNçš„åˆ—ï¼ˆé¿å…dropnaåˆ é™¤æ‰€æœ‰è¡Œï¼‰
        cols_all_nan = df.columns[df.isnull().all()].tolist()
        if cols_all_nan:
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_all_nan)} ä¸ªå…¨ä¸ºNaNçš„åˆ—")
            df = df.drop(columns=cols_all_nan)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼ˆdropna å¯èƒ½ä¼šæ”¹å˜é¡ºåºï¼‰
        df = df.sort_index()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        logger.info(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        if use_feature_selection:
            print("\nğŸ¯ åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆGBDTï¼‰...")
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)
            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                logger.info(f"ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                logger.warning(r"æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
        else:
            logger.info(f"ä½¿ç”¨å…¨éƒ¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç¼–ç ï¼‰
        categorical_features = []
        self.categorical_encoders = {}  # å­˜å‚¨ç¼–ç å™¨ï¼Œç”¨äºé¢„æµ‹æ—¶è§£ç 

        for col in self.feature_columns:
            if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                print(f"  ç¼–ç åˆ†ç±»ç‰¹å¾: {col}")
                categorical_features.append(col)
                # ä½¿ç”¨LabelEncoderè¿›è¡Œç¼–ç 
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.categorical_encoders[col] = le

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('output', exist_ok=True)

        # ========== è®­ç»ƒ GBDT æ¨¡å‹ ==========
        print("\n" + "="*70)
        print("ğŸŒ² è®­ç»ƒ GBDT æ¨¡å‹")
        print("="*70)

        # æ ¹æ®é¢„æµ‹å‘¨æœŸè°ƒæ•´å¶å­èŠ‚ç‚¹æ•°é‡å’Œæ—©åœè€å¿ƒ
        # æ¬¡æ—¥æ¨¡å‹ï¼šé€‚åº¦å‚æ•°
        # ä¸€å‘¨æ¨¡å‹ï¼šå‡å°‘å¶å­èŠ‚ç‚¹æ•°é‡ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ—©åœè€å¿ƒ
        # ä¸€ä¸ªæœˆæ¨¡å‹ï¼šå¢å¼ºæ­£åˆ™åŒ–ï¼ˆç‰¹å¾æ•°é‡å¢åŠ ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
        if horizon == 5:
            # ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
            print("ä½¿ç”¨ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆå‡å°‘å¶å­èŠ‚ç‚¹ï¼Œå¢åŠ æ—©åœè€å¿ƒï¼‰...")
            n_estimators = 32
            num_leaves = 24  # å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ32â†’24ï¼‰
            stopping_rounds = 15  # å¢åŠ æ—©åœè€å¿ƒï¼ˆ10â†’15ï¼‰
            min_child_samples = 30  # å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ20â†’30ï¼‰
            reg_alpha = 0.1     # ä¿æŒ0.1
            reg_lambda = 0.1    # ä¿æŒ0.1
            subsample = 0.7     # ä¿æŒ0.7
            colsample_bytree = 0.6  # ä¿æŒ0.6
        elif horizon == 1:
            # æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦ï¼‰
            print("ä½¿ç”¨æ¬¡æ—¥æ¨¡å‹å‚æ•°...")
            n_estimators = 32
            num_leaves = 28  # é€‚åº¦å‡å°‘ï¼ˆ32â†’28ï¼‰
            stopping_rounds = 12  # é€‚åº¦å¢åŠ 
            min_child_samples = 25
            reg_alpha = 0.15    # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.1â†’0.15ï¼‰
            reg_lambda = 0.15   # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.1â†’0.15ï¼‰
            subsample = 0.65    # å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
            colsample_bytree = 0.65  # å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.6â†’0.65ï¼‰
        else:  # horizon == 20
            # ä¸€ä¸ªæœˆæ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ– - 2026-02-16ä¼˜åŒ–ï¼‰
            # åŸå› ï¼šç‰¹å¾æ•°é‡ä»2530å¢è‡³2936ï¼ˆ+16%ï¼‰ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
            # ä¼˜åŒ–ç›®æ ‡ï¼šå°†è®­ç»ƒ/éªŒè¯å·®è·ä»Â±7.07%é™è‡³<5%
            print("ä½¿ç”¨20å¤©æ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½è¿‡æ‹Ÿåˆï¼‰...")
            n_estimators = 28           # è¿›ä¸€æ­¥å‡å°‘æ ‘æ•°é‡ï¼ˆ32â†’28ï¼‰
            num_leaves = 20              # è¿›ä¸€æ­¥å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ24â†’20ï¼‰
            stopping_rounds = 18         # è¿›ä¸€æ­¥å¢åŠ æ—©åœè€å¿ƒï¼ˆ12â†’18ï¼‰
            min_child_samples = 35       # è¿›ä¸€æ­¥å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ30â†’35ï¼‰
            reg_alpha = 0.22             # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.15â†’0.22ï¼‰
            reg_lambda = 0.22            # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.15â†’0.22ï¼‰
            subsample = 0.6              # è¿›ä¸€æ­¥å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
            colsample_bytree = 0.6       # è¿›ä¸€æ­¥å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰

        self.gbdt_model = lgb.LGBMClassifier(
            objective='binary',
            boosting_type='gbdt',
            subsample=subsample,            # æ ¹æ®å‘¨æœŸè°ƒæ•´
            min_child_weight=0.1,
            min_child_samples=min_child_samples,  # æ ¹æ®å‘¨æœŸè°ƒæ•´
            colsample_bytree=colsample_bytree,  # æ ¹æ®å‘¨æœŸè°ƒæ•´
            num_leaves=num_leaves,      # æ ¹æ®å‘¨æœŸè°ƒæ•´
            learning_rate=0.025,        # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼ˆ0.03â†’0.025ï¼‰
            n_estimators=n_estimators,
            reg_alpha=reg_alpha,        # æ ¹æ®å‘¨æœŸè°ƒæ•´L1æ­£åˆ™
            reg_lambda=reg_lambda,       # æ ¹æ®å‘¨æœŸè°ƒæ•´L2æ­£åˆ™
            min_split_gain=0.12,        # è¿›ä¸€æ­¥å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.1â†’0.12ï¼‰
            feature_fraction=0.6,       # è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.7â†’0.6ï¼‰
            bagging_fraction=0.6,       # è¿›ä¸€æ­¥å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.7â†’0.6ï¼‰
            bagging_freq=5,             # Baggingé¢‘ç‡ï¼ˆæ–°å¢ï¼‰
            random_state=2020,
            n_jobs=-1,
            verbose=-1
        )

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        gbdt_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            self.gbdt_model.fit(
                X_train_fold, y_train_fold,
                eval_set=[(X_val_fold, y_val_fold)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=False)  # æ ¹æ®å‘¨æœŸè°ƒæ•´æ—©åœè€å¿ƒ
                ]
            )

            y_pred_fold = self.gbdt_model.predict(X_val_fold)
            score = accuracy_score(y_val_fold, y_pred_fold)
            gbdt_scores.append(score)
            print(f"   Fold {fold} éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.gbdt_model.fit(X, y)

        # è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
        # æ³¨æ„ï¼šåœ¨ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæ—¶ï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨æ—©åœï¼Œbest_iteration_ å¯èƒ½ä¸º None
        # è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ n_estimators
        self.actual_n_estimators = self.gbdt_model.best_iteration_ if self.gbdt_model.best_iteration_ else n_estimators
        mean_accuracy = np.mean(gbdt_scores)
        std_accuracy = np.std(gbdt_scores)
        print(f"\nâœ… GBDT è®­ç»ƒå®Œæˆ")
        print(f"   å®é™…è®­ç»ƒæ ‘æ•°é‡: {self.actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")
        print(f"   å¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})")

        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶ï¼ˆä¾›ç»¼åˆåˆ†æä½¿ç”¨ï¼‰
        accuracy_info = {
            'model_type': 'gbdt',
            'horizon': horizon,
            'accuracy': float(mean_accuracy),
            'std': float(std_accuracy),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        import json
        accuracy_file = 'data/model_accuracy.json'
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
            
            # æ›´æ–°å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
            key = f'gbdt_{horizon}d'
            existing_data[key] = accuracy_info
            
            # ä¿å­˜å›æ–‡ä»¶
            with open(accuracy_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            logger.info(f"å‡†ç¡®ç‡å·²ä¿å­˜åˆ° {accuracy_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜å‡†ç¡®ç‡å¤±è´¥: {e}")

        # ========== Step 2: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ ==========
        print("\n" + "="*70)
        logger.info("Step 2: åˆ†æ GBDT ç‰¹å¾é‡è¦æ€§")
        print("="*70)

        feat_imp = self.processor.analyze_feature_importance(
            self.gbdt_model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘
        try:
            contrib_values = self.gbdt_model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )

            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False)
            logger.info(r"å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§è‡³ output/gbdt_feature_importance.csv")

            # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾
            print("\nğŸ“Š GBDT Top 20 é‡è¦ç‰¹å¾ (å«å½±å“æ–¹å‘):")
            print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(20))

        except Exception as e:
            logger.warning(f"ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        print("\n" + "="*70)
        logger.info(r"GBDT æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*70)

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    logger.warning(f"è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)

            # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

            # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

            # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)

            # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
            stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
            for key, value in stock_type_features.items():
                stock_df[key] = value

            # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
            for key, value in sentiment_features.items():
                stock_df[key] = value

            # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
            topic_features = self.feature_engineer.create_topic_features(code, stock_df)
            for key, value in topic_features.items():
                stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

            # æ·»åŠ æ¿å—ç‰¹å¾
            sector_features = self.feature_engineer.create_sector_features(code, stock_df)
            for key, value in sector_features.items():
                stock_df[key] = value

            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

            # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_interaction_features(stock_df)

            # è·å–æœ€æ–°æ•°æ®
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨ï¼‰
            for col, encoder in self.categorical_encoders.items():
                if col in latest_data.columns:
                    # å¦‚æœé‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œæ˜ å°„åˆ°0
                    try:
                        latest_data[col] = encoder.transform(latest_data[col].astype(str))
                    except ValueError:
                        # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
                        logger.warning(f"è­¦å‘Š: åˆ†ç±»ç‰¹å¾ {col} åŒ…å«è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        latest_data[col] = 0

            X = latest_data[self.feature_columns].values

            # ä½¿ç”¨GBDTæ¨¡å‹ç›´æ¥é¢„æµ‹
            proba = self.gbdt_model.predict_proba(X)[0]
            prediction = self.gbdt_model.predict(X)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'gbdt_model': self.gbdt_model,
            'feature_columns': self.feature_columns,
            'actual_n_estimators': self.actual_n_estimators,
            'categorical_encoders': self.categorical_encoders
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"GBDT æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.gbdt_model = model_data['gbdt_model']
        self.feature_columns = model_data['feature_columns']
        self.actual_n_estimators = model_data['actual_n_estimators']
        self.categorical_encoders = model_data.get('categorical_encoders', {})
        print(f"GBDT æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class CatBoostModel(BaseTradingModel):
    """CatBoost æ¨¡å‹ - åŸºäº CatBoost æ¢¯åº¦æå‡ç®—æ³•çš„å•ä¸€æ¨¡å‹
    
    CatBoost æ˜¯ Yandex å¼€å‘çš„æ¢¯åº¦æå‡åº“ï¼Œå…·æœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š
    1. è‡ªåŠ¨å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œæ— éœ€æ‰‹åŠ¨ç¼–ç 
    2. æ›´å¥½çš„é»˜è®¤å‚æ•°ï¼Œå‡å°‘è°ƒå‚å·¥ä½œé‡
    3. æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆGPU æ”¯æŒï¼‰
    4. æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
    """

    def __init__(self):
        super().__init__()  # è°ƒç”¨åŸºç±»åˆå§‹åŒ–
        self.catboost_model = None
        self.actual_n_estimators = 0
        self.model_type = 'catboost'  # æ¨¡å‹ç±»å‹æ ‡è¯†

    def load_selected_features(self, filepath=None, current_feature_names=None):
        """åŠ è½½é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨ç‰¹å¾åç§°äº¤é›†ï¼Œç¡®ä¿ç‰¹å¾å­˜åœ¨ï¼‰

        Args:
            filepath: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„ï¼‰
            current_feature_names: å½“å‰æ•°æ®é›†çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        import os
        import glob

        if filepath is None:
            # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾åç§°æ–‡ä»¶
            pattern = 'output/selected_features_*.csv'
            files = glob.glob(pattern)
            if not files:
                return None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            filepath = max(files, key=os.path.getmtime)

        try:
            import pandas as pd
            # è¯»å–ç‰¹å¾åç§°
            df = pd.read_csv(filepath)
            selected_names = df['Feature_Name'].tolist()

            logger.debug(f"åŠ è½½ç‰¹å¾åˆ—è¡¨æ–‡ä»¶: {filepath}")
            logger.info(f"åŠ è½½äº† {len(selected_names)} ä¸ªé€‰æ‹©çš„ç‰¹å¾")

            # å¦‚æœæä¾›äº†å½“å‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨äº¤é›†
            if current_feature_names is not None:
                current_set = set(current_feature_names)
                selected_set = set(selected_names)
                available_set = current_set & selected_set
                
                available_names = list(available_set)
                logger.info(f"å½“å‰æ•°æ®é›†ç‰¹å¾æ•°é‡: {len(current_feature_names)}")
                logger.info(f"é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_names)}")
                logger.info(f"å®é™…å¯ç”¨çš„ç‰¹å¾æ•°é‡: {len(available_names)}")
                logger.warning(f" {len(selected_set) - len(available_set)} ä¸ªç‰¹å¾åœ¨å½“å‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
                
                return available_names
            else:
                return selected_names

        except Exception as e:
            logger.warning(f"åŠ è½½ç‰¹å¾åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1, for_backtest=False):
        """å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸åº”ç”¨horizonè¿‡æ»¤ï¼‰
        """
        self.horizon = horizon
        all_data = []

        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        logger.info("è·å–ç¾è‚¡å¸‚åœºæ•°æ®...")
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            logger.info(f"æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            logger.warning(r"æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        for code in codes:
            try:
                print(f"å¤„ç†è‚¡ç¥¨: {code}")

                # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€ï¼Œè…¾è®¯è´¢ç»æ¥å£ä¸éœ€è¦
                stock_code = code.replace('.HK', '')

                # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
                if stock_df is None or stock_df.empty:
                    continue

                # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                hsi_df = get_hsi_data_tencent(period_days=730)
                if hsi_df is None or hsi_df.empty:
                    continue

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon, for_backtest=for_backtest)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value

                # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
                sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
                for key, value in sentiment_features.items():
                    stock_df[key] = value

                # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
                topic_features = self.feature_engineer.create_topic_features(code, stock_df)
                for key, value in topic_features.items():
                    stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

                # æ·»åŠ æ¿å—ç‰¹å¾
                sector_features = self.feature_engineer.create_sector_features(code, stock_df)
                for key, value in sector_features.items():
                    stock_df[key] = value

                # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
                stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

                # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
                stock_df = self.feature_engineer.create_interaction_features(stock_df)

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                logger.warning(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                continue

        if len(all_data) == 0:
            raise ValueError("æ²¡æœ‰å¯ç”¨çš„æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df = pd.concat(all_data, ignore_index=False)

        # è½¬æ¢ç´¢å¼•ä¸º datetime
        df.index = pd.to_datetime(df.index)

        # è¿‡æ»¤æ—¥æœŸèŒƒå›´ï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if start_date:
            start_date = pd.to_datetime(start_date)
            df = df[df.index >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date)
            df = df[df.index <= end_date]

        logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆï¼Œå…± {len(df)} æ¡è®°å½•")

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1, use_feature_selection=False):
        """è®­ç»ƒ CatBoost æ¨¡å‹

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆåªä½¿ç”¨500ä¸ªé€‰æ‹©çš„ç‰¹å¾ï¼‰

        Returns:
            DataFrame: ç‰¹å¾é‡è¦æ€§æ•°æ®
        """
        print("\n" + "="*70)
        logger.info("å¼€å§‹è®­ç»ƒ CatBoost æ¨¡å‹")
        print("="*70)
        print(f"é¢„æµ‹å‘¨æœŸ: {horizon} å¤©")
        print(f"è‚¡ç¥¨æ•°é‡: {len(codes)}")
        print(f"ç‰¹å¾é€‰æ‹©: {'æ˜¯' if use_feature_selection else 'å¦'}")

        # ========== å‡†å¤‡æ•°æ® ==========
        print("\n" + "="*70)
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®")
        print("="*70)

        df = self.prepare_data(codes, start_date, end_date, horizon)

        # åˆ é™¤åŒ…å« NaN çš„è¡Œ
        df = df.dropna(subset=['Label'])
        print(f"åˆ é™¤ NaN å: {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        logger.info(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # ========== ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰==========
        if use_feature_selection:
            print("\n" + "="*70)
            print("ğŸ” åº”ç”¨ç‰¹å¾é€‰æ‹©")
            print("="*70)

            # åŠ è½½é€‰æ‹©çš„ç‰¹å¾
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)

            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                logger.info(f"ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                logger.warning(r"æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
        else:
            logger.info(f"ä½¿ç”¨å…¨éƒ¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # æ£€æŸ¥ç‰¹å¾åˆ—æ˜¯å¦å­˜åœ¨
        missing_features = [col for col in self.feature_columns if col not in df.columns]
        if missing_features:
            logger.warning(f"ä»¥ä¸‹ç‰¹å¾åˆ—ä¸å­˜åœ¨ï¼Œå°†è¢«è·³è¿‡: {missing_features[:10]}")
            self.feature_columns = [col for col in self.feature_columns if col in df.columns]

        logger.info(f"æœ€ç»ˆä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # å‡†å¤‡è®­ç»ƒæ•°æ® - å…ˆå¤„ç†åˆ†ç±»ç‰¹å¾
        from sklearn.preprocessing import LabelEncoder
        
        # è¯†åˆ«åˆ†ç±»ç‰¹å¾ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰
        self.categorical_encoders = {}
        categorical_features = []
        
        for col in self.feature_columns:
            if df[col].dtype == 'object':
                print(f"  æ£€æµ‹åˆ°åˆ†ç±»ç‰¹å¾: {col}")
                encoder = LabelEncoder()
                df[col] = encoder.fit_transform(df[col].astype(str))
                self.categorical_encoders[col] = encoder
                categorical_features.append(self.feature_columns.index(col))
        
        X = df[self.feature_columns].values
        y = df['Label'].values

        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        print(f"åˆ†ç±»ç‰¹å¾æ•°é‡: {len(categorical_features)}")

        # ========== è®­ç»ƒ CatBoost æ¨¡å‹ ==========
        print("\n" + "="*70)
        print("ğŸ± è®­ç»ƒ CatBoost æ¨¡å‹")
        print("="*70)

        # æ ¹æ®é¢„æµ‹å‘¨æœŸè°ƒæ•´å‚æ•°
        if horizon == 5:
            # ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
            print("ä½¿ç”¨ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆå‡å°‘æ ‘æ·±åº¦ï¼Œå¢åŠ æ—©åœè€å¿ƒï¼‰...")
            n_estimators = 500
            depth = 6  # å‡å°‘æ·±åº¦ï¼ˆ7â†’6ï¼‰
            learning_rate = 0.05
            stopping_rounds = 50  # å¢åŠ æ—©åœè€å¿ƒï¼ˆ30â†’50ï¼‰
            l2_leaf_reg = 3  # å¢åŠ L2æ­£åˆ™ï¼ˆ2â†’3ï¼‰
            subsample = 0.7
            colsample_bylevel = 0.6
        elif horizon == 1:
            # æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦ï¼‰
            print("ä½¿ç”¨æ¬¡æ—¥æ¨¡å‹å‚æ•°...")
            n_estimators = 500
            depth = 7
            learning_rate = 0.05
            stopping_rounds = 40
            l2_leaf_reg = 3
            subsample = 0.75
            colsample_bylevel = 0.7
        else:  # horizon == 20
            # ä¸€ä¸ªæœˆæ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼‰
            print("ä½¿ç”¨20å¤©æ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½è¿‡æ‹Ÿåˆï¼‰...")
            n_estimators = 400  # å‡å°‘æ ‘æ•°é‡ï¼ˆ500â†’400ï¼‰
            depth = 5  # å‡å°‘æ·±åº¦ï¼ˆ6â†’5ï¼‰
            learning_rate = 0.04  # é™ä½å­¦ä¹ ç‡ï¼ˆ0.05â†’0.04ï¼‰
            stopping_rounds = 60  # å¢åŠ æ—©åœè€å¿ƒï¼ˆ40â†’60ï¼‰
            l2_leaf_reg = 5  # å¢å¼ºL2æ­£åˆ™ï¼ˆ3â†’5ï¼‰
            subsample = 0.6  # å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.75â†’0.6ï¼‰
            colsample_bylevel = 0.6  # å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.7â†’0.6ï¼‰

        from catboost import CatBoostClassifier, Pool

        self.catboost_model = CatBoostClassifier(
            loss_function='Logloss',
            eval_metric='Accuracy',
            depth=depth,
            learning_rate=learning_rate,
            n_estimators=n_estimators,
            l2_leaf_reg=l2_leaf_reg,
            subsample=subsample,
            colsample_bylevel=colsample_bylevel,
            random_seed=2020,
            verbose=100,
            early_stopping_rounds=stopping_rounds,
            thread_count=-1,
            allow_writing_files=False,
            cat_features=categorical_features if categorical_features else None
        )

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        catboost_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            # åˆ›å»º Pool å¯¹è±¡ï¼ˆCatBoost æ¨èï¼‰
            train_pool = Pool(data=X_train_fold, label=y_train_fold, cat_features=categorical_features if categorical_features else None)
            val_pool = Pool(data=X_val_fold, label=y_val_fold, cat_features=categorical_features if categorical_features else None)

            self.catboost_model.fit(
                train_pool,
                eval_set=val_pool,
                verbose=False
            )

            y_pred_fold = self.catboost_model.predict(X_val_fold)
            score = accuracy_score(y_val_fold, y_pred_fold)
            catboost_scores.append(score)
            print(f"   Fold {fold} éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        full_pool = Pool(data=X, label=y, cat_features=categorical_features if categorical_features else None)
        self.catboost_model.fit(full_pool, verbose=100)

        # è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
        self.actual_n_estimators = self.catboost_model.tree_count_
        mean_accuracy = np.mean(catboost_scores)
        std_accuracy = np.std(catboost_scores)
        print(f"\nâœ… CatBoost è®­ç»ƒå®Œæˆ")
        print(f"   å®é™…è®­ç»ƒæ ‘æ•°é‡: {self.actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")
        print(f"   å¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})")

        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶ï¼ˆä¾›ç»¼åˆåˆ†æä½¿ç”¨ï¼‰
        accuracy_info = {
            'model_type': 'catboost',
            'horizon': horizon,
            'accuracy': float(mean_accuracy),
            'std': float(std_accuracy),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        accuracy_file = 'data/model_accuracy.json'
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
            
            # æ›´æ–°å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
            key = f'catboost_{horizon}d'
            existing_data[key] = accuracy_info
            
            # ä¿å­˜å›æ–‡ä»¶
            with open(accuracy_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            logger.info(f"å‡†ç¡®ç‡å·²ä¿å­˜åˆ° {accuracy_file}")
        except Exception as e:
            logger.warning(f"ä¿å­˜å‡†ç¡®ç‡å¤±è´¥: {e}")

        # ========== è¾“å‡º CatBoost ç‰¹å¾é‡è¦æ€§ ==========
        print("\n" + "="*70)
        logger.info("åˆ†æ CatBoost ç‰¹å¾é‡è¦æ€§")
        print("="*70)

        # CatBoost æä¾›å¤šç§ç‰¹å¾é‡è¦æ€§è®¡ç®—æ–¹æ³•
        feature_importance = self.catboost_model.get_feature_importance(prettified=True)
        feat_imp = pd.DataFrame({
            'Feature': [self.feature_columns[i] for i in range(len(self.feature_columns))],
            'Importance': self.catboost_model.feature_importances_
        })
        feat_imp = feat_imp.sort_values('Importance', ascending=False)

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘ï¼ˆä½¿ç”¨ SHAP å€¼ï¼‰
        try:
            # CatBoost çš„ get_feature_importance è¿”å›çš„æ˜¯é‡è¦æ€§æ’åº
            # ä½¿ç”¨ predict_contributions è·å–ç‰¹å¾è´¡çŒ®å€¼
            contrib_values = self.catboost_model.predict(X, prediction_type='RawFormulaVal')
            # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„è¾¹é™…è´¡çŒ®
            # å¯¹äºäºŒåˆ†ç±»é—®é¢˜ï¼ŒCatBoost çš„è´¡çŒ®å€¼è®¡ç®—æ¯”è¾ƒå¤æ‚
            # è¿™é‡Œä½¿ç”¨ç‰¹å¾é‡è¦æ€§ä½œä¸ºæ›¿ä»£ï¼Œå¹¶åŸºäºç‰¹å¾çš„é‡è¦æ€§æ–¹å‘æ¨æ–­
            # æ³¨æ„ï¼šCatBoost çš„ç‰¹å¾é‡è¦æ€§éƒ½æ˜¯æ­£æ•°ï¼Œæ— æ³•ç›´æ¥åˆ¤æ–­å½±å“æ–¹å‘
            # å› æ­¤æˆ‘ä»¬æ ‡è®°ä¸º 'Unknown'
            feat_imp['Impact_Direction'] = 'Unknown'
            logger.info("CatBoost ç‰¹å¾è´¡çŒ®åˆ†æï¼šç”±äº CatBoost ç‰¹å¾é‡è¦æ€§ä¸ºæ­£å€¼ï¼Œæ— æ³•ç›´æ¥åˆ¤æ–­å½±å“æ–¹å‘ï¼Œæ ‡è®°ä¸º Unknown")
        except Exception as e:
            logger.warning(f"CatBoost ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        feat_imp.to_csv('output/catboost_feature_importance.csv', index=False)
        logger.info(r"å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§è‡³ output/catboost_feature_importance.csv")

        # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾
        print("\nğŸ“Š CatBoost Top 20 é‡è¦ç‰¹å¾:")
        print(feat_imp[['Feature', 'Importance', 'Impact_Direction']].head(20))

        print("\n" + "="*70)
        logger.info(r"CatBoost æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*70)

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    logger.warning(f"è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)

            # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

            # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

            # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)

            # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
            stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
            for key, value in stock_type_features.items():
                stock_df[key] = value

            # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
            for key, value in sentiment_features.items():
                stock_df[key] = value

            # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
            topic_features = self.feature_engineer.create_topic_features(code, stock_df)
            for key, value in topic_features.items():
                stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

            # æ·»åŠ æ¿å—ç‰¹å¾
            sector_features = self.feature_engineer.create_sector_features(code, stock_df)
            for key, value in sector_features.items():
                stock_df[key] = value

            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

            # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_interaction_features(stock_df)

            # è·å–æœ€æ–°æ•°æ®
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨ï¼‰
            for col, encoder in self.categorical_encoders.items():
                if col in latest_data.columns:
                    try:
                        latest_data[col] = encoder.transform(latest_data[col].astype(str))
                    except ValueError:
                        # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«ï¼Œæ˜ å°„åˆ°0
                        logger.warning(f"è­¦å‘Š: åˆ†ç±»ç‰¹å¾ {col} åŒ…å«è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        latest_data[col] = 0

            X = latest_data[self.feature_columns].values

            # ä½¿ç”¨ CatBoost æ¨¡å‹ç›´æ¥é¢„æµ‹
            from catboost import Pool
            
            # è·å–åˆ†ç±»ç‰¹å¾ç´¢å¼•
            categorical_features = [self.feature_columns.index(col) for col in self.categorical_encoders.keys() if col in self.feature_columns]
            
            test_pool = Pool(data=X, cat_features=categorical_features if categorical_features else None)
            proba = self.catboost_model.predict_proba(test_pool)[0]
            prediction = self.catboost_model.predict(test_pool)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'catboost_model': self.catboost_model,
            'feature_columns': self.feature_columns,
            'actual_n_estimators': self.actual_n_estimators,
            'horizon': self.horizon,
            'model_type': self.model_type,
            'categorical_encoders': self.categorical_encoders
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"CatBoost æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.catboost_model = model_data['catboost_model']
        self.feature_columns = model_data['feature_columns']
        self.actual_n_estimators = model_data['actual_n_estimators']
        self.horizon = model_data.get('horizon', 1)
        self.model_type = model_data.get('model_type', 'catboost')
        self.categorical_encoders = model_data.get('categorical_encoders', {})
        print(f"CatBoost æ¨¡å‹å·²ä» {filepath} åŠ è½½")

    def predict_proba(self, X):
        """
        é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºå›æµ‹è¯„ä¼°å™¨ï¼‰

        Args:
            X: æµ‹è¯•æ•°æ®ï¼ˆDataFrame æˆ– numpy æ•°ç»„ï¼‰

        Returns:
            numpy.ndarray: é¢„æµ‹æ¦‚ç‡æ•°ç»„
        """
        from catboost import Pool
        import numpy as np

        # ç¡®ä¿ test_data æ˜¯ DataFrame
        if isinstance(X, pd.DataFrame):
            # æ£€æŸ¥ X æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„ç‰¹å¾åˆ—
            if all(col in X.columns for col in self.feature_columns):
                # æƒ…å†µ1ï¼šX åŒ…å«æ‰€æœ‰éœ€è¦çš„ç‰¹å¾åˆ—ï¼ˆå¯èƒ½æ˜¯åŸå§‹ DataFrameï¼‰
                test_df = X[self.feature_columns].copy()
            elif len(X.columns) == len(self.feature_columns):
                # æƒ…å†µ2ï¼šX å·²ç»æ˜¯åªåŒ…å«ç‰¹å¾åˆ—çš„ DataFrameï¼ˆåˆ—æ•°åŒ¹é…ï¼‰
                test_df = X.copy()
                test_df.columns = self.feature_columns  # ç¡®ä¿åˆ—åæ­£ç¡®
            else:
                # æƒ…å†µ3ï¼šX çš„åˆ—æ•°ä¸åŒ¹é…ï¼Œå°è¯•æå–å­˜åœ¨çš„åˆ—
                available_cols = [col for col in self.feature_columns if col in X.columns]
                if available_cols:
                    test_df = X[available_cols].copy()
                else:
                    raise ValueError(f"æ— æ³•ä»è¾“å…¥æ•°æ®ä¸­æå–ç‰¹å¾åˆ—ã€‚éœ€è¦çš„åˆ—ï¼š{self.feature_columns[:10]}...")
        else:
            # å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œè½¬æ¢ä¸º DataFrame
            test_df = pd.DataFrame(X, columns=self.feature_columns)

        # è·å–åˆ†ç±»ç‰¹å¾ç´¢å¼•
        categorical_features = [self.feature_columns.index(col) for col in self.categorical_encoders.keys() if col in self.feature_columns]

        # åˆ›å»º Pool å¯¹è±¡
        test_pool = Pool(data=test_df, cat_features=categorical_features if categorical_features else None)

        # è¿”å›é¢„æµ‹æ¦‚ç‡
        return self.catboost_model.predict_proba(test_pool)


class EnsembleModel:
    """èåˆæ¨¡å‹ - æ•´åˆ LightGBMã€GBDTã€CatBoost ä¸‰ä¸ªæ¨¡å‹
    
    æ”¯æŒå¤šç§èåˆæ–¹æ³•ï¼š
    1. ç®€å•å¹³å‡ï¼šä¸‰ä¸ªæ¨¡å‹çš„æ¦‚ç‡å¹³å‡
    2. åŠ æƒå¹³å‡ï¼šæ ¹æ®å‡†ç¡®ç‡åŠ æƒ
    3. æŠ•ç¥¨æœºåˆ¶ï¼šå¤šæ•°æŠ•ç¥¨
    """

    def __init__(self, fusion_method='weighted'):
        """
        Args:
            fusion_method: èåˆæ–¹æ³• ('average'/'weighted'/'voting')
        """
        self.lgbm_model = LightGBMModel()
        self.gbdt_model = GBDTModel()
        self.catboost_model = CatBoostModel()
        self.fusion_method = fusion_method
        self.model_accuracies = {}
        self.horizon = 1

    def load_model_accuracy(self):
        """åŠ è½½æ¨¡å‹å‡†ç¡®ç‡"""
        accuracy_file = 'data/model_accuracy.json'
        try:
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.model_accuracies = {
                    'lgbm': data.get(f'lgbm_{self.horizon}d', {}).get('accuracy', 0.5),
                    'gbdt': data.get(f'gbdt_{self.horizon}d', {}).get('accuracy', 0.5),
                    'catboost': data.get(f'catboost_{self.horizon}d', {}).get('accuracy', 0.5)
                }
                logger.info(f"å·²åŠ è½½æ¨¡å‹å‡†ç¡®ç‡: {self.model_accuracies}")
            else:
                logger.warning(r"æœªæ‰¾åˆ°å‡†ç¡®ç‡æ–‡ä»¶ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                self.model_accuracies = {'lgbm': 0.5, 'gbdt': 0.5, 'catboost': 0.5}
        except Exception as e:
            logger.warning(f"åŠ è½½å‡†ç¡®ç‡å¤±è´¥: {e}")
            self.model_accuracies = {'lgbm': 0.5, 'gbdt': 0.5, 'catboost': 0.5}

    def load_models(self, horizon=1):
        """åŠ è½½ä¸‰ä¸ªæ¨¡å‹"""
        self.horizon = horizon
        horizon_suffix = f'_{horizon}d'
        
        print("\n" + "="*70)
        print("ğŸ“¦ åŠ è½½èåˆæ¨¡å‹")
        print("="*70)
        
        # åŠ è½½ LightGBM æ¨¡å‹
        lgbm_path = f'data/ml_trading_model_lgbm{horizon_suffix}.pkl'
        if os.path.exists(lgbm_path):
            self.lgbm_model.load_model(lgbm_path)
            logger.info(f"LightGBM æ¨¡å‹å·²åŠ è½½")
        else:
            logger.warning(f"LightGBM æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {lgbm_path}")
        
        # åŠ è½½ GBDT æ¨¡å‹
        gbdt_path = f'data/ml_trading_model_gbdt{horizon_suffix}.pkl'
        if os.path.exists(gbdt_path):
            self.gbdt_model.load_model(gbdt_path)
            logger.info(f"GBDT æ¨¡å‹å·²åŠ è½½")
        else:
            logger.warning(f"GBDT æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {gbdt_path}")
        
        # åŠ è½½ CatBoost æ¨¡å‹
        catboost_path = f'data/ml_trading_model_catboost{horizon_suffix}.pkl'
        if os.path.exists(catboost_path):
            self.catboost_model.load_model(catboost_path)
            logger.info(f"CatBoost æ¨¡å‹å·²åŠ è½½")
        else:
            logger.warning(f"CatBoost æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {catboost_path}")
        
        # åŠ è½½æ¨¡å‹å‡†ç¡®ç‡
        self.load_model_accuracy()
        
        print("="*70)

    def predict(self, code, predict_date=None):
        """èåˆé¢„æµ‹
        
        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ
            
        Returns:
            dict: èåˆé¢„æµ‹ç»“æœ
        """
        # è·å–ä¸‰ä¸ªæ¨¡å‹çš„é¢„æµ‹ç»“æœ
        lgbm_result = self.lgbm_model.predict(code, predict_date, self.horizon)
        gbdt_result = self.gbdt_model.predict(code, predict_date, self.horizon)
        catboost_result = self.catboost_model.predict(code, predict_date, self.horizon)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹é¢„æµ‹å¤±è´¥
        results = {'lgbm': lgbm_result, 'gbdt': gbdt_result, 'catboost': catboost_result}
        valid_results = {k: v for k, v in results.items() if v is not None}
        
        if len(valid_results) == 0:
            logger.error(f"æ‰€æœ‰æ¨¡å‹é¢„æµ‹å¤±è´¥: {code}")
            return None
        
        # è·å–æ¦‚ç‡å’Œé¢„æµ‹
        probabilities = []
        predictions = []
        
        for model_name, result in valid_results.items():
            probabilities.append(result['probability'])
            predictions.append(result['prediction'])
        
        # èåˆ
        if self.fusion_method == 'average':
            # ç®€å•å¹³å‡
            fused_prob = np.mean(probabilities)
            fused_pred = 1 if fused_prob > 0.5 else 0
            method_name = "ç®€å•å¹³å‡"
        elif self.fusion_method == 'weighted':
            # åŠ æƒå¹³å‡ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰
            weights = []
            for model_name in valid_results.keys():
                weights.append(self.model_accuracies.get(model_name, 0.5))
            
            total_weight = sum(weights)
            if total_weight > 0:
                fused_prob = sum(p * w for p, w in zip(probabilities, weights)) / total_weight
            else:
                fused_prob = np.mean(probabilities)
            
            fused_pred = 1 if fused_prob > 0.5 else 0
            method_name = "åŠ æƒå¹³å‡"
        else:  # voting
            # æŠ•ç¥¨æœºåˆ¶
            fused_pred = 1 if sum(predictions) >= len(predictions) / 2 else 0
            fused_prob = sum(predictions) / len(predictions)
            method_name = "æŠ•ç¥¨æœºåˆ¶"
        
        # è®¡ç®—ä¸€è‡´æ€§å’Œç½®ä¿¡åº¦
        # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§æ¯”ä¾‹
        if len(valid_results) == 3:
            # ä¸‰ä¸ªæ¨¡å‹ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
            if predictions.count(predictions[0]) == 3:
                consistency_pct = 100  # ä¸‰æ¨¡å‹ä¸€è‡´
            elif predictions.count(predictions[0]) == 2 or predictions.count(predictions[1]) == 2:
                consistency_pct = 67  # ä¸¤æ¨¡å‹ä¸€è‡´
            else:
                consistency_pct = 33  # ä¸‰æ¨¡å‹ä¸ä¸€è‡´
        elif len(valid_results) == 2:
            # ä¸¤ä¸ªæ¨¡å‹ï¼Œæ£€æŸ¥ä¸€è‡´æ€§
            if predictions.count(predictions[0]) == 2:
                consistency_pct = 100  # ä¸¤æ¨¡å‹ä¸€è‡´
            else:
                consistency_pct = 50  # ä¸¤æ¨¡å‹ä¸ä¸€è‡´
        else:
            # åªæœ‰ä¸€ä¸ªæ¨¡å‹
            consistency_pct = 100
        
        # è®¡ç®—ç½®ä¿¡åº¦å’Œé¢„æµ‹æ–¹å‘ï¼ˆåŸºäºèåˆæ¦‚ç‡ï¼‰
        # ä¸‰åˆ†ç±»ï¼šä¸Šæ¶¨(1)ã€è§‚æœ›(0.5)ã€ä¸‹è·Œ(0)
        if fused_prob > 0.60:
            confidence = "é«˜"
            fused_direction = 1  # ä¸Šæ¶¨
        elif fused_prob > 0.50:
            confidence = "ä¸­"
            fused_direction = 0.5  # è§‚æœ›
        else:
            confidence = "ä½"
            fused_direction = 0  # ä¸‹è·Œ
        
        # æ„å»ºç»“æœ
        result = {
            'code': code,
            'name': STOCK_NAMES.get(code, code),
            'fusion_method': method_name,
            'fused_prediction': fused_direction,  # ä¸Šæ¶¨=1, è§‚æœ›=0.5, ä¸‹è·Œ=0
            'fused_probability': float(fused_prob),
            'confidence': confidence,
            'consistency': f"{consistency_pct}%",
            'current_price': valid_results[list(valid_results.keys())[0]]['current_price'],
            'date': valid_results[list(valid_results.keys())[0]]['date'],
            'model_predictions': {}
        }
        
        # æ·»åŠ å„æ¨¡å‹çš„é¢„æµ‹ç»“æœ
        for model_name, pred_result in valid_results.items():
            result['model_predictions'][model_name] = {
                'prediction': int(pred_result['prediction']),
                'probability': float(pred_result['probability'])
            }
        
        return result
    
    def predict_batch(self, codes, predict_date=None):
        """æ‰¹é‡é¢„æµ‹
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            predict_date: é¢„æµ‹æ—¥æœŸ
            
        Returns:
            list: èåˆé¢„æµ‹ç»“æœåˆ—è¡¨
        """
        results = []
        for code in codes:
            result = self.predict(code, predict_date=predict_date)
            if result:
                results.append(result)
        return results
    
    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡ï¼ˆç”¨äºå›æµ‹è¯„ä¼°å™¨ï¼‰

        Args:
            X: ç‰¹å¾æ•°æ®ï¼ˆnumpy array æˆ– DataFrameï¼‰

        Returns:
            numpy array: æ¦‚ç‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (n_samples, 2)
        """
        import numpy as np

        # ä½¿ç”¨åŠ æƒå¹³å‡èåˆé¢„æµ‹æ¦‚ç‡
        n_samples = len(X)
        probabilities = np.zeros((n_samples, 2))

        # è·å–æ¯ä¸ªæ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡
        # LightGBM å’Œ GBDT å¯ä»¥ç›´æ¥ä½¿ç”¨ X
        lgbm_probs = self.lgbm_model.model.predict_proba(X)
        gbdt_probs = self.gbdt_model.gbdt_model.predict_proba(X)

        # CatBoost éœ€è¦ç‰¹æ®Šå¤„ç†åˆ†ç±»ç‰¹å¾
        # æ£€æŸ¥ X æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„ç‰¹å¾åˆ—
        if isinstance(X, pd.DataFrame):
            # æ£€æŸ¥ X æ˜¯å¦åŒ…å«æ‰€æœ‰ç‰¹å¾åˆ—ï¼ˆæŒ‰åç§°åŒ¹é…ï¼‰
            if all(col in X.columns for col in self.catboost_model.feature_columns):
                # X åŒ…å«æ‰€æœ‰ç‰¹å¾åˆ—ï¼ŒæŒ‰é¡ºåºæå–
                test_df = X[self.catboost_model.feature_columns].copy()
            elif len(X.columns) == len(self.catboost_model.feature_columns):
                # X çš„åˆ—æ•°åŒ¹é…ä½†åˆ—åå¯èƒ½ä¸åŒ
                # å‡è®¾ X çš„åˆ—é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼ˆè¿™æ˜¯å›æµ‹è¯„ä¼°å™¨çš„é»˜è®¤è¡Œä¸ºï¼‰
                test_df = X.copy()
                test_df.columns = self.catboost_model.feature_columns
            else:
                # X çš„åˆ—æ•°ä¸åŒ¹é…ï¼Œå°è¯•æå–å­˜åœ¨çš„åˆ—
                available_cols = [col for col in self.catboost_model.feature_columns if col in X.columns]
                if len(available_cols) == len(self.catboost_model.feature_columns):
                    # æ‰€æœ‰ç‰¹å¾éƒ½å­˜åœ¨ï¼ŒæŒ‰é¡ºåºæå–
                    test_df = X[self.catboost_model.feature_columns].copy()
                elif len(available_cols) > 0:
                    # éƒ¨åˆ†ç‰¹å¾å­˜åœ¨ï¼Œè¡¥é½ç¼ºå¤±çš„ç‰¹å¾
                    test_df = X[available_cols].copy()
                    for col in self.catboost_model.feature_columns:
                        if col not in test_df.columns:
                            test_df[col] = 0.0
                    test_df = test_df[self.catboost_model.feature_columns]
                else:
                    # æ— æ³•æå–ç‰¹å¾åˆ—ï¼Œå‡è®¾åˆ—é¡ºåºä¸€è‡´
                    test_df = X.copy()
                    if len(test_df.columns) >= len(self.catboost_model.feature_columns):
                        test_df = test_df.iloc[:, :len(self.catboost_model.feature_columns)]
                        test_df.columns = self.catboost_model.feature_columns
                    else:
                        raise ValueError(f"æ— æ³•ä»è¾“å…¥æ•°æ®ä¸­æå–ç‰¹å¾åˆ—: X æœ‰ {len(X.columns)} åˆ—ï¼Œéœ€è¦ {len(self.catboost_model.feature_columns)} åˆ—")
        else:
            # å¦‚æœæ˜¯ numpy æ•°ç»„ï¼Œè½¬æ¢ä¸º DataFrame
            test_df = pd.DataFrame(X, columns=self.catboost_model.feature_columns)

        # è·å–åˆ†ç±»ç‰¹å¾ç´¢å¼•
        categorical_features = [self.catboost_model.feature_columns.index(col) for col in self.catboost_model.categorical_encoders.keys() if col in self.catboost_model.feature_columns]

        # ç¡®ä¿åˆ†ç±»ç‰¹å¾åˆ—æ˜¯æ•´æ•°ç±»å‹
        for cat_idx in categorical_features:
            col_name = self.catboost_model.feature_columns[cat_idx]
            if col_name in test_df.columns:
                test_df[col_name] = test_df[col_name].astype(np.int32)

        # ä½¿ç”¨ Pool å¯¹è±¡è¿›è¡Œé¢„æµ‹
        from catboost import Pool
        test_pool = Pool(data=test_df)
        catboost_probs = self.catboost_model.catboost_model.predict_proba(test_pool)

        # è®¡ç®—æƒé‡
        if self.fusion_method == 'weighted':
            lgbm_weight = self.model_accuracies.get('lgbm', 0.5)
            gbdt_weight = self.model_accuracies.get('gbdt', 0.5)
            catboost_weight = self.model_accuracies.get('catboost', 0.5)
            total_weight = lgbm_weight + gbdt_weight + catboost_weight

            if total_weight > 0:
                lgbm_weight /= total_weight
                gbdt_weight /= total_weight
                catboost_weight /= total_weight
            else:
                lgbm_weight = gbdt_weight = catboost_weight = 1.0 / 3.0
        else:
            # ç®€å•å¹³å‡
            lgbm_weight = gbdt_weight = catboost_weight = 1.0 / 3.0

        # åŠ æƒèåˆ
        probabilities = (
            lgbm_weight * lgbm_probs +
            gbdt_weight * gbdt_probs +
            catboost_weight * catboost_probs
        )

        return probabilities
    
    def predict_classes(self, X):
        """é¢„æµ‹ç±»åˆ«ï¼ˆç”¨äºå›æµ‹è¯„ä¼°å™¨ï¼‰
        
        Args:
            X: ç‰¹å¾æ•°æ®
            
        Returns:
            numpy array: é¢„æµ‹ç±»åˆ«ï¼ˆ0æˆ–1ï¼‰
        """
        probabilities = self.predict_proba(X)
        return (probabilities[:, 1] > 0.5).astype(int)
    
    def save_predictions(self, predictions, filepath=None):
        """ä¿å­˜é¢„æµ‹ç»“æœåˆ° CSV
        
        Args:
            predictions: é¢„æµ‹ç»“æœåˆ—è¡¨
            filepath: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        if filepath is None:
            filepath = f'data/ml_trading_model_ensemble_predictions_{self.horizon}d.csv'
        
        # è½¬æ¢ä¸º DataFrame
        data = []
        for pred in predictions:
            row = {
                'code': pred['code'],
                'name': pred['name'],
                'fusion_method': pred['fusion_method'],
                'fused_prediction': pred['fused_prediction'],
                'fused_probability': pred['fused_probability'],
                'confidence': pred['confidence'],
                'consistency': pred['consistency'],
                'current_price': pred['current_price'],
                'date': pred['date'].strftime('%Y-%m-%d')
            }
            
            # æ·»åŠ å„æ¨¡å‹çš„é¢„æµ‹ç»“æœ
            for model_name, model_pred in pred['model_predictions'].items():
                row[f'{model_name}_prediction'] = model_pred['prediction']
                row[f'{model_name}_probability'] = model_pred['probability']
            
            data.append(row)
        
        df = pd.DataFrame(data)
        df.to_csv(filepath, index=False)
        logger.info(f"èåˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {filepath}")
        
        return df


def main():
    parser = argparse.ArgumentParser(description='æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'predict', 'evaluate'],
                       help='è¿è¡Œæ¨¡å¼: train=è®­ç»ƒ, predict=é¢„æµ‹, evaluate=è¯„ä¼°')
    parser.add_argument('--model-type', type=str, default='lgbm', choices=['lgbm', 'gbdt', 'catboost', 'ensemble'],
                       help='æ¨¡å‹ç±»å‹: lgbm=å•ä¸€LightGBMæ¨¡å‹, gbdt=å•ä¸€GBDTæ¨¡å‹, catboost=å•ä¸€CatBoostæ¨¡å‹, ensemble=èåˆæ¨¡å‹ï¼ˆé»˜è®¤lgbmï¼‰')
    parser.add_argument('--model-path', type=str, default='data/ml_trading_model.pkl',
                       help='æ¨¡å‹ä¿å­˜/åŠ è½½è·¯å¾„')
    parser.add_argument('--start-date', type=str, default=None,
                       help='è®­ç»ƒå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='è®­ç»ƒç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--predict-date', type=str, default=None,
                       help='é¢„æµ‹æ—¥æœŸï¼šåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ (YYYY-MM-DD)ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥')
    parser.add_argument('--horizon', type=int, default=1, choices=[1, 5, 20],
                       help='é¢„æµ‹å‘¨æœŸ: 1=æ¬¡æ—¥ï¼ˆé»˜è®¤ï¼‰, 5=ä¸€å‘¨, 20=ä¸€ä¸ªæœˆ')
    parser.add_argument('--use-feature-selection', action='store_true',
                       help='ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆåªä½¿ç”¨500ä¸ªé€‰æ‹©çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯å…¨éƒ¨2936ä¸ªï¼‰')
    parser.add_argument('--skip-feature-selection', action='store_true',
                       help='è·³è¿‡ç‰¹å¾é€‰æ‹©ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰çš„ç‰¹å¾æ–‡ä»¶ï¼ˆé€‚ç”¨äºæ‰¹é‡è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼‰')
    parser.add_argument('--fusion-method', type=str, default='weighted', choices=['average', 'weighted', 'voting'],
                       help='èåˆæ–¹æ³•: average=ç®€å•å¹³å‡, weighted=åŠ æƒå¹³å‡ï¼ˆåŸºäºå‡†ç¡®ç‡ï¼‰, voting=æŠ•ç¥¨æœºåˆ¶ï¼ˆé»˜è®¤weightedï¼‰')

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨¡å‹
    if args.model_type == 'ensemble':
        logger.info("=" * 70)
        print(f"ğŸ­ ä½¿ç”¨èåˆæ¨¡å‹ï¼ˆæ–¹æ³•: {args.fusion_method}ï¼‰")
        logger.info("=" * 70)
        lgbm_model = None
        gbdt_model = None
        catboost_model = None
        ensemble_model = EnsembleModel(fusion_method=args.fusion_method)
    elif args.model_type == 'gbdt':
        logger.info("=" * 70)
        logger.info("ä½¿ç”¨å•ä¸€ GBDT æ¨¡å‹")
        logger.info("=" * 70)
        lgbm_model = None
        gbdt_model = GBDTModel()
        catboost_model = None
        ensemble_model = None
    elif args.model_type == 'catboost':
        logger.info("=" * 70)
        print("ğŸ± ä½¿ç”¨å•ä¸€ CatBoost æ¨¡å‹")
        logger.info("=" * 70)
        lgbm_model = None
        gbdt_model = None
        catboost_model = CatBoostModel()
        ensemble_model = None
    else:
        logger.info("=" * 70)
        logger.info("ä½¿ç”¨å•ä¸€ LightGBM æ¨¡å‹")
        logger.info("=" * 70)
        lgbm_model = MLTradingModel()
        gbdt_model = None
        catboost_model = None
        ensemble_model = None

    if args.mode == 'train':
        logger.info("=" * 50)
        print("è®­ç»ƒæ¨¡å¼")
        logger.info("=" * 50)

        # è®­ç»ƒæ¨¡å‹
        horizon_suffix = f'_{args.horizon}d'
        
        # æ£€æŸ¥æ˜¯å¦åº”ç”¨ç‰¹å¾é€‰æ‹©
        # --use-feature-selection: åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆåŠ è½½å·²æœ‰ç‰¹å¾æ–‡ä»¶æˆ–è¿è¡Œç‰¹å¾é€‰æ‹©ï¼‰
        # --skip-feature-selection: è·³è¿‡è¿è¡Œç‰¹å¾é€‰æ‹©è¿‡ç¨‹ï¼Œä½†ä»åŠ è½½å·²æœ‰ç‰¹å¾æ–‡ä»¶
        apply_feature_selection = args.use_feature_selection
        run_feature_selection = args.use_feature_selection and not args.skip_feature_selection
        
        if ensemble_model:
            # èåˆæ¨¡å‹éœ€è¦è®­ç»ƒä¸‰ä¸ªå­æ¨¡å‹
            print("\n" + "=" * 70)
            print("ğŸ­ è®­ç»ƒèåˆæ¨¡å‹çš„ä¸‰ä¸ªå­æ¨¡å‹")
            logger.info("=" * 70)
            
            # è®­ç»ƒ LightGBM æ¨¡å‹
            print("\nğŸ“Š è®­ç»ƒ LightGBM æ¨¡å‹...")
            lgbm_model = LightGBMModel()
            lgbm_feature_importance = lgbm_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.save_model(lgbm_model_path)
            lgbm_importance_path = lgbm_model_path.replace('.pkl', '_importance.csv')
            lgbm_feature_importance.to_csv(lgbm_importance_path, index=False)
            logger.info(f"LightGBM æ¨¡å‹å·²ä¿å­˜åˆ° {lgbm_model_path}")
            logger.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {lgbm_importance_path}")
            
            # è®­ç»ƒ GBDT æ¨¡å‹
            print("\nğŸ“Š è®­ç»ƒ GBDT æ¨¡å‹...")
            gbdt_model = GBDTModel()
            gbdt_feature_importance = gbdt_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            gbdt_model.save_model(gbdt_model_path)
            gbdt_importance_path = gbdt_model_path.replace('.pkl', '_importance.csv')
            gbdt_feature_importance.to_csv(gbdt_importance_path, index=False)
            logger.info(f"GBDT æ¨¡å‹å·²ä¿å­˜åˆ° {gbdt_model_path}")
            logger.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {gbdt_importance_path}")
            
            # è®­ç»ƒ CatBoost æ¨¡å‹
            print("\nğŸ“Š è®­ç»ƒ CatBoost æ¨¡å‹...")
            catboost_model = CatBoostModel()
            catboost_feature_importance = catboost_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            catboost_model_path = args.model_path.replace('.pkl', f'_catboost{horizon_suffix}.pkl')
            catboost_model.save_model(catboost_model_path)
            catboost_importance_path = catboost_model_path.replace('.pkl', '_importance.csv')
            catboost_feature_importance.to_csv(catboost_importance_path, index=False)
            logger.info(f"CatBoost æ¨¡å‹å·²ä¿å­˜åˆ° {catboost_model_path}")
            logger.info(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {catboost_importance_path}")
            
            print("\n" + "=" * 70)
            logger.info(r"èåˆæ¨¡å‹çš„æ‰€æœ‰å­æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            logger.info("=" * 70)
        elif lgbm_model:
            feature_importance = lgbm_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.save_model(lgbm_model_path)
            importance_path = lgbm_model_path.replace('.pkl', '_importance.csv')
            feature_importance.to_csv(importance_path, index=False)
            print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")
        elif catboost_model:
            feature_importance = catboost_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            catboost_model_path = args.model_path.replace('.pkl', f'_catboost{horizon_suffix}.pkl')
            catboost_model.save_model(catboost_model_path)
            importance_path = catboost_model_path.replace('.pkl', '_importance.csv')
            feature_importance.to_csv(importance_path, index=False)
            print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")
        else:
            feature_importance = gbdt_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=apply_feature_selection)
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            gbdt_model.save_model(gbdt_model_path)
            importance_path = gbdt_model_path.replace('.pkl', '_importance.csv')
            feature_importance.to_csv(importance_path, index=False)
            print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")

    elif args.mode == 'predict':
        logger.info("=" * 50)
        print("é¢„æµ‹æ¨¡å¼")
        logger.info("=" * 50)

        # åŠ è½½æ¨¡å‹
        horizon_suffix = f'_{args.horizon}d'
        if ensemble_model:
            # åŠ è½½èåˆæ¨¡å‹
            ensemble_model.load_models(args.horizon)
            model_name = f"èåˆæ¨¡å‹ï¼ˆ{ensemble_model.fusion_method}ï¼‰"
            model_file_suffix = "ensemble"
        elif lgbm_model:
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.load_model(lgbm_model_path)
            model = lgbm_model
            model_name = "LightGBM"
            model_file_suffix = "lgbm"
        elif catboost_model:
            catboost_model_path = args.model_path.replace('.pkl', f'_catboost{horizon_suffix}.pkl')
            catboost_model.load_model(catboost_model_path)
            model = catboost_model
            model_name = "CatBoost"
            model_file_suffix = "catboost"
        else:
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            gbdt_model.load_model(gbdt_model_path)
            model = gbdt_model
            model_name = "GBDT"
            model_file_suffix = "gbdt"

        print(f"å·²åŠ è½½ {model_name} æ¨¡å‹")

        # é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
        predictions = []
        if args.predict_date:
            print(f"åŸºäºæ—¥æœŸ: {args.predict_date}")
        
        if ensemble_model:
            # ä½¿ç”¨èåˆæ¨¡å‹é¢„æµ‹
            predictions = ensemble_model.predict_batch(WATCHLIST, args.predict_date)
        else:
            # ä½¿ç”¨å•ä¸€æ¨¡å‹é¢„æµ‹
            for code in WATCHLIST:
                result = model.predict(code, predict_date=args.predict_date)
                if result:
                    predictions.append(result)

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        print("\né¢„æµ‹ç»“æœ:")
        horizon_text = {1: "æ¬¡æ—¥", 5: "ä¸€å‘¨", 20: "ä¸€ä¸ªæœˆ"}.get(args.horizon, f"{args.horizon}å¤©")
        if args.predict_date:
            print(f"è¯´æ˜: åŸºäº {args.predict_date} çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
        else:
            print(f"è¯´æ˜: åŸºäºæœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
        
        if ensemble_model:
            # èåˆæ¨¡å‹è¾“å‡ºæ ¼å¼
            print("-" * 140)
            print(f"{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'èåˆé¢„æµ‹':<10} {'èåˆæ¦‚ç‡':<12} {'ç½®ä¿¡åº¦':<15} {'ä¸€è‡´æ€§':<10} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15}")
            print("-" * 140)
            
            for pred in predictions:
                # ä¸‰åˆ†ç±»ï¼šä¸Šæ¶¨=1, è§‚æœ›=0.5, ä¸‹è·Œ=0
                if pred['fused_prediction'] == 1:
                    pred_label = "ä¸Šæ¶¨"
                elif pred['fused_prediction'] == 0.5:
                    pred_label = "è§‚æœ›"
                else:
                    pred_label = "ä¸‹è·Œ"
                data_date = pred['date'].strftime('%Y-%m-%d')
                
                print(f"{pred['code']:<10} {pred['name']:<12} {pred_label:<10} {pred['fused_probability']:.4f}   {pred['confidence']:<15} {pred['consistency']:<10} {pred['current_price']:.2f}        {data_date:<15}")
                
                # æ˜¾ç¤ºå„æ¨¡å‹é¢„æµ‹è¯¦æƒ…
                print(f"         å„æ¨¡å‹: ", end="")
                for model_name, model_pred in pred['model_predictions'].items():
                    model_pred_label = "ä¸Šæ¶¨" if model_pred['prediction'] == 1 else "ä¸‹è·Œ"
                    print(f"{model_name}={model_pred_label}({model_pred['probability']:.4f}) ", end="")
                print()
        else:
            # å•ä¸€æ¨¡å‹è¾“å‡ºæ ¼å¼
            print("-" * 100)
            print(f"{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹':<8} {'æ¦‚ç‡':<10} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡':<15}")
            print("-" * 100)

            for pred in predictions:
                pred_label = "ä¸Šæ¶¨" if pred['prediction'] == 1 else "ä¸‹è·Œ"
                data_date = pred['date'].strftime('%Y-%m-%d')
                target_date = get_target_date(pred['date'], horizon=args.horizon)

                print(f"{pred['code']:<10} {pred['name']:<12} {pred_label:<8} {pred['probability']:.4f}    {pred['current_price']:.2f}        {data_date:<15} {target_date:<15}")

        # ä¿å­˜é¢„æµ‹ç»“æœ
        if ensemble_model:
            # ä¿å­˜èåˆé¢„æµ‹ç»“æœ
            ensemble_model.save_predictions(predictions)
            print(f"\nèåˆé¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° data/ml_trading_model_ensemble_predictions_{args.horizon}d.csv")
        else:
            # ä¿å­˜å•ä¸€æ¨¡å‹é¢„æµ‹ç»“æœ
            pred_df = pd.DataFrame(predictions)
            pred_df['data_date'] = pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
            pred_df['target_date'] = pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))

            pred_df_export = pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']]

            pred_path = args.model_path.replace('.pkl', f'_{model_file_suffix}_predictions{horizon_suffix}.csv')
            pred_df_export.to_csv(pred_path, index=False)
            print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {pred_path}")

            # ä¿å­˜20å¤©é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ï¼ˆä¾¿äºåç»­æå–å’Œå¯¹æ¯”ï¼‰
            if args.horizon == 20:
                save_predictions_to_text(pred_df_export, args.predict_date)

    elif args.mode == 'evaluate':
        logger.info("=" * 50)
        print("è¯„ä¼°æ¨¡å¼")
        logger.info("=" * 50)

        if args.model_type == 'both':
            # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
            print("\nåŠ è½½æ¨¡å‹...")
            horizon_suffix = f'_{args.horizon}d'
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            
            lgbm_model.load_model(lgbm_model_path)
            gbdt_model.load_model(gbdt_model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = lgbm_model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[lgbm_model.feature_columns].values
            y_test = test_df['Label'].values

            # LGBM æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ³ LightGBM æ¨¡å‹è¯„ä¼°")
            print("="*70)
            y_pred_lgbm = lgbm_model.model.predict(X_test)
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_lgbm))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_lgbm))
            lgbm_accuracy = accuracy_score(y_test, y_pred_lgbm)
            print(f"\nå‡†ç¡®ç‡: {lgbm_accuracy:.4f}")

            # GBDT æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ² GBDT æ¨¡å‹è¯„ä¼°")
            print("="*70)
            y_pred_gbdt = gbdt_model.gbdt_model.predict(X_test)

            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_gbdt))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_gbdt))
            gbdt_accuracy = accuracy_score(y_test, y_pred_gbdt)
            print(f"\nå‡†ç¡®ç‡: {gbdt_accuracy:.4f}")

            # å¯¹æ¯”ç»“æœ
            print("\n" + "="*70)
            logger.info("æ¨¡å‹å¯¹æ¯”")
            print("="*70)
            print(f"LightGBM å‡†ç¡®ç‡: {lgbm_accuracy:.4f}")
            print(f"GBDT å‡†ç¡®ç‡: {gbdt_accuracy:.4f}")
            print(f"å‡†ç¡®ç‡å·®å¼‚: {abs(lgbm_accuracy - gbdt_accuracy):.4f}")
            
            if gbdt_accuracy > lgbm_accuracy:
                print(f"\nâœ… GBDT æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {gbdt_accuracy - lgbm_accuracy:.4f} ({(gbdt_accuracy - lgbm_accuracy)/lgbm_accuracy*100:.2f}%)")
            elif lgbm_accuracy > gbdt_accuracy:
                print(f"\nâœ… LightGBM æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {lgbm_accuracy - gbdt_accuracy:.4f} ({(lgbm_accuracy - gbdt_accuracy)/gbdt_accuracy*100:.2f}%)")
            else:
                print(f"\nâš–ï¸  ä¸¤ç§æ¨¡å‹è¡¨ç°ç›¸åŒ")

        else:
            # å•ä¸ªæ¨¡å‹è¯„ä¼°
            model = lgbm_model if lgbm_model else gbdt_model
            model.load_model(args.model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[model.feature_columns].values
            y_test = test_df['Label'].values

            # ä½¿ç”¨æ¨¡å‹ç›´æ¥é¢„æµ‹
            y_pred = model.gbdt_model.predict(X_test)

            # è¯„ä¼°
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred))

            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred))

            print(f"\nå‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

    else:
        logger.error(f"ä¸æ”¯æŒçš„è¿è¡Œæ¨¡å¼: {args.mode}")
        print("è¯·ä½¿ç”¨ä»¥ä¸‹æ¨¡å¼ä¹‹ä¸€: train, evaluate, predict")
        sys.exit(1)


# å‘åå…¼å®¹åˆ«å
MLTradingModel = LightGBMModel


if __name__ == '__main__':
    main()
