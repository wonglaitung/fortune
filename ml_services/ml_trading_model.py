#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹ - äºŒåˆ†ç±»æ¨¡å‹é¢„æµ‹æ¬¡æ—¥æ¶¨è·Œ
æ•´åˆæŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢ã€èµ„é‡‘æµå‘ç­‰ç‰¹å¾ï¼Œä½¿ç”¨LightGBMè¿›è¡Œè®­ç»ƒ
"""

import warnings
import os
import sys
import argparse
from datetime import datetime, timedelta
import pickle
import hashlib
import json
from concurrent.futures import ThreadPoolExecutor, as_completed

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

# ç¼“å­˜é…ç½®
CACHE_DIR = 'data/stock_cache'
STOCK_DATA_CACHE_DAYS = 7  # è‚¡ç¥¨å†å²æ•°æ®ç¼“å­˜7å¤©
HSI_DATA_CACHE_HOURS = 1   # æ’ç”ŸæŒ‡æ•°æ•°æ®ç¼“å­˜1å°æ—¶

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data_services.tencent_finance import get_hk_stock_data_tencent, get_hsi_data_tencent
from data_services.technical_analysis import TechnicalAnalyzer
from data_services.fundamental_data import get_comprehensive_fundamental_data
from ml_services.base_model_processor import BaseModelProcessor
from ml_services.us_market_data import us_market_data
from config import WATCHLIST as STOCK_LIST

# è‚¡ç¥¨åç§°æ˜ å°„
STOCK_NAMES = STOCK_LIST

# è‡ªé€‰è‚¡åˆ—è¡¨ï¼ˆè½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼‰
WATCHLIST = list(STOCK_NAMES.keys())


# ========== ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ ==========
def save_predictions_to_text(predictions_df, predict_date=None):
    """
    ä¿å­˜é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œæ–¹ä¾¿åç»­æå–å’Œå¯¹æ¯”

    å‚æ•°:
    - predictions_df: é¢„æµ‹ç»“æœDataFrame
    - predict_date: é¢„æµ‹æ—¥æœŸ
    """
    try:
        from datetime import datetime

        # ç”Ÿæˆæ–‡ä»¶åï¼ˆä½¿ç”¨æ—¥æœŸï¼‰
        if predict_date:
            date_str = predict_date
        else:
            date_str = datetime.now().strftime('%Y-%m-%d')

        # åˆ›å»ºdataç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not os.path.exists('data'):
            os.makedirs('data')

        # æ–‡ä»¶è·¯å¾„
        filepath = f'data/ml_predictions_20d_{date_str}.txt'

        # æ„å»ºå†…å®¹
        content = f"{'=' * 80}\n"
        content += f"æœºå™¨å­¦ä¹ 20å¤©é¢„æµ‹ç»“æœ\n"
        content += f"é¢„æµ‹æ—¥æœŸ: {date_str}\n"
        content += f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        content += f"{'=' * 80}\n\n"

        # æ·»åŠ é¢„æµ‹ç»“æœ
        content += "ã€é¢„æµ‹ç»“æœã€‘\n"
        content += "-" * 80 + "\n"
        content += f"{'è‚¡ç¥¨ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹æ–¹å‘':<10} {'ä¸Šæ¶¨æ¦‚ç‡':<12} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡æ—¥æœŸ':<15}\n"
        content += "-" * 80 + "\n"

        # æŒ‰ä¸€è‡´æ€§æ’åºï¼ˆå¦‚æœæœ‰consistentåˆ—ï¼‰
        if 'consistent' in predictions_df.columns:
            predictions_df_sorted = predictions_df.sort_values(by=['consistent', 'avg_probability'], ascending=[False, False])
        else:
            predictions_df_sorted = predictions_df.sort_values(by='probability', ascending=False)

        for _, row in predictions_df_sorted.iterrows():
            code = row.get('code', 'N/A')
            name = row.get('name', 'N/A')
            current_price = row.get('current_price', None)
            data_date = row.get('data_date', 'N/A')
            target_date = row.get('target_date', 'N/A')
            
            # å°è¯•è·å–é¢„æµ‹å’Œæ¦‚ç‡ï¼ˆæ”¯æŒå¤šç§åˆ—åæ ¼å¼ï¼‰
            prediction = None
            probability = None
            
            # ä¼˜å…ˆä½¿ç”¨å¹³å‡æ¦‚ç‡å’Œä¸€è‡´æ€§åˆ¤æ–­
            if 'avg_probability' in row and 'consistent' in row:
                if row['consistent']:
                    # ä¸¤ä¸ªæ¨¡å‹ä¸€è‡´ï¼Œä½¿ç”¨å¹³å‡æ¦‚ç‡
                    probability = row['avg_probability']
                    prediction = 1 if probability >= 0.5 else 0
            elif 'prediction' in row:
                prediction = row.get('prediction', None)
                probability = row.get('probability', None)
            elif 'prediction_LGBM' in row:
                # ä½¿ç”¨LGBMçš„é¢„æµ‹
                prediction = row.get('prediction_LGBM', None)
                probability = row.get('probability_LGBM', None)

            if prediction is not None:
                pred_label = "ä¸Šæ¶¨" if prediction == 1 else "ä¸‹è·Œ"
                prob_str = f"{probability:.4f}" if probability is not None else "N/A"
                price_str = f"{current_price:.2f}" if current_price is not None else "N/A"
            else:
                pred_label = "N/A"
                prob_str = "N/A"
                price_str = "N/A"

            content += f"{code:<10} {name:<12} {pred_label:<10} {prob_str:<12} {price_str:<12} {data_date:<15} {target_date:<15}\n"

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        content += "\n" + "-" * 80 + "\n"
        content += "ã€ç»Ÿè®¡ä¿¡æ¯ã€‘\n"
        content += "-" * 80 + "\n"

        # åˆå§‹åŒ–å˜é‡
        total_count = 0
        up_count = 0
        down_count = 0
        consistent_count = 0
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        total_count = len(predictions_df)
        
        # è®¡ç®—ä¸Šæ¶¨å’Œä¸‹è·Œæ•°é‡
        if 'avg_probability' in predictions_df.columns:
            up_count = (predictions_df['avg_probability'] >= 0.5).sum()
            down_count = total_count - up_count
        elif 'prediction' in predictions_df.columns:
            up_count = (predictions_df['prediction'] == 1).sum()
            down_count = (predictions_df['prediction'] == 0).sum()
        elif 'prediction_LGBM' in predictions_df.columns:
            up_count = (predictions_df['prediction_LGBM'] == 1).sum()
            down_count = total_count - up_count
        
        if total_count > 0:
            content += f"é¢„æµ‹ä¸Šæ¶¨: {up_count} åª\n"
            content += f"é¢„æµ‹ä¸‹è·Œ: {down_count} åª\n"
            content += f"æ€»è®¡: {total_count} åª\n"
            content += f"ä¸Šæ¶¨æ¯”ä¾‹: {up_count/total_count*100:.1f}%\n"

        if 'consistent' in predictions_df.columns:
            consistent_count = predictions_df['consistent'].sum()
            content += f"\nä¸¤ä¸ªæ¨¡å‹ä¸€è‡´æ€§: {consistent_count}/{total_count} ({consistent_count/total_count*100:.1f}%)\n"

        if 'avg_probability' in predictions_df.columns:
            avg_prob = predictions_df['avg_probability'].mean()
            content += f"å¹³å‡ä¸Šæ¶¨æ¦‚ç‡: {avg_prob:.4f}\n"

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"âœ… 20å¤©é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {filepath}")
        return filepath

    except Exception as e:
        print(f"âŒ ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


def get_target_date(date, horizon):
    """è®¡ç®—ç›®æ ‡æ—¥æœŸï¼ˆæ•°æ®æ—¥æœŸ + é¢„æµ‹å‘¨æœŸï¼‰"""
    if isinstance(date, str):
        date = datetime.strptime(date, '%Y-%m-%d')
    target_date = date + timedelta(days=horizon)
    return target_date.strftime('%Y-%m-%d')


# ========== ç¼“å­˜è¾…åŠ©å‡½æ•° ==========
def _get_cache_key(stock_code, period_days):
    """ç”Ÿæˆç¼“å­˜é”®"""
    return f"{stock_code}_{period_days}d"

def _get_cache_file_path(cache_key):
    """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
    if not os.path.exists(CACHE_DIR):
        os.makedirs(CACHE_DIR)
    return os.path.join(CACHE_DIR, f"{cache_key}.pkl")

def _is_cache_valid(cache_file_path, cache_hours):
    """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
    if not os.path.exists(cache_file_path):
        return False
    cache_time = os.path.getmtime(cache_file_path)
    current_time = datetime.now().timestamp()
    age_hours = (current_time - cache_time) / 3600
    return age_hours < cache_hours

def _save_cache(cache_file_path, data):
    """ä¿å­˜ç¼“å­˜"""
    try:
        with open(cache_file_path, 'wb') as f:
            pickle.dump({
                'data': data,
                'timestamp': datetime.now().isoformat()
            }, f)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

def _load_cache(cache_file_path):
    """åŠ è½½ç¼“å­˜"""
    try:
        with open(cache_file_path, 'rb') as f:
            cache = pickle.load(f)
            return cache['data']
    except Exception as e:
        print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
        return None

def get_stock_data_with_cache(stock_code, period_days=730):
    """è·å–è‚¡ç¥¨æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = _get_cache_key(stock_code, period_days)
    cache_file_path = _get_cache_file_path(cache_key)
    
    # æ£€æŸ¥ç¼“å­˜
    if _is_cache_valid(cache_file_path, STOCK_DATA_CACHE_DAYS * 24):
        print(f"  ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„è‚¡ç¥¨æ•°æ® {stock_code}")
        cached_data = _load_cache(cache_file_path)
        if cached_data is not None:
            return cached_data
    
    # ä»ç½‘ç»œè·å–
    print(f"  ğŸŒ ä¸‹è½½è‚¡ç¥¨æ•°æ® {stock_code}")
    stock_df = get_hk_stock_data_tencent(stock_code, period_days)
    
    # ä¿å­˜ç¼“å­˜
    if stock_df is not None and not stock_df.empty:
        _save_cache(cache_file_path, stock_df)
    
    return stock_df

def get_hsi_data_with_cache(period_days=730):
    """è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
    cache_key = _get_cache_key("HSI", period_days)
    cache_file_path = _get_cache_file_path(cache_key)
    
    # æ£€æŸ¥ç¼“å­˜
    if _is_cache_valid(cache_file_path, HSI_DATA_CACHE_HOURS):
        print(f"  ğŸ“¦ ä½¿ç”¨ç¼“å­˜çš„æ’ç”ŸæŒ‡æ•°æ•°æ®")
        cached_data = _load_cache(cache_file_path)
        if cached_data is not None:
            return cached_data
    
    # ä»ç½‘ç»œè·å–
    print(f"  ğŸŒ ä¸‹è½½æ’ç”ŸæŒ‡æ•°æ•°æ®")
    hsi_df = get_hsi_data_tencent(period_days)
    
    # ä¿å­˜ç¼“å­˜
    if hsi_df is not None and not hsi_df.empty:
        _save_cache(cache_file_path, hsi_df)
    
    return hsi_df


class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹ç±»"""

    def __init__(self):
        self.tech_analyzer = TechnicalAnalyzer()
        # æ¿å—åˆ†æç¼“å­˜ï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰
        self._sector_analyzer = None
        self._sector_performance_cache = {}

    def _get_sector_analyzer(self):
        """è·å–æ¿å—åˆ†æå™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
        if self._sector_analyzer is None:
            try:
                from data_services.hk_sector_analysis import SectorAnalyzer
                self._sector_analyzer = SectorAnalyzer()
                print("  ğŸ“Š æ¿å—åˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
            except ImportError:
                print("  âš ï¸ æ¿å—åˆ†ææ¨¡å—ä¸å¯ç”¨")
                return None
        return self._sector_analyzer

    def _get_sector_performance(self, period):
        """è·å–æ¿å—è¡¨ç°æ•°æ®ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        cache_key = f'period_{period}'
        
        if cache_key not in self._sector_performance_cache:
            analyzer = self._get_sector_analyzer()
            if analyzer is None:
                return None
            
            try:
                perf_df = analyzer.calculate_sector_performance(period)
                self._sector_performance_cache[cache_key] = perf_df
            except Exception as e:
                print(f"  âš ï¸ è·å–æ¿å—è¡¨ç°å¤±è´¥ (period={period}): {e}")
                return None
        
        return self._sector_performance_cache[cache_key]

    def calculate_technical_features(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾ï¼ˆæ‰©å±•ç‰ˆï¼š80ä¸ªæŒ‡æ ‡ï¼‰"""
        if df.empty or len(df) < 200:
            return df

        # ========== åŸºç¡€ç§»åŠ¨å¹³å‡çº¿ ==========
        df = self.tech_analyzer.calculate_moving_averages(df, periods=[5, 10, 20, 50, 100, 200])

        # ========== RSI (Wilder å¹³æ»‘) ==========
        df = self.tech_analyzer.calculate_rsi(df, period=14)
        # RSI å˜åŒ–ç‡
        df['RSI_ROC'] = df['RSI'].pct_change()

        # ========== MACD ==========
        df = self.tech_analyzer.calculate_macd(df)
        # MACD æŸ±çŠ¶å›¾
        df['MACD_Hist'] = df['MACD'] - df['MACD_signal']
        # MACD æŸ±çŠ¶å›¾å˜åŒ–ç‡
        df['MACD_Hist_ROC'] = df['MACD_Hist'].pct_change()

        # ========== å¸ƒæ—å¸¦ ==========
        df = self.tech_analyzer.calculate_bollinger_bands(df, period=20, std_dev=2)
        # å¸ƒæ—å¸¦å®½åº¦
        df['BB_Width'] = (df['BB_upper'] - df['BB_lower']) / df['BB_middle']
        # å¸ƒæ—å¸¦çªç ´
        df['BB_Breakout'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

        # ========== ATR ==========
        df = self.tech_analyzer.calculate_atr(df, period=14)
        # ATR æ¯”ç‡ï¼ˆATRç›¸å¯¹äº10æ—¥å‡çº¿çš„æ¯”ç‡ï¼‰
        df['ATR_MA'] = df['ATR'].rolling(window=10, min_periods=1).mean()
        df['ATR_Ratio'] = df['ATR'] / df['ATR_MA']

        # ========== æˆäº¤é‡ç›¸å…³ ==========
        df['Vol_MA20'] = df['Volume'].rolling(window=20, min_periods=1).mean()
        df['Vol_Ratio'] = df['Volume'] / df['Vol_MA20']
        # æˆäº¤é‡ z-score
        df['Vol_Mean_20'] = df['Volume'].rolling(20, min_periods=1).mean()
        df['Vol_Std_20'] = df['Volume'].rolling(20, min_periods=1).std()
        df['Vol_Z_Score'] = (df['Volume'] - df['Vol_Mean_20']) / df['Vol_Std_20']
        # æˆäº¤é¢
        df['Turnover'] = df['Close'] * df['Volume']
        # æˆäº¤é¢ z-score
        df['Turnover_Mean_20'] = df['Turnover'].rolling(20, min_periods=1).mean()
        df['Turnover_Std_20'] = df['Turnover'].rolling(20, min_periods=1).std()
        df['Turnover_Z_Score'] = (df['Turnover'] - df['Turnover_Mean_20']) / df['Turnover_Std_20']
        # æˆäº¤é¢å˜åŒ–ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Turnover_Change_1d'] = df['Turnover'].pct_change()
        df['Turnover_Change_5d'] = df['Turnover'].pct_change(5)
        df['Turnover_Change_10d'] = df['Turnover'].pct_change(10)
        df['Turnover_Change_20d'] = df['Turnover'].pct_change(20)
        # æ¢æ‰‹ç‡ï¼ˆå‡è®¾æ€»è‚¡æœ¬ä¸ºå¸¸æ•°ï¼Œè¿™é‡Œä½¿ç”¨æˆäº¤é¢/ä»·æ ¼ä½œä¸ºè¿‘ä¼¼ï¼‰
        df['Turnover_Rate'] = (df['Turnover'] / (df['Close'] * 1000000)) * 100
        # æ¢æ‰‹ç‡å˜åŒ–ç‡
        df['Turnover_Rate_Change_5d'] = df['Turnover_Rate'].pct_change(5)
        df['Turnover_Rate_Change_20d'] = df['Turnover_Rate'].pct_change(20)

        # ========== VWAP (æˆäº¤é‡åŠ æƒå¹³å‡ä»·) ==========
        df['TP'] = (df['High'] + df['Low'] + df['Close']) / 3
        df['VWAP'] = (df['TP'] * df['Volume']).rolling(window=20, min_periods=1).sum() / df['Volume'].rolling(window=20, min_periods=1).sum()

        # ========== OBV (èƒ½é‡æ½®) ==========
        df['OBV'] = 0.0
        for i in range(1, len(df)):
            if df['Close'].iloc[i] > df['Close'].iloc[i-1]:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1] + df['Volume'].iloc[i]
            elif df['Close'].iloc[i] < df['Close'].iloc[i-1]:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1] - df['Volume'].iloc[i]
            else:
                df['OBV'].iloc[i] = df['OBV'].iloc[i-1]

        # ========== CMF (Chaikin Money Flow) ==========
        df['MF_Multiplier'] = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])
        df['MF_Volume'] = df['MF_Multiplier'] * df['Volume']
        df['CMF'] = df['MF_Volume'].rolling(20, min_periods=1).sum() / df['Volume'].rolling(20, min_periods=1).sum()
        # CMF ä¿¡å·çº¿
        df['CMF_Signal'] = df['CMF'].rolling(5, min_periods=1).mean()

        # ========== ADX (å¹³å‡è¶‹å‘æŒ‡æ•°) ==========
        # +DM and -DM
        up_move = df['High'].diff()
        down_move = -df['Low'].diff()
        df['+DM'] = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
        df['-DM'] = np.where((down_move > up_move) & (down_move > 0), down_move, 0)
        # +DI and -DI
        df['+DI'] = 100 * (df['+DM'].ewm(alpha=1/14, adjust=False).mean() / df['ATR'])
        df['-DI'] = 100 * (df['-DM'].ewm(alpha=1/14, adjust=False).mean() / df['ATR'])
        # ADX
        dx = 100 * (np.abs(df['+DI'] - df['-DI']) / (df['+DI'] + df['-DI']))
        df['ADX'] = dx.ewm(alpha=1/14, adjust=False).mean()

        # ========== éšæœºæŒ¯è¡å™¨ (Stochastic Oscillator) ==========
        K_Period = 14
        D_Period = 3
        df['Low_Min'] = df['Low'].rolling(window=K_Period, min_periods=1).min()
        df['High_Max'] = df['High'].rolling(window=K_Period, min_periods=1).max()
        df['Stoch_K'] = 100 * (df['Close'] - df['Low_Min']) / (df['High_Max'] - df['Low_Min'])
        df['Stoch_D'] = df['Stoch_K'].rolling(window=D_Period, min_periods=1).mean()

        # ========== Williams %R ==========
        df['Williams_R'] = (df['High_Max'] - df['Close']) / (df['High_Max'] - df['Low_Min']) * -100

        # ========== ROC (ä»·æ ¼å˜åŒ–ç‡) ==========
        df['ROC'] = df['Close'].pct_change(periods=12)

        # ========== æ³¢åŠ¨ç‡ï¼ˆå¹´åŒ–ï¼‰ ==========
        df['Returns'] = df['Close'].pct_change()
        df['Volatility'] = df['Returns'].rolling(20, min_periods=10).std() * np.sqrt(252)

        # ========== ä»·æ ¼ä½ç½®ç‰¹å¾ ==========
        # ä»·æ ¼ç›¸å¯¹äºå‡çº¿çš„åç¦»
        df['MA5_Deviation'] = (df['Close'] - df['MA5']) / df['MA5'] * 100
        df['MA10_Deviation'] = (df['Close'] - df['MA10']) / df['MA10'] * 100
        # ä»·æ ¼ç™¾åˆ†ä½ï¼ˆç›¸å¯¹äº60æ—¥çª—å£ï¼‰
        df['Price_Percentile'] = df['Close'].rolling(window=60, min_periods=1).apply(
            lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()) * 100
        )
        # å¸ƒæ—å¸¦ä½ç½®
        df['BB_Position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

        # ========== å¤šå‘¨æœŸæ”¶ç›Šç‡ ==========
        df['Return_1d'] = df['Close'].pct_change()
        df['Return_3d'] = df['Close'].pct_change(3)
        df['Return_5d'] = df['Close'].pct_change(5)
        df['Return_10d'] = df['Close'].pct_change(10)
        df['Return_20d'] = df['Close'].pct_change(20)
        df['Return_60d'] = df['Close'].pct_change(60)

        # ========== ä»·æ ¼ç›¸å¯¹äºå‡çº¿çš„æ¯”ç‡ ==========
        df['Price_Ratio_MA5'] = df['Close'] / df['MA5']
        df['Price_Ratio_MA20'] = df['Close'] / df['MA20']
        df['Price_Ratio_MA50'] = df['Close'] / df['MA50']

        # ========== é«˜ä¼˜å…ˆçº§ï¼šæ»šåŠ¨ç»Ÿè®¡ç‰¹å¾ ==========
        # å‡çº¿åç¦»åº¦ï¼ˆæ ‡å‡†åŒ–ï¼‰
        df['MA5_Deviation_Std'] = (df['Close'] - df['MA5']) / df['Close'].rolling(5).std()
        df['MA20_Deviation_Std'] = (df['Close'] - df['MA20']) / df['Close'].rolling(20).std()

        # æ»šåŠ¨æ³¢åŠ¨ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Volatility_5d'] = df['Close'].pct_change().rolling(5).std()
        df['Volatility_10d'] = df['Close'].pct_change().rolling(10).std()
        df['Volatility_20d'] = df['Close'].pct_change().rolling(20).std()

        # æ»šåŠ¨ååº¦/å³°åº¦ï¼ˆä¸šç•Œå¸¸ç”¨ï¼‰
        df['Skewness_20d'] = df['Close'].pct_change().rolling(20).skew()
        df['Kurtosis_20d'] = df['Close'].pct_change().rolling(20).kurt()

        # åŠ¨é‡åŠ é€Ÿåº¦ï¼ˆä¸šç•Œé‡è¦ç‰¹å¾ï¼‰
        df['Momentum_Accel_5d'] = df['Return_5d'] - df['Return_5d'].shift(5)
        df['Momentum_Accel_10d'] = df['Return_10d'] - df['Return_10d'].shift(5)

        # ========== é«˜ä¼˜å…ˆçº§ï¼šä»·æ ¼å½¢æ€ç‰¹å¾ ==========
        # Næ—¥é«˜ä½ç‚¹ä½ç½®ï¼ˆ0-1ä¹‹é—´ï¼Œ1è¡¨ç¤ºåœ¨æœ€é«˜ç‚¹ï¼‰
        df['High_Position_20d'] = (df['Close'] - df['Low'].rolling(20).min()) / (df['High'].rolling(20).max() - df['Low'].rolling(20).min())
        df['High_Position_60d'] = (df['Close'] - df['Low'].rolling(60).min()) / (df['High'].rolling(60).max() - df['Low'].rolling(60).min())

        # è·ç¦»è¿‘æœŸé«˜ç‚¹/ä½ç‚¹çš„å¤©æ•°ï¼ˆä¸šç•Œå¸¸ç”¨ï¼‰
        df['Days_Since_High_20d'] = df['Close'].rolling(20).apply(lambda x: 20 - np.argmax(x), raw=False)
        df['Days_Since_Low_20d'] = df['Close'].rolling(20).apply(lambda x: 20 - np.argmin(x), raw=False)

        # æ—¥å†…ç‰¹å¾ï¼ˆä¸šç•Œæ ¸å¿ƒä¿¡å·ï¼‰
        df['Intraday_Range'] = (df['High'] - df['Low']) / df['Close']
        df['Intraday_Range_MA5'] = df['Intraday_Range'].rolling(5).mean()
        df['Intraday_Range_MA20'] = df['Intraday_Range'].rolling(20).mean()

        # æ”¶ç›˜ä½ç½®ï¼ˆé˜³çº¿/é˜´çº¿å¼ºåº¦ï¼Œ0-1ä¹‹é—´ï¼‰
        df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'])
        # ä¸Šå½±çº¿/ä¸‹å½±çº¿æ¯”ä¾‹
        df['Upper_Shadow'] = (df['High'] - df[['Close', 'Open']].max(axis=1)) / (df['High'] - df['Low'] + 1e-10)
        df['Lower_Shadow'] = (df[['Close', 'Open']].min(axis=1) - df['Low']) / (df['High'] - df['Low'] + 1e-10)

        # å¼€ç›˜ç¼ºå£
        df['Gap_Size'] = (df['Open'] - df['Close'].shift(1)) / df['Close'].shift(1)
        df['Gap_Up'] = (df['Gap_Size'] > 0.01).astype(int)  # è·³ç©ºé«˜å¼€ >1%
        df['Gap_Down'] = (df['Gap_Size'] < -0.01).astype(int)  # è·³ç©ºä½å¼€ >1%

        # ========== ä¸­ä¼˜å…ˆçº§ï¼šé‡ä»·å…³ç³»ç‰¹å¾ ==========
        # é‡ä»·èƒŒç¦»ï¼ˆä¸šç•Œé‡è¦ä¿¡å·ï¼‰
        df['Price_Up_Volume_Down'] = ((df['Return_1d'] > 0) & (df['Turnover'].pct_change() < 0)).astype(int)
        df['Price_Down_Volume_Up'] = ((df['Return_1d'] < 0) & (df['Turnover'].pct_change() > 0)).astype(int)

        # OBV è¶‹åŠ¿
        df['OBV_MA5'] = df['OBV'].rolling(5).mean()
        df['OBV_Trend'] = (df['OBV'] > df['OBV_MA5']).astype(int)

        # æˆäº¤é‡æ³¢åŠ¨ç‡
        df['Volume_Volatility'] = df['Turnover'].rolling(20).std() / (df['Turnover'].rolling(20).mean() + 1e-10)

        # æˆäº¤é‡æ¯”ç‡ï¼ˆå¤šå‘¨æœŸï¼‰
        df['Volume_Ratio_5d'] = df['Volume'] / df['Volume'].rolling(5).mean()
        df['Volume_Ratio_20d'] = df['Volume'] / df['Volume'].rolling(20).mean()

        # ========== é•¿æœŸè¶‹åŠ¿ç‰¹å¾ï¼ˆä¸“é—¨ä¼˜åŒ–ä¸€ä¸ªæœˆæ¨¡å‹ï¼‰ ==========
        # é•¿æœŸå‡çº¿ï¼ˆ120æ—¥åŠå¹´çº¿ã€250æ—¥å¹´çº¿ï¼‰
        df['MA120'] = df['Close'].rolling(window=120, min_periods=1).mean()
        df['MA250'] = df['Close'].rolling(window=250, min_periods=1).mean()

        # ä»·æ ¼ç›¸å¯¹é•¿æœŸå‡çº¿çš„æ¯”ç‡ï¼ˆä¸šç•Œé•¿æœŸè¶‹åŠ¿æŒ‡æ ‡ï¼‰
        df['Price_Ratio_MA120'] = df['Close'] / df['MA120']
        df['Price_Ratio_MA250'] = df['Close'] / df['MA250']

        # é•¿æœŸæ”¶ç›Šç‡ï¼ˆä¸šç•Œæ ¸å¿ƒé•¿æœŸç‰¹å¾ï¼‰
        df['Return_120d'] = df['Close'].pct_change(120)
        df['Return_250d'] = df['Close'].pct_change(250)

        # é•¿æœŸåŠ¨é‡ï¼ˆMomentum = å½“å‰ä»·æ ¼ / Næ—¥å‰ä»·æ ¼ - 1ï¼‰
        df['Momentum_120d'] = df['Close'] / df['Close'].shift(120) - 1
        df['Momentum_250d'] = df['Close'] / df['Close'].shift(250) - 1

        # é•¿æœŸåŠ¨é‡åŠ é€Ÿåº¦ï¼ˆè¶‹åŠ¿å˜åŒ–çš„äºŒé˜¶å¯¼æ•°ï¼‰
        df['Momentum_Accel_120d'] = df['Return_120d'] - df['Return_120d'].shift(30)

        # é•¿æœŸå‡çº¿æ–œç‡ï¼ˆè¶‹åŠ¿å¼ºåº¦æŒ‡æ ‡ï¼‰
        df['MA120_Slope'] = (df['MA120'] - df['MA120'].shift(10)) / df['MA120'].shift(10)
        df['MA250_Slope'] = (df['MA250'] - df['MA250'].shift(20)) / df['MA250'].shift(20)

        # é•¿æœŸå‡çº¿æ’åˆ—ï¼ˆå¤šå¤´/ç©ºå¤´/æ··ä¹±ï¼‰
        df['MA_Alignment_Long'] = np.where(
            (df['MA50'] > df['MA120']) & (df['MA120'] > df['MA250']), 1,  # å¤šå¤´æ’åˆ—
            np.where(
                (df['MA50'] < df['MA120']) & (df['MA120'] < df['MA250']), -1,  # ç©ºå¤´æ’åˆ—
                0  # æ··ä¹±æ’åˆ—
            )
        )

        # é•¿æœŸå‡çº¿ä¹–ç¦»ç‡ï¼ˆä»·æ ¼åç¦»é•¿æœŸå‡çº¿çš„ç¨‹åº¦ï¼‰
        df['MA120_Deviation'] = (df['Close'] - df['MA120']) / df['MA120'] * 100
        df['MA250_Deviation'] = (df['Close'] - df['MA250']) / df['MA250'] * 100

        # é•¿æœŸæ³¢åŠ¨ç‡ï¼ˆé£é™©æŒ‡æ ‡ï¼‰
        df['Volatility_60d'] = df['Close'].pct_change().rolling(60).std()
        df['Volatility_120d'] = df['Close'].pct_change().rolling(120).std()

        # é•¿æœŸATRï¼ˆé•¿æœŸé£é™©ï¼‰
        df['ATR_MA60'] = df['ATR'].rolling(60, min_periods=1).mean()
        df['ATR_MA120'] = df['ATR'].rolling(120, min_periods=1).mean()
        df['ATR_Ratio_60d'] = df['ATR'] / df['ATR_MA60']
        df['ATR_Ratio_120d'] = df['ATR'] / df['ATR_MA120']

        # é•¿æœŸæˆäº¤é‡è¶‹åŠ¿
        df['Volume_MA120'] = df['Volume'].rolling(120, min_periods=1).mean()
        df['Volume_MA250'] = df['Volume'].rolling(250, min_periods=1).mean()
        df['Volume_Ratio_120d'] = df['Volume'] / df['Volume_MA120']
        df['Volume_Trend_Long'] = np.where(
            df['Volume_MA120'] > df['Volume_MA250'], 1, -1
        )

        # é•¿æœŸæ”¯æ’‘é˜»åŠ›ä½ï¼ˆåŸºäº120æ—¥é«˜ä½ç‚¹ï¼‰
        df['Support_120d'] = df['Low'].rolling(120, min_periods=1).min()
        df['Resistance_120d'] = df['High'].rolling(120, min_periods=1).max()
        df['Distance_Support_120d'] = (df['Close'] - df['Support_120d']) / df['Close']
        df['Distance_Resistance_120d'] = (df['Resistance_120d'] - df['Close']) / df['Close']

        # é•¿æœŸRSIï¼ˆåŸºäº120æ—¥ï¼‰
        df['RSI_120'] = self.tech_analyzer.calculate_rsi(df.copy(), period=120)['RSI']

        return df

    def create_fundamental_features(self, code):
        """åˆ›å»ºåŸºæœ¬é¢ç‰¹å¾ï¼ˆåªä½¿ç”¨å®é™…å¯ç”¨çš„æ•°æ®ï¼‰"""
        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            fundamental_data = get_comprehensive_fundamental_data(stock_code)
            if fundamental_data:
                # åªä½¿ç”¨å®é™…å¯ç”¨çš„åŸºæœ¬é¢æ•°æ®
                return {
                    'PE': fundamental_data.get('fi_pe_ratio', np.nan),
                    'PB': fundamental_data.get('fi_pb_ratio', np.nan),
                    'Market_Cap': fundamental_data.get('fi_market_cap', np.nan),
                    'ROE': np.nan,  # æš‚ä¸å¯ç”¨
                    'ROA': np.nan,  # æš‚ä¸å¯ç”¨
                    'Dividend_Yield': np.nan,  # æš‚ä¸å¯ç”¨
                    'EPS': np.nan,  # æš‚ä¸å¯ç”¨
                    'Net_Margin': np.nan,  # æš‚ä¸å¯ç”¨
                    'Gross_Margin': np.nan  # æš‚ä¸å¯ç”¨
                }
        except Exception as e:
            print(f"è·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥ {code}: {e}")
        return {}

    def create_smart_money_features(self, df):
        """åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾"""
        if df.empty or len(df) < 50:
            return df

        # ä»·æ ¼ç›¸å¯¹ä½ç½®
        df['Price_Pct_20d'] = df['Close'].rolling(window=20).apply(lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()))

        # æ”¾é‡ä¸Šæ¶¨ä¿¡å·
        df['Strong_Volume_Up'] = (df['Close'] > df['Open']) & (df['Vol_Ratio'] > 1.5)

        # ç¼©é‡å›è°ƒä¿¡å·
        df['Prev_Close'] = df['Close'].shift(1)
        df['Weak_Volume_Down'] = (df['Close'] < df['Prev_Close']) & (df['Vol_Ratio'] < 1.0) & ((df['Prev_Close'] - df['Close']) / df['Prev_Close'] < 0.02)

        # åŠ¨é‡ä¿¡å·
        df['Momentum_5d'] = df['Close'] / df['Close'].shift(5) - 1
        df['Momentum_10d'] = df['Close'] / df['Close'].shift(10) - 1

        return df

    def create_stock_type_features(self, code, df):
        """åˆ›å»ºè‚¡ç¥¨ç±»å‹ç‰¹å¾ï¼ˆåŸºäºä¸šç•Œæƒ¯ä¾‹ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆç”¨äºè®¡ç®—æµåŠ¨æ€§ç­‰åŠ¨æ€ç‰¹å¾ï¼‰

        Returns:
            dict: è‚¡ç¥¨ç±»å‹ç‰¹å¾å­—å…¸
        """
        # è‚¡ç¥¨ç±»å‹åˆ†ç±»ï¼ˆåŸºäºä¸åŒè‚¡ç¥¨ç±»å‹åˆ†ææ¡†æ¶å¯¹æ¯”.mdï¼‰
        stock_type_mapping = {
            # é“¶è¡Œè‚¡
            '0005.HK': {'type': 'bank', 'name': 'æ±‡ä¸°é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 70, 'risk': 20},
            '0939.HK': {'type': 'bank', 'name': 'å»ºè®¾é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 80, 'risk': 20},
            '1288.HK': {'type': 'bank', 'name': 'å†œä¸šé“¶è¡Œ', 'defensive': 95, 'growth': 25, 'cyclical': 20, 'liquidity': 85, 'risk': 15},
            '1398.HK': {'type': 'bank', 'name': 'å·¥å•†é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 85, 'risk': 20},
            '3968.HK': {'type': 'bank', 'name': 'æ‹›å•†é“¶è¡Œ', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 25},

            # å…¬ç”¨äº‹ä¸šè‚¡
            '0728.HK': {'type': 'utility', 'name': 'ä¸­å›½ç”µä¿¡', 'defensive': 90, 'growth': 25, 'cyclical': 15, 'liquidity': 70, 'risk': 20},
            '0941.HK': {'type': 'utility', 'name': 'ä¸­å›½ç§»åŠ¨', 'defensive': 95, 'growth': 30, 'cyclical': 15, 'liquidity': 80, 'risk': 15},

            # ç§‘æŠ€è‚¡
            '0700.HK': {'type': 'tech', 'name': 'è…¾è®¯æ§è‚¡', 'defensive': 40, 'growth': 85, 'cyclical': 30, 'liquidity': 90, 'risk': 60},
            '9988.HK': {'type': 'tech', 'name': 'é˜¿é‡Œå·´å·´-SW', 'defensive': 35, 'growth': 85, 'cyclical': 35, 'liquidity': 85, 'risk': 65},
            '3690.HK': {'type': 'tech', 'name': 'ç¾å›¢-W', 'defensive': 30, 'growth': 80, 'cyclical': 40, 'liquidity': 85, 'risk': 70},
            '1810.HK': {'type': 'tech', 'name': 'å°ç±³é›†å›¢-W', 'defensive': 35, 'growth': 75, 'cyclical': 45, 'liquidity': 80, 'risk': 65},

            # åŠå¯¼ä½“è‚¡
            '0981.HK': {'type': 'semiconductor', 'name': 'ä¸­èŠ¯å›½é™…', 'defensive': 25, 'growth': 80, 'cyclical': 70, 'liquidity': 75, 'risk': 75},
            '1347.HK': {'type': 'semiconductor', 'name': 'åè™¹åŠå¯¼ä½“', 'defensive': 20, 'growth': 85, 'cyclical': 75, 'liquidity': 70, 'risk': 80},

            # äººå·¥æ™ºèƒ½è‚¡
            '6682.HK': {'type': 'ai', 'name': 'ç¬¬å››èŒƒå¼', 'defensive': 20, 'growth': 90, 'cyclical': 50, 'liquidity': 60, 'risk': 85},
            '9660.HK': {'type': 'ai', 'name': 'åœ°å¹³çº¿æœºå™¨äºº', 'defensive': 15, 'growth': 95, 'cyclical': 60, 'liquidity': 55, 'risk': 90},
            '2533.HK': {'type': 'ai', 'name': 'é»‘èŠéº»æ™ºèƒ½', 'defensive': 15, 'growth': 95, 'cyclical': 65, 'liquidity': 50, 'risk': 90},

            # æ–°èƒ½æºè‚¡
            '1211.HK': {'type': 'new_energy', 'name': 'æ¯”äºšè¿ªè‚¡ä»½', 'defensive': 30, 'growth': 85, 'cyclical': 60, 'liquidity': 80, 'risk': 70},
            '1330.HK': {'type': 'environmental', 'name': 'ç»¿è‰²åŠ¨åŠ›ç¯ä¿', 'defensive': 25, 'growth': 75, 'cyclical': 80, 'liquidity': 60, 'risk': 80},

            # èƒ½æº/å‘¨æœŸè‚¡
            '0883.HK': {'type': 'energy', 'name': 'ä¸­å›½æµ·æ´‹çŸ³æ²¹', 'defensive': 30, 'growth': 50, 'cyclical': 90, 'liquidity': 75, 'risk': 75},
            '1088.HK': {'type': 'energy', 'name': 'ä¸­å›½ç¥å', 'defensive': 40, 'growth': 45, 'cyclical': 85, 'liquidity': 70, 'risk': 70},
            '1138.HK': {'type': 'shipping', 'name': 'ä¸­è¿œæµ·èƒ½', 'defensive': 25, 'growth': 45, 'cyclical': 95, 'liquidity': 65, 'risk': 80},
            '0388.HK': {'type': 'exchange', 'name': 'é¦™æ¸¯äº¤æ˜“æ‰€', 'defensive': 25, 'growth': 50, 'cyclical': 90, 'liquidity': 70, 'risk': 75},

            # ä¿é™©è‚¡
            '1299.HK': {'type': 'insurance', 'name': 'å‹é‚¦ä¿é™©', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 30},

            # ç”Ÿç‰©åŒ»è¯è‚¡
            '2269.HK': {'type': 'biotech', 'name': 'è¯æ˜ç”Ÿç‰©', 'defensive': 30, 'growth': 80, 'cyclical': 55, 'liquidity': 70, 'risk': 70},

            # æˆ¿åœ°äº§è‚¡
            '0012.HK': {'type': 'real_estate', 'name': 'æ’åŸºåœ°äº§', 'defensive': 20, 'growth': 30, 'cyclical': 95, 'liquidity': 50, 'risk': 85},
            '0016.HK': {'type': 'real_estate', 'name': 'æ–°é¸¿åŸºåœ°äº§', 'defensive': 25, 'growth': 35, 'cyclical': 90, 'liquidity': 55, 'risk': 80},
            '1109.HK': {'type': 'real_estate', 'name': 'åæ¶¦ç½®åœ°', 'defensive': 30, 'growth': 40, 'cyclical': 85, 'liquidity': 60, 'risk': 75},

            # æŒ‡æ•°åŸºé‡‘
            '2800.HK': {'type': 'index', 'name': 'ç›ˆå¯ŒåŸºé‡‘', 'defensive': 80, 'growth': 40, 'cyclical': 30, 'liquidity': 90, 'risk': 25},
        }

        # è·å–è‚¡ç¥¨ç±»å‹ä¿¡æ¯
        stock_info_mapping = {
            # é“¶è¡Œè‚¡
            '0005.HK': {'type': 'bank', 'name': 'æ±‡ä¸°é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 70, 'risk': 20},
            '0388.HK': {'type': 'exchange', 'name': 'é¦™æ¸¯äº¤æ˜“æ‰€', 'defensive': 25, 'growth': 50, 'cyclical': 90, 'liquidity': 70, 'risk': 75},
            '0700.HK': {'type': 'tech', 'name': 'è…¾è®¯æ§è‚¡', 'defensive': 40, 'growth': 85, 'cyclical': 30, 'liquidity': 90, 'risk': 60},
            '0728.HK': {'type': 'utility', 'name': 'ä¸­å›½ç”µä¿¡', 'defensive': 90, 'growth': 25, 'cyclical': 15, 'liquidity': 70, 'risk': 20},
            '0883.HK': {'type': 'energy', 'name': 'ä¸­å›½æµ·æ´‹çŸ³æ²¹', 'defensive': 30, 'growth': 50, 'cyclical': 90, 'liquidity': 75, 'risk': 75},
            '0939.HK': {'type': 'bank', 'name': 'å»ºè®¾é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 80, 'risk': 20},
            '0941.HK': {'type': 'utility', 'name': 'ä¸­å›½ç§»åŠ¨', 'defensive': 95, 'growth': 30, 'cyclical': 15, 'liquidity': 80, 'risk': 15},
            '0981.HK': {'type': 'semiconductor', 'name': 'ä¸­èŠ¯å›½é™…', 'defensive': 25, 'growth': 80, 'cyclical': 70, 'liquidity': 75, 'risk': 75},
            '1088.HK': {'type': 'energy', 'name': 'ä¸­å›½ç¥å', 'defensive': 40, 'growth': 45, 'cyclical': 85, 'liquidity': 70, 'risk': 70},
            '1138.HK': {'type': 'shipping', 'name': 'ä¸­è¿œæµ·èƒ½', 'defensive': 25, 'growth': 45, 'cyclical': 95, 'liquidity': 65, 'risk': 80},
            '1211.HK': {'type': 'new_energy', 'name': 'æ¯”äºšè¿ªè‚¡ä»½', 'defensive': 30, 'growth': 85, 'cyclical': 60, 'liquidity': 80, 'risk': 70},
            '1288.HK': {'type': 'bank', 'name': 'å†œä¸šé“¶è¡Œ', 'defensive': 95, 'growth': 25, 'cyclical': 20, 'liquidity': 85, 'risk': 15},
            '1299.HK': {'type': 'insurance', 'name': 'å‹é‚¦ä¿é™©', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 30},
            '1330.HK': {'type': 'environmental', 'name': 'ç»¿è‰²åŠ¨åŠ›ç¯ä¿', 'defensive': 25, 'growth': 75, 'cyclical': 80, 'liquidity': 60, 'risk': 80},
            '1347.HK': {'type': 'semiconductor', 'name': 'åè™¹åŠå¯¼ä½“', 'defensive': 20, 'growth': 85, 'cyclical': 75, 'liquidity': 70, 'risk': 80},
            '1398.HK': {'type': 'bank', 'name': 'å·¥å•†é“¶è¡Œ', 'defensive': 90, 'growth': 30, 'cyclical': 20, 'liquidity': 85, 'risk': 20},
            '1810.HK': {'type': 'tech', 'name': 'å°ç±³é›†å›¢-W', 'defensive': 35, 'growth': 75, 'cyclical': 45, 'liquidity': 80, 'risk': 65},
            '2269.HK': {'type': 'biotech', 'name': 'è¯æ˜ç”Ÿç‰©', 'defensive': 30, 'growth': 80, 'cyclical': 55, 'liquidity': 70, 'risk': 70},
            '2533.HK': {'type': 'ai', 'name': 'é»‘èŠéº»æ™ºèƒ½', 'defensive': 15, 'growth': 95, 'cyclical': 65, 'liquidity': 50, 'risk': 90},
            '2800.HK': {'type': 'index', 'name': 'ç›ˆå¯ŒåŸºé‡‘', 'defensive': 80, 'growth': 40, 'cyclical': 30, 'liquidity': 90, 'risk': 25},
            '3690.HK': {'type': 'tech', 'name': 'ç¾å›¢-W', 'defensive': 30, 'growth': 80, 'cyclical': 40, 'liquidity': 85, 'risk': 70},
            '3968.HK': {'type': 'bank', 'name': 'æ‹›å•†é“¶è¡Œ', 'defensive': 85, 'growth': 40, 'cyclical': 25, 'liquidity': 75, 'risk': 25},
            '6682.HK': {'type': 'ai', 'name': 'ç¬¬å››èŒƒå¼', 'defensive': 20, 'growth': 90, 'cyclical': 50, 'liquidity': 60, 'risk': 85},
            '9660.HK': {'type': 'ai', 'name': 'åœ°å¹³çº¿æœºå™¨äºº', 'defensive': 15, 'growth': 95, 'cyclical': 60, 'liquidity': 55, 'risk': 90},
            '9988.HK': {'type': 'tech', 'name': 'é˜¿é‡Œå·´å·´-SW', 'defensive': 35, 'growth': 85, 'cyclical': 35, 'liquidity': 85, 'risk': 65},
            # æˆ¿åœ°äº§è‚¡
            '0012.HK': {'type': 'real_estate', 'name': 'æ’åŸºåœ°äº§', 'defensive': 20, 'growth': 30, 'cyclical': 95, 'liquidity': 50, 'risk': 85},
            '0016.HK': {'type': 'real_estate', 'name': 'æ–°é¸¿åŸºåœ°äº§', 'defensive': 25, 'growth': 35, 'cyclical': 90, 'liquidity': 55, 'risk': 80},
            '1109.HK': {'type': 'real_estate', 'name': 'åæ¶¦ç½®åœ°', 'defensive': 30, 'growth': 40, 'cyclical': 85, 'liquidity': 60, 'risk': 75},
        }

        # è·å–è‚¡ç¥¨ç±»å‹ä¿¡æ¯
        stock_info = stock_info_mapping.get(code, None)
        if not stock_info:
            print(f"âš ï¸ æœªæ‰¾åˆ°è‚¡ç¥¨ {code} çš„ç±»å‹ä¿¡æ¯")
            return {}

        features = {
            # è‚¡ç¥¨ç±»å‹ç‰¹å¾ï¼ˆå­—ç¬¦ä¸²ç±»å‹ï¼‰
            'Stock_Type': stock_info['type'],

            # ç»¼åˆè¯„åˆ†ç‰¹å¾ï¼ˆåŸºäºä¸šç•Œæƒ¯ä¾‹ï¼‰
            'Stock_Defensive_Score': stock_info['defensive'] / 100.0,  # é˜²å¾¡æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Growth_Score': stock_info['growth'] / 100.0,          # æˆé•¿æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Cyclical_Score': stock_info['cyclical'] / 100.0,        # å‘¨æœŸæ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Liquidity_Score': stock_info['liquidity'] / 100.0,      # æµåŠ¨æ€§è¯„åˆ†ï¼ˆ0-1ï¼‰
            'Stock_Risk_Score': stock_info['risk'] / 100.0,                # é£é™©è¯„åˆ†ï¼ˆ0-1ï¼‰

            # è¡ç”Ÿç‰¹å¾ï¼ˆåŸºäºä¸šç•Œåˆ†ææƒé‡ï¼‰
            # é“¶è¡Œè‚¡ï¼šåŸºæœ¬é¢æƒé‡70%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡30%
            'Bank_Style_Fundamental_Weight': 0.7 if stock_info['type'] == 'bank' else 0.0,
            'Bank_Style_Technical_Weight': 0.3 if stock_info['type'] == 'bank' else 0.0,

            # ç§‘æŠ€è‚¡ï¼šåŸºæœ¬é¢æƒé‡40%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡60%
            'Tech_Style_Fundamental_Weight': 0.4 if stock_info['type'] == 'tech' else 0.0,
            'Tech_Style_Technical_Weight': 0.6 if stock_info['type'] == 'tech' else 0.0,

            # å‘¨æœŸè‚¡ï¼šåŸºæœ¬é¢æƒé‡10%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡70%ï¼Œèµ„é‡‘æµå‘æƒé‡20%
            'Cyclical_Style_Fundamental_Weight': 0.1 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,
            'Cyclical_Style_Technical_Weight': 0.7 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,
            'Cyclical_Style_Flow_Weight': 0.2 if stock_info['type'] in ['energy', 'shipping', 'exchange'] else 0.0,

            # æˆ¿åœ°äº§è‚¡ï¼šåŸºæœ¬é¢æƒé‡20%ï¼ŒæŠ€æœ¯åˆ†ææƒé‡60%ï¼Œèµ„é‡‘æµå‘æƒé‡20%
            'RealEstate_Style_Fundamental_Weight': 0.2 if stock_info['type'] == 'real_estate' else 0.0,
            'RealEstate_Style_Technical_Weight': 0.6 if stock_info['type'] == 'real_estate' else 0.0,
            'RealEstate_Style_Flow_Weight': 0.2 if stock_info['type'] == 'real_estate' else 0.0,
        }

        # åŠ¨æ€ç‰¹å¾ï¼ˆåŸºäºå†å²æ•°æ®è®¡ç®—ï¼‰
        if df is not None and not df.empty and len(df) >= 60:
            # å†å²æ³¢åŠ¨ç‡ï¼ˆåŸºäº60æ—¥æ•°æ®ï¼‰
            returns = df['Close'].pct_change().dropna()
            if len(returns) >= 30:
                historical_volatility = returns.rolling(window=30, min_periods=10).std().iloc[-1]
                features['Stock_Historical_Volatility'] = historical_volatility

                # å®é™…æµåŠ¨æ€§è¯„åˆ†ï¼ˆåŸºäºæˆäº¤é¢æ³¢åŠ¨ï¼‰
                turnover_volatility = df['Turnover'].rolling(window=20, min_periods=10).std().iloc[-1] / df['Turnover'].rolling(window=20, min_periods=10).mean().iloc[-1]
                features['Stock_Actual_Liquidity_Score'] = max(0, min(1, 1 - turnover_volatility))

                # ä»·æ ¼ç¨³å®šæ€§è¯„åˆ†ï¼ˆåŸºäºä»·æ ¼æ³¢åŠ¨ï¼‰
                price_volatility = df['Close'].rolling(window=20, min_periods=10).std().iloc[-1] / df['Close'].rolling(window=20, min_periods=10).mean().iloc[-1]
                features['Stock_Price_Stability_Score'] = max(0, min(1, 1 - price_volatility))

        return features

    def calculate_multi_period_metrics(self, df):
        """è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡ï¼ˆè¶‹åŠ¿å’Œç›¸å¯¹å¼ºåº¦ï¼‰"""
        if df.empty or len(df) < 60:
            return df

        periods = [3, 5, 10, 20, 60]

        for period in periods:
            if len(df) < period:
                continue

            # è®¡ç®—æ”¶ç›Šç‡
            return_col = f'Return_{period}d'
            if return_col in df.columns:
                # è®¡ç®—è¶‹åŠ¿æ–¹å‘ï¼ˆ1=ä¸Šæ¶¨ï¼Œ0=ä¸‹è·Œï¼‰
                trend_col = f'{period}d_Trend'
                df[trend_col] = (df[return_col] > 0).astype(int)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦ä¿¡å·ï¼ˆåŸºäºæ”¶ç›Šç‡ï¼‰
                rs_signal_col = f'{period}d_RS_Signal'
                df[rs_signal_col] = (df[return_col] > 0).astype(int)

        # è®¡ç®—å¤šå‘¨æœŸè¶‹åŠ¿è¯„åˆ†
        trend_cols = [f'{p}d_Trend' for p in periods]
        if all(col in df.columns for col in trend_cols):
            df['Multi_Period_Trend_Score'] = df[trend_cols].sum(axis=1)

        # è®¡ç®—å¤šå‘¨æœŸç›¸å¯¹å¼ºåº¦è¯„åˆ†
        rs_cols = [f'{p}d_RS_Signal' for p in periods]
        if all(col in df.columns for col in rs_cols):
            df['Multi_Period_RS_Score'] = df[rs_cols].sum(axis=1)

        return df

    def calculate_relative_strength(self, stock_df, hsi_df):
        """è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡ï¼ˆç›¸å¯¹äºæ’ç”ŸæŒ‡æ•°ï¼‰"""
        if stock_df.empty or hsi_df.empty:
            return stock_df

        # ç¡®ä¿ç´¢å¼•å¯¹é½
        stock_df = stock_df.copy()
        hsi_df = hsi_df.copy()

        # è®¡ç®—æ’ç”ŸæŒ‡æ•°æ”¶ç›Šç‡
        hsi_df['HSI_Return_1d'] = hsi_df['Close'].pct_change()
        hsi_df['HSI_Return_3d'] = hsi_df['Close'].pct_change(3)
        hsi_df['HSI_Return_5d'] = hsi_df['Close'].pct_change(5)
        hsi_df['HSI_Return_10d'] = hsi_df['Close'].pct_change(10)
        hsi_df['HSI_Return_20d'] = hsi_df['Close'].pct_change(20)
        hsi_df['HSI_Return_60d'] = hsi_df['Close'].pct_change(60)

        # åˆå¹¶æ’ç”ŸæŒ‡æ•°æ•°æ®
        hsi_cols = ['HSI_Return_1d', 'HSI_Return_3d', 'HSI_Return_5d', 'HSI_Return_10d', 'HSI_Return_20d', 'HSI_Return_60d']
        stock_df = stock_df.merge(hsi_df[hsi_cols], left_index=True, right_index=True, how='left')

        # è®¡ç®—ç›¸å¯¹å¼ºåº¦ï¼ˆRS_ratio = (1+stock_ret)/(1+hsi_ret)-1ï¼‰
        periods = [1, 3, 5, 10, 20, 60]
        for period in periods:
            stock_ret_col = f'Return_{period}d'
            hsi_ret_col = f'HSI_Return_{period}d'

            if stock_ret_col in stock_df.columns and hsi_ret_col in stock_df.columns:
                # RS_ratioï¼ˆå¤åˆæ”¶ç›Šæ¯”ï¼‰
                rs_ratio_col = f'RS_Ratio_{period}d'
                stock_df[rs_ratio_col] = (1 + stock_df[stock_ret_col]) / (1 + stock_df[hsi_ret_col]) - 1

                # RS_diffï¼ˆæ”¶ç›Šå·®å€¼ï¼‰
                rs_diff_col = f'RS_Diff_{period}d'
                stock_df[rs_diff_col] = stock_df[stock_ret_col] - stock_df[hsi_ret_col]

        # è·‘èµ¢æ’æŒ‡ï¼ˆåŸºäº5æ—¥ç›¸å¯¹å¼ºåº¦ï¼‰
        if 'RS_Ratio_5d' in stock_df.columns:
            stock_df['Outperforms_HSI'] = (stock_df['RS_Ratio_5d'] > 0).astype(int)

        return stock_df

    def create_market_environment_features(self, stock_df, hsi_df, us_market_df=None):
        """åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰

        Args:
            stock_df: è‚¡ç¥¨æ•°æ®
            hsi_df: æ’ç”ŸæŒ‡æ•°æ•°æ®
            us_market_df: ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        if stock_df.empty or hsi_df.empty:
            return stock_df

        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ HSI_Return_5d åˆ—ï¼ˆç”± calculate_relative_strength åˆ›å»ºï¼‰
        if 'HSI_Return_5d' not in stock_df.columns:
            # å¦‚æœä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå¹¶åˆå¹¶
            hsi_df = hsi_df.copy()
            hsi_df['HSI_Return'] = hsi_df['Close'].pct_change()
            hsi_df['HSI_Return_5d'] = hsi_df['Close'].pct_change(5)
            stock_df = stock_df.merge(hsi_df[['HSI_Return', 'HSI_Return_5d']], left_index=True, right_index=True, how='left')

        # ç›¸å¯¹è¡¨ç°ï¼ˆç›¸å¯¹äºæ’ç”ŸæŒ‡æ•°ï¼‰
        stock_df['Relative_Return'] = stock_df['Return_5d'] - stock_df['HSI_Return_5d']

        # å¦‚æœæä¾›äº†ç¾è‚¡æ•°æ®ï¼Œåˆå¹¶ç¾è‚¡ç‰¹å¾
        if us_market_df is not None and not us_market_df.empty:
            # ç¾è‚¡ç‰¹å¾åˆ—
            us_features = [
                'SP500_Return', 'SP500_Return_5d', 'SP500_Return_20d',
                'NASDAQ_Return', 'NASDAQ_Return_5d', 'NASDAQ_Return_20d',
                'VIX_Change', 'VIX_Ratio_MA20', 'VIX_Level',
                'US_10Y_Yield', 'US_10Y_Yield_Change'
            ]

            # åªåˆå¹¶å­˜åœ¨çš„ç‰¹å¾
            existing_us_features = [f for f in us_features if f in us_market_df.columns]
            if existing_us_features:
                # å¯¹ç¾è‚¡ç‰¹å¾è¿›è¡Œ shift(1)ï¼Œç¡®ä¿ä¸åŒ…å«æœªæ¥ä¿¡æ¯
                # å› ä¸ºç¾è‚¡æ•°æ®æ¯”æ¸¯è‚¡æ™š15å°æ—¶å¼€ç›˜ï¼Œæ‰€ä»¥åœ¨é¢„æµ‹æ¸¯è‚¡ T+1 æ—¥æ¶¨è·Œæ—¶ï¼Œ
                # åªèƒ½ä½¿ç”¨ T æ—¥åŠä¹‹å‰çš„ç¾è‚¡æ•°æ®
                us_market_df_shifted = us_market_df[existing_us_features].shift(1)

                stock_df = stock_df.merge(
                    us_market_df_shifted,
                    left_index=True, right_index=True, how='left'
                )

        return stock_df

    def create_label(self, df, horizon, for_backtest=False):
        """åˆ›å»ºæ ‡ç­¾ï¼šæ¬¡æ—¥æ¶¨è·Œ
        
        Args:
            df: è‚¡ç¥¨æ•°æ®
            horizon: é¢„æµ‹å‘¨æœŸ
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸ç§»é™¤æœ€åhorizonè¡Œï¼‰
        """
        if df.empty or len(df) < horizon + 1:
            return df

        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
        df['Future_Return'] = df['Close'].shift(-horizon) / df['Close'] - 1

        # äºŒåˆ†ç±»æ ‡ç­¾ï¼š1=ä¸Šæ¶¨ï¼Œ0=ä¸‹è·Œ
        df['Label'] = (df['Future_Return'] > 0).astype(int)

        # å¦‚æœä¸æ˜¯å›æµ‹æ¨¡å¼ï¼Œç§»é™¤æœ€åhorizonè¡Œï¼ˆæ²¡æœ‰æ ‡ç­¾çš„æ•°æ®ï¼‰
        if not for_backtest:
            df = df.iloc[:-horizon]

        return df

    def create_technical_fundamental_interactions(self, df):
        """åˆ›å»ºæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢çš„äº¤äº’ç‰¹å¾

        æ ¹æ®ä¸šç•Œæœ€ä½³å®è·µï¼ŒæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢çš„äº¤äº’èƒ½å¤Ÿæ•æ‰éçº¿æ€§å…³ç³»ï¼Œ
        æé«˜æ¨¡å‹é¢„æµ‹å‡†ç¡®ç‡ã€‚å‚è€ƒï¼šarXiv 2025è®ºæ–‡ã€é‡åŒ–äº¤æ˜“æœ€ä½³å®è·µã€‚

        äº¤äº’ç‰¹å¾åˆ—è¡¨ï¼š
        1. RSI Ã— PEï¼šè¶…å–+ä½ä¼°=å¼ºåŠ›ä¹°å…¥ï¼Œè¶…ä¹°+é«˜ä¼°=å¼ºåŠ›å–å‡º
        2. RSI Ã— PBï¼šè¶…å–+ä½ä¼°å€¼=ä»·å€¼æœºä¼š
        3. MACD Ã— ROEï¼šè¶‹åŠ¿å‘ä¸Š+é«˜ç›ˆåˆ©èƒ½åŠ›=å¼ºåŠ²å¢é•¿
        4. MACD_Hist Ã— ROEï¼šåŠ¨èƒ½å¢å¼º+ç›ˆåˆ©èƒ½åŠ›å¼º=åŠ é€Ÿä¸Šæ¶¨
        5. BB_Position Ã— Dividend_Yieldï¼šä¸‹è½¨é™„è¿‘+é«˜è‚¡æ¯=é˜²å®ˆä»·å€¼
        6. Price_Pct_20d Ã— PEï¼šä½ä½+ä½ä¼°=è¶…è·Œåå¼¹
        7. Price_Pct_20d Ã— PBï¼šä½ä½+ä½ä¼°å€¼=ä»·å€¼ä¿®å¤
        8. Price_Pct_20d Ã— ROEï¼šä½ä½+é«˜ç›ˆåˆ©=é”™æ€æœºä¼š
        9. ATR Ã— PEï¼šé«˜æ³¢åŠ¨+ä½ä¼°=é«˜é£é™©é«˜å›æŠ¥
        10. ATR Ã— ROEï¼šé«˜æ³¢åŠ¨+é«˜ç›ˆåˆ©=æˆé•¿æ½œåŠ›
        11. Vol_Ratio Ã— PEï¼šæ”¾é‡+ä½ä¼°=èµ„é‡‘æµå…¥ä»·å€¼è‚¡
        12. OBV_Slope Ã— ROEï¼šèµ„é‡‘æµå…¥+é«˜ç›ˆåˆ©=åŸºæœ¬é¢é©±åŠ¨ä¸Šæ¶¨
        13. CMF Ã— Dividend_Yieldï¼šèµ„é‡‘æµå…¥+é«˜è‚¡æ¯=é˜²å¾¡æ€§ä¹°å…¥
        14. Return_5d Ã— PEï¼šçŸ­æœŸä¸Šæ¶¨+ä½ä¼°å€¼=å¯æŒç»­ä¸Šæ¶¨
        15. Return_5d Ã— ROEï¼šçŸ­æœŸä¸Šæ¶¨+é«˜ç›ˆåˆ©=ç›ˆåˆ©ç¡®è®¤
        """
        if df.empty:
            return df

        # åŸºæœ¬é¢ç‰¹å¾åˆ—è¡¨ï¼ˆåªä½¿ç”¨å®é™…å¯ç”¨çš„ï¼‰
        fundamental_features = ['PE', 'PB']  # ç›®å‰åªæ”¯æŒPEå’ŒPB

        # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨å®é™…å­˜åœ¨çš„åˆ—åï¼‰
        technical_features = ['RSI', 'RSI_ROC', 'MACD', 'MACD_Hist', 'MACD_Hist_ROC',
                             'BB_Position', 'ATR', 'Vol_Ratio', 'CMF',
                             'Return_5d', 'Price_Pct_20d', 'Momentum_5d']

        # é¢„å®šä¹‰çš„é«˜ä»·å€¼äº¤äº’ç»„åˆï¼ˆåŸºäºä¸šç•Œå®è·µï¼Œåªä½¿ç”¨å®é™…å¯ç”¨çš„åŸºæœ¬é¢ç‰¹å¾ï¼‰
        high_value_interactions = [
            # è¶…ä¹°è¶…å–ä¸ä¼°å€¼çš„äº¤äº’
            ('RSI', 'PE'),           # RSI Ã— PE
            ('RSI', 'PB'),           # RSI Ã— PB
            # è¶‹åŠ¿ä¸ä¼°å€¼çš„äº¤äº’
            ('MACD', 'PE'),         # MACD Ã— PE
            ('MACD', 'PB'),         # MACD Ã— PB
            ('MACD_Hist', 'PE'),    # MACDæŸ±çŠ¶å›¾ Ã— PE
            ('MACD_Hist', 'PB'),    # MACDæŸ±çŠ¶å›¾ Ã— PB
            # ä½ç½®ä¸ä¼°å€¼çš„äº¤äº’
            ('Price_Pct_20d', 'PE'), # ä»·æ ¼ä½ç½® Ã— PE
            ('Price_Pct_20d', 'PB'), # ä»·æ ¼ä½ç½® Ã— PB
            # æ³¢åŠ¨ä¸ä¼°å€¼çš„äº¤äº’
            ('ATR', 'PE'),           # ATR Ã— PE
            ('ATR', 'PB'),           # ATR Ã— PB
            # æˆäº¤é‡ä¸ä¼°å€¼çš„äº¤äº’
            ('Vol_Ratio', 'PE'),     # æˆäº¤é‡æ¯”ç‡ Ã— PE
            ('Vol_Ratio', 'PB'),     # æˆäº¤é‡æ¯”ç‡ Ã— PB
            # èµ„é‡‘æµä¸ä¼°å€¼çš„äº¤äº’
            ('CMF', 'PE'),           # CMF Ã— PE
            ('CMF', 'PB'),           # CMF Ã— PB
            # æ”¶ç›Šä¸ä¼°å€¼çš„äº¤äº’
            ('Return_5d', 'PE'),     # 5æ—¥æ”¶ç›Š Ã— PE
            ('Return_5d', 'PB'),     # 5æ—¥æ”¶ç›Š Ã— PB
            # åŠ¨é‡ä¸ä¼°å€¼çš„äº¤äº’
            ('Momentum_5d', 'PE'),   # 5æ—¥åŠ¨é‡ Ã— PE
            ('Momentum_5d', 'PB'),   # 5æ—¥åŠ¨é‡ Ã— PB
        ]

        print(f"ğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")

        interaction_count = 0
        for tech_feat, fund_feat in high_value_interactions:
            if tech_feat in df.columns and fund_feat in df.columns:
                # äº¤äº’ç‰¹å¾å‘½åï¼šæŠ€æœ¯_åŸºæœ¬é¢
                interaction_name = f"{tech_feat}_{fund_feat}"
                df[interaction_name] = df[tech_feat] * df[fund_feat]
                interaction_count += 1

        print(f"âœ… æˆåŠŸç”Ÿæˆ {interaction_count} ä¸ªæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾")

        # åˆ é™¤æ‰€æœ‰å€¼å…¨ä¸ºNaNçš„äº¤äº’ç‰¹å¾ï¼ˆåŸºæœ¬é¢æ•°æ®ä¸å¯ç”¨å¯¼è‡´çš„ï¼‰
        interaction_cols = [col for col in df.columns if any(sub in col for sub in ['_PE', '_PB', '_ROE', '_ROA', '_Dividend_Yield', '_EPS', '_Net_Margin', '_Gross_Margin'])]
        cols_to_drop = [col for col in interaction_cols if df[col].isnull().all()]
        if cols_to_drop:
            df = df.drop(columns=cols_to_drop)
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_to_drop)} ä¸ªå…¨ä¸ºNaNçš„äº¤äº’ç‰¹å¾")

        return df

    def create_sentiment_features(self, code, df):
        """åˆ›å»ºæƒ…æ„ŸæŒ‡æ ‡ç‰¹å¾ï¼ˆå‚è€ƒ hk_smart_money_tracker.pyï¼‰

        ä»æ–°é—»æ•°æ®ä¸­è®¡ç®—æƒ…æ„Ÿè¶‹åŠ¿ç‰¹å¾ï¼š
        - sentiment_ma3: 3æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆçŸ­æœŸæƒ…ç»ªï¼‰
        - sentiment_ma7: 7æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆä¸­æœŸæƒ…ç»ªï¼‰
        - sentiment_ma14: 14æ—¥æƒ…æ„Ÿç§»åŠ¨å¹³å‡ï¼ˆé•¿æœŸæƒ…ç»ªï¼‰
        - sentiment_volatility: æƒ…æ„Ÿæ³¢åŠ¨ç‡ï¼ˆæƒ…ç»ªç¨³å®šæ€§ï¼‰
        - sentiment_change_rate: æƒ…æ„Ÿå˜åŒ–ç‡ï¼ˆæƒ…ç»ªå˜åŒ–æ–¹å‘ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«æƒ…æ„Ÿç‰¹å¾çš„å­—å…¸
        """
        try:
            # è¯»å–æ–°é—»æ•°æ®
            news_file_path = 'data/all_stock_news_records.csv'
            if not os.path.exists(news_file_path):
                # æ²¡æœ‰æ–°é—»æ–‡ä»¶ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            news_df = pd.read_csv(news_file_path)
            if news_df.empty:
                # æ–°é—»æ–‡ä»¶ä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # ç­›é€‰è¯¥è‚¡ç¥¨çš„æ–°é—»
            stock_news = news_df[news_df['è‚¡ç¥¨ä»£ç '] == code].copy()
            if stock_news.empty:
                # è¯¥è‚¡ç¥¨æ²¡æœ‰æ–°é—»ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # è½¬æ¢æ—¥æœŸæ ¼å¼
            stock_news['æ–°é—»æ—¶é—´'] = pd.to_datetime(stock_news['æ–°é—»æ—¶é—´'])

            # åªä½¿ç”¨å·²åˆ†ææƒ…æ„Ÿåˆ†æ•°çš„æ–°é—»
            stock_news = stock_news[stock_news['æƒ…æ„Ÿåˆ†æ•°'].notna()].copy()
            if stock_news.empty:
                # æ²¡æœ‰æƒ…æ„Ÿåˆ†æ•°æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sentiment_ma3': 0.0,
                    'sentiment_ma7': 0.0,
                    'sentiment_ma14': 0.0,
                    'sentiment_volatility': 0.0,
                    'sentiment_change_rate': 0.0,
                    'sentiment_days': 0
                }

            # ç¡®ä¿æŒ‰æ—¥æœŸæ’åº
            stock_news = stock_news.sort_values('æ–°é—»æ—¶é—´')

            # æŒ‰æ—¥æœŸèšåˆæƒ…æ„Ÿåˆ†æ•°ï¼ˆä½¿ç”¨å¹³å‡å€¼ï¼‰
            sentiment_by_date = stock_news.groupby('æ–°é—»æ—¶é—´')['æƒ…æ„Ÿåˆ†æ•°'].mean()

            # è·å–å®é™…æ•°æ®å¤©æ•°
            actual_days = len(sentiment_by_date)

            # åŠ¨æ€è°ƒæ•´ç§»åŠ¨å¹³å‡çª—å£
            window_ma3 = min(3, actual_days)
            window_ma7 = min(7, actual_days)
            window_ma14 = min(14, actual_days)
            window_volatility = min(14, actual_days)

            # è®¡ç®—ç§»åŠ¨å¹³å‡
            sentiment_ma3 = sentiment_by_date.rolling(window=window_ma3, min_periods=1).mean().iloc[-1]
            sentiment_ma7 = sentiment_by_date.rolling(window=window_ma7, min_periods=1).mean().iloc[-1]
            sentiment_ma14 = sentiment_by_date.rolling(window=window_ma14, min_periods=1).mean().iloc[-1]

            # è®¡ç®—æ³¢åŠ¨ç‡
            sentiment_volatility = sentiment_by_date.rolling(window=window_volatility, min_periods=2).std().iloc[-1] if actual_days >= 2 else np.nan

            # è®¡ç®—å˜åŒ–ç‡
            if actual_days >= 2:
                latest_sentiment = sentiment_by_date.iloc[-1]
                prev_sentiment = sentiment_by_date.iloc[-2]
                sentiment_change_rate = (latest_sentiment - prev_sentiment) / abs(prev_sentiment) if prev_sentiment != 0 else np.nan
            else:
                sentiment_change_rate = np.nan

            return {
                'sentiment_ma3': sentiment_ma3,
                'sentiment_ma7': sentiment_ma7,
                'sentiment_ma14': sentiment_ma14,
                'sentiment_volatility': sentiment_volatility,
                'sentiment_change_rate': sentiment_change_rate,
                'sentiment_days': actual_days
            }

        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æƒ…æ„Ÿç‰¹å¾å¤±è´¥ {code}: {e}")
            # å¼‚å¸¸æƒ…å†µè¿”å›é»˜è®¤å€¼
            return {
                'sentiment_ma3': 0.0,
                'sentiment_ma7': 0.0,
                'sentiment_ma14': 0.0,
                'sentiment_volatility': 0.0,
                'sentiment_change_rate': 0.0,
                'sentiment_days': 0
            }

    def create_topic_features(self, code, df):
        """åˆ›å»ºä¸»é¢˜åˆ†å¸ƒç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰

        ä»æ–°é—»æ•°æ®ä¸­æå–ä¸»é¢˜åˆ†å¸ƒç‰¹å¾ï¼š
        - Topic_1 ~ Topic_10: 10ä¸ªä¸»é¢˜çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆ0-1ä¹‹é—´ï¼Œæ€»å’Œä¸º1ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«ä¸»é¢˜ç‰¹å¾çš„å­—å…¸
        """
        try:
            from ml_services.topic_modeling import TopicModeler

            # åˆ›å»ºä¸»é¢˜å»ºæ¨¡å™¨
            topic_modeler = TopicModeler(n_topics=10, language='mixed')

            # å°è¯•åŠ è½½å·²è®­ç»ƒçš„æ¨¡å‹
            model_path = 'data/lda_topic_model.pkl'

            if os.path.exists(model_path):
                topic_modeler.load_model(model_path)

                # è·å–è‚¡ç¥¨ä¸»é¢˜ç‰¹å¾
                topic_features = topic_modeler.get_stock_topic_features(code)

                if topic_features:
                    print(f"âœ… è·å–ä¸»é¢˜ç‰¹å¾: {code}")
                    return topic_features
                else:
                    print(f"âš ï¸  è¯¥è‚¡ç¥¨æ²¡æœ‰æ–°é—»æ•°æ®: {code}")
                    return {f'Topic_{i+1}': 0.0 for i in range(10)}
            else:
                print(f"âš ï¸  ä¸»é¢˜æ¨¡å‹ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ: python ml_services/topic_modeling.py")
                return {f'Topic_{i+1}': 0.0 for i in range(10)}

        except Exception as e:
            print(f"âŒ åˆ›å»ºä¸»é¢˜ç‰¹å¾å¤±è´¥ {code}: {e}")
            return {f'Topic_{i+1}': 0.0 for i in range(10)}

    def create_topic_sentiment_interaction_features(self, code, df):
        """åˆ›å»ºä¸»é¢˜ä¸æƒ…æ„Ÿäº¤äº’ç‰¹å¾

        å°†ä¸»é¢˜åˆ†å¸ƒä¸æƒ…æ„Ÿè¯„åˆ†è¿›è¡Œäº¤äº’ï¼Œæ•æ‰"æŸä¸ªä¸»é¢˜çš„æ–°é—»å¸¦æœ‰æŸç§æƒ…æ„Ÿæ—¶"çš„ç‰¹å®šæ•ˆæœï¼š
        - Topic_1 Ã— sentiment_ma3: ä¸»é¢˜1ä¸3æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_ma7: ä¸»é¢˜1ä¸7æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_ma14: ä¸»é¢˜1ä¸14æ—¥ç§»åŠ¨å¹³å‡æƒ…æ„Ÿçš„äº¤äº’
        - Topic_1 Ã— sentiment_volatility: ä¸»é¢˜1ä¸æƒ…æ„Ÿæ³¢åŠ¨ç‡çš„äº¤äº’
        - Topic_1 Ã— sentiment_change_rate: ä¸»é¢˜1ä¸æƒ…æ„Ÿå˜åŒ–ç‡çš„äº¤äº’
        - ... å…±10ä¸ªä¸»é¢˜ Ã— 5ä¸ªæƒ…æ„ŸæŒ‡æ ‡ = 50ä¸ªäº¤äº’ç‰¹å¾

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–ä¸»é¢˜ç‰¹å¾
            topic_features = self.create_topic_features(code, df)

            # è·å–æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.create_sentiment_features(code, df)

            # åˆ›å»ºäº¤äº’ç‰¹å¾
            interaction_features = {}

            # æƒ…æ„ŸæŒ‡æ ‡åˆ—è¡¨
            sentiment_keys = ['sentiment_ma3', 'sentiment_ma7', 'sentiment_ma14',
                            'sentiment_volatility', 'sentiment_change_rate']

            # ä¸ºæ¯ä¸ªä¸»é¢˜ä¸æ¯ä¸ªæƒ…æ„ŸæŒ‡æ ‡åˆ›å»ºäº¤äº’ç‰¹å¾
            for topic_idx in range(10):
                topic_key = f'Topic_{topic_idx + 1}'
                topic_prob = topic_features.get(topic_key, 0.0)

                for sentiment_key in sentiment_keys:
                    sentiment_value = sentiment_features.get(sentiment_key, 0.0)

                    # äº¤äº’ç‰¹å¾ = ä¸»é¢˜æ¦‚ç‡ Ã— æƒ…æ„Ÿå€¼
                    interaction_key = f'{topic_key}_x_{sentiment_key}'
                    interaction_features[interaction_key] = topic_prob * sentiment_value

            if interaction_features:
                print(f"âœ… è·å–ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾: {code} (å…±{len(interaction_features)}ä¸ª)")
                return interaction_features
            else:
                print(f"âš ï¸  æ— æ³•åˆ›å»ºä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾: {code}")
                return {}

        except Exception as e:
            print(f"âŒ åˆ›å»ºä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾å¤±è´¥ {code}: {e}")
            return {}

    def create_expectation_gap_features(self, code, df):
        """åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾

        è®¡ç®—æ–°é—»æƒ…æ„Ÿç›¸å¯¹äºå¸‚åœºé¢„æœŸçš„å·®è·ï¼š
        - Sentiment_Gap_MA7: å½“å‰æƒ…æ„Ÿä¸7æ—¥ç§»åŠ¨å¹³å‡çš„å·®è·
        - Sentiment_Gap_MA14: å½“å‰æƒ…æ„Ÿä¸14æ—¥ç§»åŠ¨å¹³å‡çš„å·®è·
        - Positive_Surprise: æ­£å‘æ„å¤–ï¼ˆæƒ…æ„Ÿè¶…è¿‡é¢„æœŸçš„ç¨‹åº¦ï¼‰
        - Negative_Surprise: è´Ÿå‘æ„å¤–ï¼ˆæƒ…æ„Ÿä½äºé¢„æœŸçš„ç¨‹åº¦ï¼‰
        - Expectation_Change_Strength: é¢„æœŸå˜åŒ–å¼ºåº¦

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«é¢„æœŸå·®è·ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.create_sentiment_features(code, df)

            # åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾
            expectation_gap_features = {}

            # è·å–å½“å‰æƒ…æ„Ÿå€¼ï¼ˆä½¿ç”¨æœ€æ–°çš„æƒ…æ„Ÿå€¼ï¼‰
            current_sentiment = sentiment_features.get('sentiment_ma3', 0.0)

            # è®¡ç®—ä¸ä¸åŒå‘¨æœŸç§»åŠ¨å¹³å‡çš„å·®è·
            ma7 = sentiment_features.get('sentiment_ma7', 0.0)
            ma14 = sentiment_features.get('sentiment_ma14', 0.0)

            # é¢„æœŸå·®è· = å½“å‰æƒ…æ„Ÿ - é•¿æœŸç§»åŠ¨å¹³å‡
            expectation_gap_features['Sentiment_Gap_MA7'] = current_sentiment - ma7
            expectation_gap_features['Sentiment_Gap_MA14'] = current_sentiment - ma14

            # æ­£å‘æ„å¤–ï¼ˆæƒ…æ„Ÿè¶…é¢„æœŸï¼Œå·®è·ä¸ºæ­£ï¼‰
            expectation_gap_features['Positive_Surprise'] = max(0, current_sentiment - ma14)

            # è´Ÿå‘æ„å¤–ï¼ˆæƒ…æ„Ÿä¸åŠé¢„æœŸï¼Œå·®è·ä¸ºè´Ÿï¼Œå–ç»å¯¹å€¼ï¼‰
            expectation_gap_features['Negative_Surprise'] = max(0, ma14 - current_sentiment)

            # ä½¿ç”¨æƒ…æ„Ÿå˜åŒ–ç‡æ¥è¡¡é‡é¢„æœŸå·®è·çš„å¼ºåº¦
            sentiment_change_rate = sentiment_features.get('sentiment_change_rate', 0.0)
            expectation_gap_features['Expectation_Change_Strength'] = abs(sentiment_change_rate)

            if expectation_gap_features:
                print(f"âœ… è·å–é¢„æœŸå·®è·ç‰¹å¾: {code} (å…±{len(expectation_gap_features)}ä¸ª)")
                return expectation_gap_features
            else:
                print(f"âš ï¸  æ— æ³•åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾: {code}")
                return {}

        except Exception as e:
            print(f"âŒ åˆ›å»ºé¢„æœŸå·®è·ç‰¹å¾å¤±è´¥ {code}: {e}")
            return {}

    def create_sector_features(self, code, df):
        """åˆ›å»ºæ¿å—åˆ†æç‰¹å¾ï¼ˆä¼˜åŒ–ç‰ˆï¼Œä½¿ç”¨ç¼“å­˜ï¼‰

        ä»æ¿å—åˆ†æä¸­æå–æ¿å—æ¶¨è·Œå¹…ã€æ¿å—æ’åã€æ¿å—è¶‹åŠ¿ç­‰ç‰¹å¾ï¼š
        - sector_avg_change: æ¿å—å¹³å‡æ¶¨è·Œå¹…ï¼ˆ1æ—¥/5æ—¥/20æ—¥ï¼‰
        - sector_rank: æ¿å—æ¶¨è·Œå¹…æ’åï¼ˆ1æ—¥/5æ—¥/20æ—¥ï¼‰
        - sector_rising_ratio: æ¿å—ä¸Šæ¶¨è‚¡ç¥¨æ¯”ä¾‹
        - sector_total_volume: æ¿å—æ€»æˆäº¤é‡
        - sector_stock_count: æ¿å—è‚¡ç¥¨æ•°é‡
        - sector_trend: æ¿å—è¶‹åŠ¿ï¼ˆé‡åŒ–ä¸ºæ•°å€¼ï¼‰
        - sector_flow_score: æ¿å—èµ„é‡‘æµå‘è¯„åˆ†
        - is_sector_leader: æ˜¯å¦ä¸ºæ¿å—é¾™å¤´
        - sector_best_stock_change: æ¿å—æœ€ä½³è‚¡ç¥¨æ¶¨è·Œå¹…
        - sector_worst_stock_change: æ¿å—æœ€å·®è‚¡ç¥¨æ¶¨è·Œå¹…

        Args:
            code: è‚¡ç¥¨ä»£ç 
            df: è‚¡ç¥¨æ•°æ®DataFrameï¼ˆæ—¥æœŸç´¢å¼•ï¼‰

        Returns:
            dict: åŒ…å«æ¿å—ç‰¹å¾çš„å­—å…¸
        """
        try:
            # è·å–æ¿å—åˆ†æå™¨ï¼ˆå•ä¾‹ï¼‰
            sector_analyzer = self._get_sector_analyzer()
            if sector_analyzer is None:
                # æ¨¡å—ä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sector_avg_change_1d': 0.0,
                    'sector_avg_change_5d': 0.0,
                    'sector_avg_change_20d': 0.0,
                    'sector_rank_1d': 0,
                    'sector_rank_5d': 0,
                    'sector_rank_20d': 0,
                    'sector_rising_ratio_1d': 0.5,
                    'sector_rising_ratio_5d': 0.5,
                    'sector_rising_ratio_20d': 0.5,
                    'sector_total_volume': 0.0,
                    'sector_stock_count': 0,
                    'sector_trend_score': 0.0,
                    'sector_flow_score': 0.0,
                    'is_sector_leader': 0,
                    'sector_best_stock_change': 0.0,
                    'sector_worst_stock_change': 0.0,
                    'sector_outperform_hsi': 0
                }

            # è·å–è‚¡ç¥¨æ‰€å±æ¿å—
            sector_info = sector_analyzer.stock_mapping.get(code)
            if not sector_info:
                # æœªæ‰¾åˆ°æ¿å—ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    'sector_avg_change_1d': 0.0,
                    'sector_avg_change_5d': 0.0,
                    'sector_avg_change_20d': 0.0,
                    'sector_rank_1d': 0,
                    'sector_rank_5d': 0,
                    'sector_rank_20d': 0,
                    'sector_rising_ratio_1d': 0.5,
                    'sector_rising_ratio_5d': 0.5,
                    'sector_rising_ratio_20d': 0.5,
                    'sector_total_volume': 0.0,
                    'sector_stock_count': 0,
                    'sector_trend_score': 0.0,
                    'sector_flow_score': 0.0,
                    'is_sector_leader': 0,
                    'sector_best_stock_change': 0.0,
                    'sector_worst_stock_change': 0.0,
                    'sector_outperform_hsi': 0
                }

            sector_code = sector_info['sector']

            features = {}

            # è®¡ç®—ä¸åŒå‘¨æœŸçš„æ¿å—è¡¨ç°ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
            for period in [1, 5, 20]:
                try:
                    perf_df = self._get_sector_performance(period)

                    if perf_df is not None and not perf_df.empty:
                        # æ‰¾åˆ°è¯¥æ¿å—çš„æ’å
                        sector_row = perf_df[perf_df['sector_code'] == sector_code]

                        if not sector_row.empty:
                            sector_data = sector_row.iloc[0]

                            # æ¿å—å¹³å‡æ¶¨è·Œå¹…
                            features[f'sector_avg_change_{period}d'] = sector_data['avg_change_pct']

                            # æ¿å—æ’å
                            sector_rank = perf_df[perf_df['sector_code'] == sector_code].index[0] + 1
                            features[f'sector_rank_{period}d'] = sector_rank

                            # æ¿å—ä¸Šæ¶¨è‚¡ç¥¨æ¯”ä¾‹
                            rising_count = sum(1 for s in sector_data['stocks'] if s['change_pct'] > 0)
                            total_count = len(sector_data['stocks'])
                            features[f'sector_rising_ratio_{period}d'] = rising_count / total_count if total_count > 0 else 0.5

                            # æ¿å—æ€»æˆäº¤é‡
                            features['sector_total_volume'] = sector_data['total_volume']

                            # æ¿å—è‚¡ç¥¨æ•°é‡
                            features['sector_stock_count'] = sector_data['stock_count']

                            # æœ€ä½³å’Œæœ€å·®è‚¡ç¥¨è¡¨ç°
                            if sector_data['best_stock']:
                                features['sector_best_stock_change'] = sector_data['best_stock']['change_pct']
                            if sector_data['worst_stock']:
                                features['sector_worst_stock_change'] = sector_data['worst_stock']['change_pct']

                            # æ˜¯å¦ä¸ºæ¿å—é¾™å¤´ï¼ˆå‰3åï¼‰
                            features['is_sector_leader'] = 1 if sector_rank <= 3 else 0
                        else:
                            # æ¿å—æœªæ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
                            features[f'sector_avg_change_{period}d'] = 0.0
                            features[f'sector_rank_{period}d'] = 0
                            features[f'sector_rising_ratio_{period}d'] = 0.5
                    else:
                        # æ— æ³•è·å–æ¿å—æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å€¼
                        features[f'sector_avg_change_{period}d'] = 0.0
                        features[f'sector_rank_{period}d'] = 0
                        features[f'sector_rising_ratio_{period}d'] = 0.5

                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—æ¿å—è¡¨ç°å¤±è´¥ (period={period}): {e}")
                    features[f'sector_avg_change_{period}d'] = 0.0
                    features[f'sector_rank_{period}d'] = 0
                    features[f'sector_rising_ratio_{period}d'] = 0.5

            # è®¡ç®—æ¿å—è¶‹åŠ¿
            try:
                trend_result = sector_analyzer.analyze_sector_trend(sector_code, days=20)

                if 'trend' in trend_result:
                    # å°†è¶‹åŠ¿é‡åŒ–ä¸ºæ•°å€¼
                    trend_mapping = {
                        'å¼ºåŠ¿ä¸Šæ¶¨': 2.0,
                        'æ¸©å’Œä¸Šæ¶¨': 1.0,
                        'éœ‡è¡æ•´ç†': 0.0,
                        'æ¸©å’Œä¸‹è·Œ': -1.0,
                        'å¼ºåŠ¿ä¸‹è·Œ': -2.0
                    }
                    features['sector_trend_score'] = trend_mapping.get(trend_result['trend'], 0.0)
                else:
                    features['sector_trend_score'] = 0.0
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—æ¿å—è¶‹åŠ¿å¤±è´¥: {e}")
                features['sector_trend_score'] = 0.0

            # è®¡ç®—æ¿å—èµ„é‡‘æµå‘
            try:
                flow_result = sector_analyzer.analyze_sector_fund_flow(sector_code, days=5)

                if 'avg_flow_score' in flow_result:
                    features['sector_flow_score'] = flow_result['avg_flow_score']
                else:
                    features['sector_flow_score'] = 0.0
            except Exception as e:
                print(f"âš ï¸ è®¡ç®—æ¿å—èµ„é‡‘æµå‘å¤±è´¥: {e}")
                features['sector_flow_score'] = 0.0

            # åˆ¤æ–­æ¿å—æ˜¯å¦è·‘èµ¢æ’æŒ‡ï¼ˆåŸºäºæ¿å—å¹³å‡æ¶¨è·Œå¹…ï¼‰
            if 'sector_avg_change_1d' in features and 'sector_avg_change_5d' in features:
                # ç®€åŒ–å¤„ç†ï¼šå‡è®¾æ’æŒ‡æ¶¨è·Œå¹…ä¸º0ï¼ˆå®é™…åº”è¯¥ä»æ’æŒ‡æ•°æ®ä¸­è·å–ï¼‰
                # è¿™é‡Œä½¿ç”¨æ¿å—è‡ªèº«çš„æ¶¨è·Œå¹…ä½œä¸ºå‚è€ƒ
                features['sector_outperform_hsi'] = 1 if features['sector_avg_change_5d'] > 0 else 0

            return features

        except Exception as e:
            print(f"âš ï¸ è®¡ç®—æ¿å—ç‰¹å¾å¤±è´¥ {code}: {e}")
            # å¼‚å¸¸æƒ…å†µè¿”å›é»˜è®¤å€¼
            return {
                'sector_avg_change_1d': 0.0,
                'sector_avg_change_5d': 0.0,
                'sector_avg_change_20d': 0.0,
                'sector_rank_1d': 0,
                'sector_rank_5d': 0,
                'sector_rank_20d': 0,
                'sector_rising_ratio_1d': 0.5,
                'sector_rising_ratio_5d': 0.5,
                'sector_rising_ratio_20d': 0.5,
                'sector_total_volume': 0.0,
                'sector_stock_count': 0,
                'sector_trend_score': 0.0,
                'sector_flow_score': 0.0,
                'is_sector_leader': 0,
                'sector_best_stock_change': 0.0,
                'sector_worst_stock_change': 0.0,
                'sector_outperform_hsi': 0
            }

    def create_interaction_features(self, df):
        """åˆ›å»ºæ‰€æœ‰å¯èƒ½çš„äº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰

        ç”Ÿæˆç­–ç•¥ï¼šå°†æ‰€æœ‰ç±»åˆ«å‹ç‰¹å¾ï¼ˆ13ä¸ªï¼‰ä¸æ‰€æœ‰æ•°å€¼å‹ç‰¹å¾ï¼ˆ90ä¸ªï¼‰è¿›è¡Œäº¤å‰ï¼Œ
        å½¢æˆ 1170 ä¸ªäº¤å‰ç‰¹å¾ã€‚GBDT+LR ç®—æ³•ä¼šè‡ªåŠ¨è¿‡æ»¤æ— ç”¨ç‰¹å¾ã€‚
        """
        if df.empty:
            return df

        # ç±»åˆ«å‹ç‰¹å¾ï¼ˆ13ä¸ªï¼‰
        categorical_features = [
            'Outperforms_HSI',
            'Strong_Volume_Up',
            'Weak_Volume_Down',
            '3d_Trend', '5d_Trend', '10d_Trend', '20d_Trend', '60d_Trend',
            '3d_RS_Signal', '5d_RS_Signal', '10d_RS_Signal', '20d_RS_Signal', '60d_RS_Signal'
        ]

        # æ•°å€¼å‹ç‰¹å¾ï¼ˆæ’é™¤ç±»åˆ«å‹ç‰¹å¾ã€æ ‡ç­¾å’ŒåŸå§‹ä»·æ ¼æ•°æ®ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Returns', 'TP', 'MF_Multiplier', 'MF_Volume',
                          'High_Max', 'Low_Min'] + categorical_features

        numeric_features = [col for col in df.columns if col not in exclude_columns]

        print(f"ç”Ÿæˆäº¤å‰ç‰¹å¾: {len(categorical_features)} ä¸ªç±»åˆ« Ã— {len(numeric_features)} ä¸ªæ•°å€¼ = {len(categorical_features) * len(numeric_features)} ä¸ªäº¤å‰ç‰¹å¾")

        # ç”Ÿæˆæ‰€æœ‰äº¤å‰ç‰¹å¾
        interaction_count = 0
        for cat_feat in categorical_features:
            if cat_feat not in df.columns:
                continue

            for num_feat in numeric_features:
                if num_feat not in df.columns:
                    continue

                # äº¤å‰ç‰¹å¾å‘½åï¼šç±»åˆ«_æ•°å€¼
                interaction_name = f"{cat_feat}_{num_feat}"
                df[interaction_name] = df[cat_feat] * df[num_feat]
                interaction_count += 1

        print(f"âœ… æˆåŠŸç”Ÿæˆ {interaction_count} ä¸ªäº¤å‰ç‰¹å¾")
        return df


class MLTradingModel:
    """æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹"""

    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.processor = BaseModelProcessor()
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.horizon = 1  # é»˜è®¤é¢„æµ‹å‘¨æœŸ
        self.model_type = 'lgbm'  # æ¨¡å‹ç±»å‹æ ‡è¯†

    def load_selected_features(self, filepath=None, current_feature_names=None):
        """åŠ è½½é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨ç‰¹å¾åç§°äº¤é›†ï¼Œç¡®ä¿ç‰¹å¾å­˜åœ¨ï¼‰

        Args:
            filepath: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„ï¼‰
            current_feature_names: å½“å‰æ•°æ®é›†çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        import os
        import glob

        if filepath is None:
            # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾åç§°æ–‡ä»¶
            pattern = 'output/selected_features_*.csv'
            files = glob.glob(pattern)
            if not files:
                return None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            filepath = max(files, key=os.path.getmtime)

        try:
            import pandas as pd
            # è¯»å–ç‰¹å¾åç§°
            df = pd.read_csv(filepath)
            selected_names = df['Feature_Name'].tolist()

            print(f"ğŸ“‚ åŠ è½½ç‰¹å¾åˆ—è¡¨æ–‡ä»¶: {filepath}")
            print(f"âœ… åŠ è½½äº† {len(selected_names)} ä¸ªé€‰æ‹©çš„ç‰¹å¾")

            # å¦‚æœæä¾›äº†å½“å‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨äº¤é›†
            if current_feature_names is not None:
                current_set = set(current_feature_names)
                selected_set = set(selected_names)
                available_set = current_set & selected_set
                
                available_names = list(available_set)
                print(f"ğŸ“Š å½“å‰æ•°æ®é›†ç‰¹å¾æ•°é‡: {len(current_feature_names)}")
                print(f"ğŸ“Š é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_names)}")
                print(f"ğŸ“Š å®é™…å¯ç”¨çš„ç‰¹å¾æ•°é‡: {len(available_names)}")
                print(f"âš ï¸  {len(selected_set) - len(available_names)} ä¸ªç‰¹å¾åœ¨å½“å‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
                
                return available_names
            else:
                return selected_names

        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç‰¹å¾åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1, for_backtest=False):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼Œä¼˜åŒ–ç‰ˆï¼‰
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            for_backtest: æ˜¯å¦ä¸ºå›æµ‹å‡†å¤‡æ•°æ®ï¼ˆTrueæ—¶ä¸åº”ç”¨horizonè¿‡æ»¤ï¼‰
        """
        self.horizon = horizon
        all_data = []

        # ========== æ­¥éª¤1ï¼šè·å–å…±äº«æ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰ ==========
        print("ğŸ“Š è·å–å…±äº«æ•°æ®...")
        
        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            print(f"âœ… æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            print("âš ï¸ æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼Œæ‰€æœ‰è‚¡ç¥¨å…±äº«ï¼‰
        hsi_df = get_hsi_data_with_cache(period_days=730)
        if hsi_df is None or hsi_df.empty:
            raise ValueError("æ— æ³•è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®")

        # ========== æ­¥éª¤2ï¼šå¹¶è¡Œä¸‹è½½è‚¡ç¥¨æ•°æ® ==========
        print(f"\nğŸš€ å¹¶è¡Œä¸‹è½½ {len(codes)} åªè‚¡ç¥¨æ•°æ®...")
        
        def fetch_single_stock_data(code):
            """è·å–å•åªè‚¡ç¥¨æ•°æ®"""
            try:
                stock_code = code.replace('.HK', '')
                stock_df = get_stock_data_with_cache(stock_code, period_days=730)
                if stock_df is not None and not stock_df.empty:
                    return (code, stock_df)
                return None
            except Exception as e:
                print(f"âš ï¸ ä¸‹è½½è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                return None

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½ï¼ˆæœ€å¤š8ä¸ªå¹¶å‘ï¼‰
        stock_data_list = []
        with ThreadPoolExecutor(max_workers=8) as executor:
            future_to_code = {executor.submit(fetch_single_stock_data, code): code for code in codes}
            
            for i, future in enumerate(as_completed(future_to_code), 1):
                result = future.result()
                if result is not None:
                    stock_data_list.append(result)
                    print(f"  âœ… [{i}/{len(codes)}] {result[0]}")

        print(f"âœ… æˆåŠŸä¸‹è½½ {len(stock_data_list)} åªè‚¡ç¥¨æ•°æ®")

        # ========== æ­¥éª¤3ï¼šè®¡ç®—ç‰¹å¾ ==========
        print(f"\nğŸ”§ è®¡ç®—ç‰¹å¾...")
        
        for i, (code, stock_df) in enumerate(stock_data_list, 1):
            try:
                print(f"  [{i}/{len(stock_data_list)}] å¤„ç†è‚¡ç¥¨: {code}")

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡ï¼ˆä½¿ç”¨å…±äº«çš„æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼‰
                stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value

                # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
                sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
                for key, value in sentiment_features.items():
                    stock_df[key] = value

                # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
                topic_features = self.feature_engineer.create_topic_features(code, stock_df)
                for key, value in topic_features.items():
                    stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

                # æ·»åŠ æ¿å—ç‰¹å¾
                sector_features = self.feature_engineer.create_sector_features(code, stock_df)
                for key, value in sector_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆä¿ç•™æ—¥æœŸç´¢å¼•ï¼Œä¸é‡ç½®ç´¢å¼•ï¼‰
        df = pd.concat(all_data, ignore_index=False)

        # æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        df = df.sort_index()

        # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆå…ˆæ‰§è¡Œï¼Œå› ä¸ºè¿™æ˜¯é«˜ä»·å€¼ç‰¹å¾ï¼‰
        print("\nğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")
        df = self.feature_engineer.create_technical_fundamental_interactions(df)

        # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰
        print("\nğŸ”— ç”Ÿæˆäº¤å‰ç‰¹å¾...")
        df = self.feature_engineer.create_interaction_features(df)

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1, use_feature_selection=False):
        """è®­ç»ƒæ¨¡å‹

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
        """
        print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # å…ˆåˆ é™¤å…¨ä¸ºNaNçš„åˆ—ï¼ˆé¿å…dropnaåˆ é™¤æ‰€æœ‰è¡Œï¼‰
        cols_all_nan = df.columns[df.isnull().all()].tolist()
        if cols_all_nan:
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_all_nan)} ä¸ªå…¨ä¸ºNaNçš„åˆ—")
            df = df.drop(columns=cols_all_nan)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼ˆdropna å¯èƒ½ä¼šæ”¹å˜é¡ºåºï¼‰
        df = df.sort_index()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        print(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        # æ³¨æ„ï¼šGBDT+LRå¯¹ç‰¹å¾é€‰æ‹©ä¸æ•æ„Ÿï¼Œå»ºè®®ä¸ä½¿ç”¨
        if use_feature_selection and self.model_type == 'lgbm':
            print("\nğŸ¯ åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆLightGBMï¼‰...")
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)
            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                print(f"âœ… ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
        elif use_feature_selection and self.model_type == 'gbdt':
            print("\nğŸ¯ åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆGBDTï¼‰...")
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)
            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                print(f"âœ… ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")

        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç¼–ç ï¼‰
        categorical_features = []
        self.categorical_encoders = {}  # å­˜å‚¨ç¼–ç å™¨ï¼Œç”¨äºé¢„æµ‹æ—¶è§£ç 

        for col in self.feature_columns:
            if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                print(f"  ç¼–ç åˆ†ç±»ç‰¹å¾: {col}")
                categorical_features.append(col)
                # ä½¿ç”¨LabelEncoderè¿›è¡Œç¼–ç 
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.categorical_encoders[col] = le

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=5)

        # æ ¹æ®é¢„æµ‹å‘¨æœŸè°ƒæ•´æ­£åˆ™åŒ–å‚æ•°ï¼ˆåˆ†å‘¨æœŸä¼˜åŒ–ç­–ç•¥ï¼‰
        # æ¬¡æ—¥æ¨¡å‹ï¼šæœ€å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
        # ä¸€å‘¨æ¨¡å‹ï¼šé€‚åº¦æ­£åˆ™åŒ–ä¿æŒå­¦ä¹ èƒ½åŠ›
        # ä¸€ä¸ªæœˆæ¨¡å‹ï¼šå¢å¼ºæ­£åˆ™åŒ–ï¼ˆç‰¹å¾æ•°é‡å¤šï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
        if horizon == 1:
            # æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆæœ€å¼ºæ­£åˆ™åŒ–ï¼‰
            print("ä½¿ç”¨æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆå¼ºæ­£åˆ™åŒ–ï¼‰...")
            lgb_params = {
                'n_estimators': 40,           # å‡å°‘æ ‘æ•°é‡ï¼ˆ50â†’40ï¼‰
                'learning_rate': 0.02,         # é™ä½å­¦ä¹ ç‡ï¼ˆ0.03â†’0.02ï¼‰
                'max_depth': 3,                # é™ä½æ·±åº¦ï¼ˆ4â†’3ï¼‰
                'num_leaves': 12,              # å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ15â†’12ï¼‰
                'min_child_samples': 40,       # å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ30â†’40ï¼‰
                'subsample': 0.65,             # å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'colsample_bytree': 0.65,      # å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'reg_alpha': 0.2,              # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.1â†’0.2ï¼‰
                'reg_lambda': 0.2,             # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.1â†’0.2ï¼‰
                'min_split_gain': 0.15,        # å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.1â†’0.15ï¼‰
                'feature_fraction': 0.65,      # å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'bagging_fraction': 0.65,      # å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }
        elif horizon == 5:
            # ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰
            print("ä½¿ç”¨5å¤©æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦æ­£åˆ™åŒ–ï¼‰...")
            lgb_params = {
                'n_estimators': 50,           # ä¿æŒ50
                'learning_rate': 0.03,         # ä¿æŒ0.03
                'max_depth': 4,                # ä¿æŒ4
                'num_leaves': 15,              # ä¿æŒ15
                'min_child_samples': 30,       # ä¿æŒ30
                'subsample': 0.7,              # ä¿æŒ0.7
                'colsample_bytree': 0.7,       # ä¿æŒ0.7
                'reg_alpha': 0.1,              # ä¿æŒ0.1
                'reg_lambda': 0.1,             # ä¿æŒ0.1
                'min_split_gain': 0.1,         # ä¿æŒ0.1
                'feature_fraction': 0.7,       # ä¿æŒ0.7
                'bagging_fraction': 0.7,       # ä¿æŒ0.7
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }
        else:  # horizon == 20
            # ä¸€ä¸ªæœˆæ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ– - 2026-02-16ä¼˜åŒ–ï¼‰
            # åŸå› ï¼šç‰¹å¾æ•°é‡ä»2530å¢è‡³2936ï¼ˆ+16%ï¼‰ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
            # ä¼˜åŒ–ç›®æ ‡ï¼šå°†è®­ç»ƒ/éªŒè¯å·®è·ä»Â±7.07%é™è‡³<5%
            print("ä½¿ç”¨20å¤©æ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½è¿‡æ‹Ÿåˆï¼‰...")
            lgb_params = {
                'n_estimators': 40,           # è¿›ä¸€æ­¥å‡å°‘æ ‘æ•°é‡ï¼ˆ45â†’40ï¼‰
                'learning_rate': 0.02,         # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼ˆ0.025â†’0.02ï¼‰
                'max_depth': 3,                # é™ä½æ·±åº¦ï¼ˆ4â†’3ï¼‰å‡å°‘è¿‡æ‹Ÿåˆ
                'num_leaves': 11,              # è¿›ä¸€æ­¥å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ13â†’11ï¼‰
                'min_child_samples': 40,       # è¿›ä¸€æ­¥å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ35â†’40ï¼‰
                'subsample': 0.6,              # è¿›ä¸€æ­¥å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'colsample_bytree': 0.6,       # è¿›ä¸€æ­¥å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'reg_alpha': 0.25,             # è¶…å¢å¼ºL1æ­£åˆ™ï¼ˆ0.18â†’0.25ï¼‰
                'reg_lambda': 0.25,            # è¶…å¢å¼ºL2æ­£åˆ™ï¼ˆ0.18â†’0.25ï¼‰
                'min_split_gain': 0.15,        # è¿›ä¸€æ­¥å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.12â†’0.15ï¼‰
                'feature_fraction': 0.6,       # è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'bagging_fraction': 0.6,       # è¿›ä¸€æ­¥å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
                'bagging_freq': 5,
                'random_state': 42,
                'verbose': -1
            }

        # è®­ç»ƒæ¨¡å‹ï¼ˆå¢åŠ æ­£åˆ™åŒ–ä»¥å‡å°‘è¿‡æ‹Ÿåˆï¼‰
        print("è®­ç»ƒLightGBMæ¨¡å‹...")
        self.model = lgb.LGBMClassifier(**lgb_params)

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        scores = []
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            # æ·»åŠ early_stoppingä»¥å‡å°‘è¿‡æ‹Ÿåˆ
            self.model.fit(
                X_train, y_train,
                eval_set=[(X_val, y_val)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(stopping_rounds=15, verbose=False)  # å¢åŠ patienceï¼ˆ10â†’15ï¼‰
                ]
            )
            y_pred = self.model.predict(X_val)
            score = accuracy_score(y_val, y_pred)
            scores.append(score)
            print(f"éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.model.fit(X, y)

        mean_accuracy = np.mean(scores)
        std_accuracy = np.std(scores)
        print(f"\nå¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})")

        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶ï¼ˆä¾›ç»¼åˆåˆ†æä½¿ç”¨ï¼‰
        accuracy_info = {
            'model_type': 'lgbm',
            'horizon': horizon,
            'accuracy': float(mean_accuracy),
            'std': float(std_accuracy),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        import json
        accuracy_file = 'data/model_accuracy.json'
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
            
            # æ›´æ–°å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
            key = f'lgbm_{horizon}d'
            existing_data[key] = accuracy_info
            
            # ä¿å­˜å›æ–‡ä»¶
            with open(accuracy_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… å‡†ç¡®ç‡å·²ä¿å­˜åˆ° {accuracy_file}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‡†ç¡®ç‡å¤±è´¥: {e}")

        # ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨ BaseModelProcessor ç»Ÿä¸€æ ¼å¼ï¼‰
        feat_imp = self.processor.analyze_feature_importance(
            self.model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            contrib_values = self.model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(10))

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    print(f"âš ï¸ è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)

            # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

            # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

            # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)

            # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
            stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
            for key, value in stock_type_features.items():
                stock_df[key] = value

            # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
            for key, value in sentiment_features.items():
                stock_df[key] = value

            # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
            topic_features = self.feature_engineer.create_topic_features(code, stock_df)
            for key, value in topic_features.items():
                stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

            # æ·»åŠ æ¿å—ç‰¹å¾
            sector_features = self.feature_engineer.create_sector_features(code, stock_df)
            for key, value in sector_features.items():
                stock_df[key] = value

            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

            # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_interaction_features(stock_df)

            # è·å–æœ€æ–°æ•°æ®ï¼ˆæˆ–æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼‰
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨ï¼‰
            for col, encoder in self.categorical_encoders.items():
                if col in latest_data.columns:
                    # å¦‚æœé‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œæ˜ å°„åˆ°0
                    try:
                        latest_data[col] = encoder.transform(latest_data[col].astype(str))
                    except ValueError:
                        # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
                        print(f"âš ï¸ è­¦å‘Š: åˆ†ç±»ç‰¹å¾ {col} åŒ…å«è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        latest_data[col] = 0

            X = latest_data[self.feature_columns].values

            # é¢„æµ‹
            proba = self.model.predict_proba(X)[0]
            prediction = self.model.predict(X)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns,
            'categorical_encoders': self.categorical_encoders
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_columns = model_data['feature_columns']
        self.categorical_encoders = model_data.get('categorical_encoders', {})
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class GBDTModel:
    """GBDT æ¨¡å‹ - åŸºäºæ¢¯åº¦æå‡å†³ç­–æ ‘çš„å•ä¸€æ¨¡å‹"""

    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.processor = BaseModelProcessor()
        self.gbdt_model = None
        self.feature_columns = []
        self.actual_n_estimators = 0
        self.horizon = 1  # é»˜è®¤é¢„æµ‹å‘¨æœŸ
        self.model_type = 'gbdt'  # æ¨¡å‹ç±»å‹æ ‡è¯†

    def load_selected_features(self, filepath=None, current_feature_names=None):
        """åŠ è½½é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä½¿ç”¨ç‰¹å¾åç§°äº¤é›†ï¼Œç¡®ä¿ç‰¹å¾å­˜åœ¨ï¼‰

        Args:
            filepath: ç‰¹å¾åç§°æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°çš„ï¼‰
            current_feature_names: å½“å‰æ•°æ®é›†çš„ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            list: ç‰¹å¾åç§°åˆ—è¡¨ï¼ˆå¦‚æœæ‰¾åˆ°ï¼‰ï¼Œå¦åˆ™è¿”å›None
        """
        import os
        import glob

        if filepath is None:
            # æŸ¥æ‰¾æœ€æ–°çš„ç‰¹å¾åç§°æ–‡ä»¶
            pattern = 'output/selected_features_*.csv'
            files = glob.glob(pattern)
            if not files:
                return None
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
            filepath = max(files, key=os.path.getmtime)

        try:
            import pandas as pd
            # è¯»å–ç‰¹å¾åç§°
            df = pd.read_csv(filepath)
            selected_names = df['Feature_Name'].tolist()

            print(f"ğŸ“‚ åŠ è½½ç‰¹å¾åˆ—è¡¨æ–‡ä»¶: {filepath}")
            print(f"âœ… åŠ è½½äº† {len(selected_names)} ä¸ªé€‰æ‹©çš„ç‰¹å¾")

            # å¦‚æœæä¾›äº†å½“å‰ç‰¹å¾åç§°ï¼Œä½¿ç”¨äº¤é›†
            if current_feature_names is not None:
                current_set = set(current_feature_names)
                selected_set = set(selected_names)
                available_set = current_set & selected_set
                
                available_names = list(available_set)
                print(f"ğŸ“Š å½“å‰æ•°æ®é›†ç‰¹å¾æ•°é‡: {len(current_feature_names)}")
                print(f"ğŸ“Š é€‰æ‹©çš„ç‰¹å¾æ•°é‡: {len(selected_names)}")
                print(f"ğŸ“Š å®é™…å¯ç”¨çš„ç‰¹å¾æ•°é‡: {len(available_names)}")
                print(f"âš ï¸  {len(selected_set) - len(available_set)} ä¸ªç‰¹å¾åœ¨å½“å‰æ•°æ®é›†ä¸­ä¸å­˜åœ¨")
                
                return available_names
            else:
                return selected_names

        except Exception as e:
            print(f"âš ï¸ åŠ è½½ç‰¹å¾åˆ—è¡¨å¤±è´¥: {e}")
            return None

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1):
        """å‡†å¤‡è®­ç»ƒæ•°æ®ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
        """
        self.horizon = horizon
        all_data = []

        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        print("ğŸ“Š è·å–ç¾è‚¡å¸‚åœºæ•°æ®...")
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            print(f"âœ… æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            print("âš ï¸ æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        for code in codes:
            try:
                print(f"å¤„ç†è‚¡ç¥¨: {code}")

                # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€ï¼Œè…¾è®¯è´¢ç»æ¥å£ä¸éœ€è¦
                stock_code = code.replace('.HK', '')

                # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
                if stock_df is None or stock_df.empty:
                    continue

                # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                hsi_df = get_hsi_data_tencent(period_days=730)
                if hsi_df is None or hsi_df.empty:
                    continue

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

                # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                
                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon, for_backtest=for_backtest)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
                stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
                for key, value in stock_type_features.items():
                    stock_df[key] = value

                # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
                sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
                for key, value in sentiment_features.items():
                    stock_df[key] = value

                # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
                topic_features = self.feature_engineer.create_topic_features(code, stock_df)
                for key, value in topic_features.items():
                    stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

                # æ·»åŠ æ¿å—ç‰¹å¾
                sector_features = self.feature_engineer.create_sector_features(code, stock_df)
                for key, value in sector_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼ˆä¿ç•™æ—¥æœŸç´¢å¼•ï¼Œä¸é‡ç½®ç´¢å¼•ï¼‰
        df = pd.concat(all_data, ignore_index=False)

        # æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼Œç¡®ä¿æ—¶é—´é¡ºåºæ­£ç¡®
        df = df.sort_index()

        # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆå…ˆæ‰§è¡Œï¼Œå› ä¸ºè¿™æ˜¯é«˜ä»·å€¼ç‰¹å¾ï¼‰
        print("\nğŸ”— ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾...")
        df = self.feature_engineer.create_technical_fundamental_interactions(df)

        # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆç±»åˆ«å‹ Ã— æ•°å€¼å‹ï¼‰
        print("\nğŸ”— ç”Ÿæˆäº¤å‰ç‰¹å¾...")
        df = self.feature_engineer.create_interaction_features(df)

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—ï¼ˆåŒ…æ‹¬ä¸­é—´è®¡ç®—åˆ—ï¼‰
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle',
                          'Low_Min', 'High_Max', '+DM', '-DM', '+DI', '-DI',
                          'TP', 'MF_Multiplier', 'MF_Volume']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1, use_feature_selection=False):
        """è®­ç»ƒ GBDT æ¨¡å‹

        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            use_feature_selection: æ˜¯å¦ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆé»˜è®¤Falseï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾ï¼‰
        """
        print("="*70)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ GBDT æ¨¡å‹")
        print("="*70)

        # å‡†å¤‡æ•°æ®
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # å…ˆåˆ é™¤å…¨ä¸ºNaNçš„åˆ—ï¼ˆé¿å…dropnaåˆ é™¤æ‰€æœ‰è¡Œï¼‰
        cols_all_nan = df.columns[df.isnull().all()].tolist()
        if cols_all_nan:
            print(f"ğŸ—‘ï¸  åˆ é™¤ {len(cols_all_nan)} ä¸ªå…¨ä¸ºNaNçš„åˆ—")
            df = df.drop(columns=cols_all_nan)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸç´¢å¼•æ’åºï¼ˆdropna å¯èƒ½ä¼šæ”¹å˜é¡ºåºï¼‰
        df = df.sort_index()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        print(f"âœ… ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆå¯é€‰ï¼‰
        if use_feature_selection:
            print("\nğŸ¯ åº”ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆGBDTï¼‰...")
            selected_features = self.load_selected_features(current_feature_names=self.feature_columns)
            if selected_features:
                # ç­›é€‰ç‰¹å¾åˆ—
                self.feature_columns = [col for col in self.feature_columns if col in selected_features]
                print(f"âœ… ç‰¹å¾é€‰æ‹©åº”ç”¨å®Œæˆï¼šä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°ç‰¹å¾é€‰æ‹©æ–‡ä»¶ï¼Œä½¿ç”¨å…¨éƒ¨ç‰¹å¾")
        else:
            print(f"âœ… ä½¿ç”¨å…¨éƒ¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°ç¼–ç ï¼‰
        categorical_features = []
        self.categorical_encoders = {}  # å­˜å‚¨ç¼–ç å™¨ï¼Œç”¨äºé¢„æµ‹æ—¶è§£ç 

        for col in self.feature_columns:
            if df[col].dtype == 'object' or df[col].dtype.name == 'category':
                print(f"  ç¼–ç åˆ†ç±»ç‰¹å¾: {col}")
                categorical_features.append(col)
                # ä½¿ç”¨LabelEncoderè¿›è¡Œç¼–ç 
                from sklearn.preprocessing import LabelEncoder
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.categorical_encoders[col] = le

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('output', exist_ok=True)

        # ========== è®­ç»ƒ GBDT æ¨¡å‹ ==========
        print("\n" + "="*70)
        print("ğŸŒ² è®­ç»ƒ GBDT æ¨¡å‹")
        print("="*70)

        # æ ¹æ®é¢„æµ‹å‘¨æœŸè°ƒæ•´å¶å­èŠ‚ç‚¹æ•°é‡å’Œæ—©åœè€å¿ƒ
        # æ¬¡æ—¥æ¨¡å‹ï¼šé€‚åº¦å‚æ•°
        # ä¸€å‘¨æ¨¡å‹ï¼šå‡å°‘å¶å­èŠ‚ç‚¹æ•°é‡ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ—©åœè€å¿ƒ
        # ä¸€ä¸ªæœˆæ¨¡å‹ï¼šå¢å¼ºæ­£åˆ™åŒ–ï¼ˆç‰¹å¾æ•°é‡å¢åŠ ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–ï¼‰
        if horizon == 5:
            # ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼‰
            print("ä½¿ç”¨ä¸€å‘¨æ¨¡å‹å‚æ•°ï¼ˆå‡å°‘å¶å­èŠ‚ç‚¹ï¼Œå¢åŠ æ—©åœè€å¿ƒï¼‰...")
            n_estimators = 32
            num_leaves = 24  # å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ32â†’24ï¼‰
            stopping_rounds = 15  # å¢åŠ æ—©åœè€å¿ƒï¼ˆ10â†’15ï¼‰
            min_child_samples = 30  # å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ20â†’30ï¼‰
            reg_alpha = 0.1     # ä¿æŒ0.1
            reg_lambda = 0.1    # ä¿æŒ0.1
            subsample = 0.7     # ä¿æŒ0.7
            colsample_bytree = 0.6  # ä¿æŒ0.6
        elif horizon == 1:
            # æ¬¡æ—¥æ¨¡å‹å‚æ•°ï¼ˆé€‚åº¦ï¼‰
            print("ä½¿ç”¨æ¬¡æ—¥æ¨¡å‹å‚æ•°...")
            n_estimators = 32
            num_leaves = 28  # é€‚åº¦å‡å°‘ï¼ˆ32â†’28ï¼‰
            stopping_rounds = 12  # é€‚åº¦å¢åŠ 
            min_child_samples = 25
            reg_alpha = 0.15    # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.1â†’0.15ï¼‰
            reg_lambda = 0.15   # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.1â†’0.15ï¼‰
            subsample = 0.65    # å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.7â†’0.65ï¼‰
            colsample_bytree = 0.65  # å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.6â†’0.65ï¼‰
        else:  # horizon == 20
            # ä¸€ä¸ªæœˆæ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ– - 2026-02-16ä¼˜åŒ–ï¼‰
            # åŸå› ï¼šç‰¹å¾æ•°é‡ä»2530å¢è‡³2936ï¼ˆ+16%ï¼‰ï¼Œéœ€è¦æ›´å¼ºçš„æ­£åˆ™åŒ–é˜²æ­¢è¿‡æ‹Ÿåˆ
            # ä¼˜åŒ–ç›®æ ‡ï¼šå°†è®­ç»ƒ/éªŒè¯å·®è·ä»Â±7.07%é™è‡³<5%
            print("ä½¿ç”¨20å¤©æ¨¡å‹å‚æ•°ï¼ˆè¶…å¢å¼ºæ­£åˆ™åŒ–ï¼Œé™ä½è¿‡æ‹Ÿåˆï¼‰...")
            n_estimators = 28           # è¿›ä¸€æ­¥å‡å°‘æ ‘æ•°é‡ï¼ˆ32â†’28ï¼‰
            num_leaves = 20              # è¿›ä¸€æ­¥å‡å°‘å¶å­èŠ‚ç‚¹ï¼ˆ24â†’20ï¼‰
            stopping_rounds = 18         # è¿›ä¸€æ­¥å¢åŠ æ—©åœè€å¿ƒï¼ˆ12â†’18ï¼‰
            min_child_samples = 35       # è¿›ä¸€æ­¥å¢åŠ æœ€å°æ ·æœ¬ï¼ˆ30â†’35ï¼‰
            reg_alpha = 0.22             # å¢å¼ºL1æ­£åˆ™ï¼ˆ0.15â†’0.22ï¼‰
            reg_lambda = 0.22            # å¢å¼ºL2æ­£åˆ™ï¼ˆ0.15â†’0.22ï¼‰
            subsample = 0.6              # è¿›ä¸€æ­¥å‡å°‘è¡Œé‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰
            colsample_bytree = 0.6       # è¿›ä¸€æ­¥å‡å°‘åˆ—é‡‡æ ·ï¼ˆ0.65â†’0.6ï¼‰

        self.gbdt_model = lgb.LGBMClassifier(
            objective='binary',
            boosting_type='gbdt',
            subsample=subsample,            # æ ¹æ®å‘¨æœŸè°ƒæ•´
            min_child_weight=0.1,
            min_child_samples=min_child_samples,  # æ ¹æ®å‘¨æœŸè°ƒæ•´
            colsample_bytree=colsample_bytree,  # æ ¹æ®å‘¨æœŸè°ƒæ•´
            num_leaves=num_leaves,      # æ ¹æ®å‘¨æœŸè°ƒæ•´
            learning_rate=0.025,        # è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡ï¼ˆ0.03â†’0.025ï¼‰
            n_estimators=n_estimators,
            reg_alpha=reg_alpha,        # æ ¹æ®å‘¨æœŸè°ƒæ•´L1æ­£åˆ™
            reg_lambda=reg_lambda,       # æ ¹æ®å‘¨æœŸè°ƒæ•´L2æ­£åˆ™
            min_split_gain=0.12,        # è¿›ä¸€æ­¥å¢åŠ åˆ†å‰²å¢ç›Šï¼ˆ0.1â†’0.12ï¼‰
            feature_fraction=0.6,       # è¿›ä¸€æ­¥å‡å°‘ç‰¹å¾é‡‡æ ·ï¼ˆ0.7â†’0.6ï¼‰
            bagging_fraction=0.6,       # è¿›ä¸€æ­¥å‡å°‘Baggingé‡‡æ ·ï¼ˆ0.7â†’0.6ï¼‰
            bagging_freq=5,             # Baggingé¢‘ç‡ï¼ˆæ–°å¢ï¼‰
            random_state=2020,
            n_jobs=-1,
            verbose=-1
        )

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        gbdt_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            self.gbdt_model.fit(
                X_train_fold, y_train_fold,
                eval_set=[(X_val_fold, y_val_fold)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=False)  # æ ¹æ®å‘¨æœŸè°ƒæ•´æ—©åœè€å¿ƒ
                ]
            )

            y_pred_fold = self.gbdt_model.predict(X_val_fold)
            score = accuracy_score(y_val_fold, y_pred_fold)
            gbdt_scores.append(score)
            print(f"   Fold {fold} éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.gbdt_model.fit(X, y)

        # è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
        # æ³¨æ„ï¼šåœ¨ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæ—¶ï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨æ—©åœï¼Œbest_iteration_ å¯èƒ½ä¸º None
        # è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ n_estimators
        self.actual_n_estimators = self.gbdt_model.best_iteration_ if self.gbdt_model.best_iteration_ else n_estimators
        mean_accuracy = np.mean(gbdt_scores)
        std_accuracy = np.std(gbdt_scores)
        print(f"\nâœ… GBDT è®­ç»ƒå®Œæˆ")
        print(f"   å®é™…è®­ç»ƒæ ‘æ•°é‡: {self.actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")
        print(f"   å¹³å‡éªŒè¯å‡†ç¡®ç‡: {mean_accuracy:.4f} (+/- {std_accuracy:.4f})")

        # ä¿å­˜å‡†ç¡®ç‡åˆ°æ–‡ä»¶ï¼ˆä¾›ç»¼åˆåˆ†æä½¿ç”¨ï¼‰
        accuracy_info = {
            'model_type': 'gbdt',
            'horizon': horizon,
            'accuracy': float(mean_accuracy),
            'std': float(std_accuracy),
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        import json
        accuracy_file = 'data/model_accuracy.json'
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(accuracy_file):
                with open(accuracy_file, 'r', encoding='utf-8') as f:
                    existing_data = json.load(f)
            else:
                existing_data = {}
            
            # æ›´æ–°å½“å‰æ¨¡å‹çš„å‡†ç¡®ç‡
            key = f'gbdt_{horizon}d'
            existing_data[key] = accuracy_info
            
            # ä¿å­˜å›æ–‡ä»¶
            with open(accuracy_file, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            print(f"âœ… å‡†ç¡®ç‡å·²ä¿å­˜åˆ° {accuracy_file}")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜å‡†ç¡®ç‡å¤±è´¥: {e}")

        # ========== Step 2: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ ==========
        print("\n" + "="*70)
        print("ğŸ“Š Step 2: åˆ†æ GBDT ç‰¹å¾é‡è¦æ€§")
        print("="*70)

        feat_imp = self.processor.analyze_feature_importance(
            self.gbdt_model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘
        try:
            contrib_values = self.gbdt_model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )

            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False)
            print("âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§è‡³ output/gbdt_feature_importance.csv")

            # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾
            print("\nğŸ“Š GBDT Top 20 é‡è¦ç‰¹å¾ (å«å½±å“æ–¹å‘):")
            print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(20))

        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        print("\n" + "="*70)
        print("âœ… GBDT æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*70)

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨ï¼ˆ80ä¸ªæŒ‡æ ‡ç‰ˆæœ¬ï¼‰

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    print(f"âš ï¸ è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ï¼ˆ80ä¸ªæŒ‡æ ‡ï¼‰
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)

            # è®¡ç®—å¤šå‘¨æœŸæŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_multi_period_metrics(stock_df)

            # è®¡ç®—ç›¸å¯¹å¼ºåº¦æŒ‡æ ‡
            stock_df = self.feature_engineer.calculate_relative_strength(stock_df, hsi_df)

            # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)

            # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # æ·»åŠ è‚¡ç¥¨ç±»å‹ç‰¹å¾
            stock_type_features = self.feature_engineer.create_stock_type_features(code, stock_df)
            for key, value in stock_type_features.items():
                stock_df[key] = value

            # æ·»åŠ æƒ…æ„Ÿç‰¹å¾
            sentiment_features = self.feature_engineer.create_sentiment_features(code, stock_df)
            for key, value in sentiment_features.items():
                stock_df[key] = value

            # æ·»åŠ ä¸»é¢˜ç‰¹å¾ï¼ˆLDAä¸»é¢˜å»ºæ¨¡ï¼‰
            topic_features = self.feature_engineer.create_topic_features(code, stock_df)
            for key, value in topic_features.items():
                stock_df[key] = value
                # æ·»åŠ ä¸»é¢˜æƒ…æ„Ÿäº¤äº’ç‰¹å¾
                topic_sentiment_interaction = self.feature_engineer.create_topic_sentiment_interaction_features(code, stock_df)
                for key, value in topic_sentiment_interaction.items():
                    stock_df[key] = value
                # æ·»åŠ é¢„æœŸå·®è·ç‰¹å¾
                expectation_gap = self.feature_engineer.create_expectation_gap_features(code, stock_df)
                for key, value in expectation_gap.items():
                    stock_df[key] = value

            # æ·»åŠ æ¿å—ç‰¹å¾
            sector_features = self.feature_engineer.create_sector_features(code, stock_df)
            for key, value in sector_features.items():
                stock_df[key] = value

            # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ä¸åŸºæœ¬é¢äº¤äº’ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_technical_fundamental_interactions(stock_df)

            # ç”Ÿæˆäº¤å‰ç‰¹å¾ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
            stock_df = self.feature_engineer.create_interaction_features(stock_df)

            # è·å–æœ€æ–°æ•°æ®
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            # å¤„ç†åˆ†ç±»ç‰¹å¾ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„ç¼–ç å™¨ï¼‰
            for col, encoder in self.categorical_encoders.items():
                if col in latest_data.columns:
                    # å¦‚æœé‡åˆ°è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œæ˜ å°„åˆ°0
                    try:
                        latest_data[col] = encoder.transform(latest_data[col].astype(str))
                    except ValueError:
                        # å¤„ç†æœªè§è¿‡çš„ç±»åˆ«
                        print(f"âš ï¸ è­¦å‘Š: åˆ†ç±»ç‰¹å¾ {col} åŒ…å«è®­ç»ƒæ—¶æœªè§è¿‡çš„ç±»åˆ«ï¼Œä½¿ç”¨é»˜è®¤å€¼")
                        latest_data[col] = 0

            X = latest_data[self.feature_columns].values

            # ä½¿ç”¨GBDTæ¨¡å‹ç›´æ¥é¢„æµ‹
            proba = self.gbdt_model.predict_proba(X)[0]
            prediction = self.gbdt_model.predict(X)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'gbdt_model': self.gbdt_model,
            'feature_columns': self.feature_columns,
            'actual_n_estimators': self.actual_n_estimators,
            'categorical_encoders': self.categorical_encoders
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"GBDT æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.gbdt_model = model_data['gbdt_model']
        self.feature_columns = model_data['feature_columns']
        self.actual_n_estimators = model_data['actual_n_estimators']
        self.categorical_encoders = model_data.get('categorical_encoders', {})
        print(f"GBDT æ¨¡å‹å·²ä» {filepath} åŠ è½½")


def main():
    parser = argparse.ArgumentParser(description='æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'predict', 'evaluate', 'backtest'],
                       help='è¿è¡Œæ¨¡å¼: train=è®­ç»ƒ, predict=é¢„æµ‹, evaluate=è¯„ä¼°, backtest=å›æµ‹')
    parser.add_argument('--model-type', type=str, default='lgbm', choices=['lgbm', 'gbdt'],
                       help='æ¨¡å‹ç±»å‹: lgbm=å•ä¸€LightGBMæ¨¡å‹, gbdt=å•ä¸€GBDTæ¨¡å‹ï¼ˆé»˜è®¤lgbmï¼‰')
    parser.add_argument('--model-path', type=str, default='data/ml_trading_model.pkl',
                       help='æ¨¡å‹ä¿å­˜/åŠ è½½è·¯å¾„')
    parser.add_argument('--start-date', type=str, default=None,
                       help='è®­ç»ƒå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='è®­ç»ƒç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--predict-date', type=str, default=None,
                       help='é¢„æµ‹æ—¥æœŸï¼šåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ (YYYY-MM-DD)ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥')
    parser.add_argument('--horizon', type=int, default=1, choices=[1, 5, 20],
                       help='é¢„æµ‹å‘¨æœŸ: 1=æ¬¡æ—¥ï¼ˆé»˜è®¤ï¼‰, 5=ä¸€å‘¨, 20=ä¸€ä¸ªæœˆ')
    parser.add_argument('--use-feature-selection', action='store_true',
                       help='ä½¿ç”¨ç‰¹å¾é€‰æ‹©ï¼ˆåªä½¿ç”¨500ä¸ªé€‰æ‹©çš„ç‰¹å¾ï¼Œè€Œä¸æ˜¯å…¨éƒ¨2936ä¸ªï¼‰')

    args = parser.parse_args()

    # åˆå§‹åŒ–æ¨¡å‹
    if args.model_type == 'gbdt':
        print("=" * 70)
        print("ğŸš€ ä½¿ç”¨å•ä¸€ GBDT æ¨¡å‹")
        print("=" * 70)
        lgbm_model = None
        gbdt_model = GBDTModel()
    else:
        print("=" * 70)
        print("ğŸš€ ä½¿ç”¨å•ä¸€ LightGBM æ¨¡å‹")
        print("=" * 70)
        lgbm_model = MLTradingModel()
        gbdt_model = None

    if args.mode == 'train':
        print("=" * 50)
        print("è®­ç»ƒæ¨¡å¼")
        print("=" * 50)

        # è®­ç»ƒæ¨¡å‹
        horizon_suffix = f'_{args.horizon}d'
        if lgbm_model:
            feature_importance = lgbm_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=args.use_feature_selection)
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.save_model(lgbm_model_path)
            importance_path = lgbm_model_path.replace('.pkl', '_importance.csv')
            feature_importance.to_csv(importance_path, index=False)
            print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")
        else:
            feature_importance = gbdt_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon, use_feature_selection=args.use_feature_selection)
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            gbdt_model.save_model(gbdt_model_path)
            importance_path = gbdt_model_path.replace('.pkl', '_importance.csv')
            feature_importance.to_csv(importance_path, index=False)
            print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")

    elif args.mode == 'predict':
        print("=" * 50)
        print("é¢„æµ‹æ¨¡å¼")
        print("=" * 50)

        # åŠ è½½æ¨¡å‹
        horizon_suffix = f'_{args.horizon}d'
        if lgbm_model:
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.load_model(lgbm_model_path)
            model = lgbm_model
            model_name = "LightGBM"
            model_file_suffix = "lgbm"
        else:
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            gbdt_model.load_model(gbdt_model_path)
            model = gbdt_model
            model_name = "GBDT"
            model_file_suffix = "gbdt"

        print(f"å·²åŠ è½½ {model_name} æ¨¡å‹")

        # é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
        predictions = []
        if args.predict_date:
            print(f"åŸºäºæ—¥æœŸ: {args.predict_date}")
        for code in WATCHLIST:
            result = model.predict(code, predict_date=args.predict_date)
            if result:
                predictions.append(result)

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        print("\né¢„æµ‹ç»“æœ:")
        horizon_text = {1: "æ¬¡æ—¥", 5: "ä¸€å‘¨", 20: "ä¸€ä¸ªæœˆ"}.get(args.horizon, f"{args.horizon}å¤©")
        if args.predict_date:
            print(f"è¯´æ˜: åŸºäº {args.predict_date} çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
        else:
            print(f"è¯´æ˜: åŸºäºæœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
        print("-" * 100)
        print(f"{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹':<8} {'æ¦‚ç‡':<10} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡':<15}")
        print("-" * 100)

        for pred in predictions:
            pred_label = "ä¸Šæ¶¨" if pred['prediction'] == 1 else "ä¸‹è·Œ"
            data_date = pred['date'].strftime('%Y-%m-%d')
            target_date = get_target_date(pred['date'], horizon=args.horizon)

            print(f"{pred['code']:<10} {pred['name']:<12} {pred_label:<8} {pred['probability']:.4f}    {pred['current_price']:.2f}        {data_date:<15} {target_date:<15}")

        # ä¿å­˜é¢„æµ‹ç»“æœ
        pred_df = pd.DataFrame(predictions)
        pred_df['data_date'] = pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
        pred_df['target_date'] = pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))

        pred_df_export = pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']]

        pred_path = args.model_path.replace('.pkl', f'_{model_file_suffix}_predictions{horizon_suffix}.csv')
        pred_df_export.to_csv(pred_path, index=False)
        print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {pred_path}")

        # ä¿å­˜20å¤©é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ï¼ˆä¾¿äºåç»­æå–å’Œå¯¹æ¯”ï¼‰
        if args.horizon == 20:
            save_predictions_to_text(pred_df_export, args.predict_date)
            horizon_suffix = f'_{args.horizon}d'
            if lgbm_model:
                model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            else:
                model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            model.load_model(model_path)

            # é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
            predictions = []
            if args.predict_date:
                print(f"åŸºäºæ—¥æœŸ: {args.predict_date}")
            for code in WATCHLIST:
                result = model.predict(code, predict_date=args.predict_date)
                if result:
                    predictions.append(result)

            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            print("\né¢„æµ‹ç»“æœ:")
            horizon_text = {1: "æ¬¡æ—¥", 5: "ä¸€å‘¨", 20: "ä¸€ä¸ªæœˆ"}.get(args.horizon, f"{args.horizon}å¤©")
            if args.predict_date:
                print(f"è¯´æ˜: åŸºäº {args.predict_date} çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
            else:
                print(f"è¯´æ˜: åŸºäºæœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
            print("-" * 100)
            print(f"{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹':<8} {'æ¦‚ç‡':<10} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡':<15}")
            print("-" * 100)

            for pred in predictions:
                pred_label = "ä¸Šæ¶¨" if pred['prediction'] == 1 else "ä¸‹è·Œ"
                data_date = pred['date'].strftime('%Y-%m-%d')
                target_date = get_target_date(pred['date'], horizon=args.horizon)

                print(f"{pred['code']:<10} {pred['name']:<12} {pred_label:<8} {pred['probability']:.4f}    {pred['current_price']:.2f}        {data_date:<15} {target_date:<15}")

            # ä¿å­˜é¢„æµ‹ç»“æœ
            pred_df = pd.DataFrame(predictions)
            pred_df['data_date'] = pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
            pred_df['target_date'] = pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))
            
            pred_df_export = pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']]
            
            horizon_suffix = f'_{args.horizon}d'
            pred_path = args.model_path.replace('.pkl', f'_predictions{horizon_suffix}.csv')
            pred_df_export.to_csv(pred_path, index=False)
            print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {pred_path}")

            # ä¿å­˜20å¤©é¢„æµ‹ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ï¼ˆä¾¿äºåç»­æå–å’Œå¯¹æ¯”ï¼‰
            if args.horizon == 20:
                save_predictions_to_text(pred_df_export, args.predict_date)

    elif args.mode == 'evaluate':
        print("=" * 50)
        print("è¯„ä¼°æ¨¡å¼")
        print("=" * 50)

        if args.model_type == 'both':
            # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
            print("\nåŠ è½½æ¨¡å‹...")
            horizon_suffix = f'_{args.horizon}d'
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            gbdt_model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
            
            lgbm_model.load_model(lgbm_model_path)
            gbdt_model.load_model(gbdt_model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = lgbm_model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[lgbm_model.feature_columns].values
            y_test = test_df['Label'].values

            # LGBM æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ³ LightGBM æ¨¡å‹è¯„ä¼°")
            print("="*70)
            y_pred_lgbm = lgbm_model.model.predict(X_test)
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_lgbm))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_lgbm))
            lgbm_accuracy = accuracy_score(y_test, y_pred_lgbm)
            print(f"\nå‡†ç¡®ç‡: {lgbm_accuracy:.4f}")

            # GBDT æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ² GBDT æ¨¡å‹è¯„ä¼°")
            print("="*70)
            y_pred_gbdt = gbdt_model.gbdt_model.predict(X_test)

            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_gbdt))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_gbdt))
            gbdt_accuracy = accuracy_score(y_test, y_pred_gbdt)
            print(f"\nå‡†ç¡®ç‡: {gbdt_accuracy:.4f}")

            # å¯¹æ¯”ç»“æœ
            print("\n" + "="*70)
            print("ğŸ“Š æ¨¡å‹å¯¹æ¯”")
            print("="*70)
            print(f"LightGBM å‡†ç¡®ç‡: {lgbm_accuracy:.4f}")
            print(f"GBDT å‡†ç¡®ç‡: {gbdt_accuracy:.4f}")
            print(f"å‡†ç¡®ç‡å·®å¼‚: {abs(lgbm_accuracy - gbdt_accuracy):.4f}")
            
            if gbdt_accuracy > lgbm_accuracy:
                print(f"\nâœ… GBDT æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {gbdt_accuracy - lgbm_accuracy:.4f} ({(gbdt_accuracy - lgbm_accuracy)/lgbm_accuracy*100:.2f}%)")
            elif lgbm_accuracy > gbdt_accuracy:
                print(f"\nâœ… LightGBM æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {lgbm_accuracy - gbdt_accuracy:.4f} ({(lgbm_accuracy - gbdt_accuracy)/gbdt_accuracy*100:.2f}%)")
            else:
                print(f"\nâš–ï¸  ä¸¤ç§æ¨¡å‹è¡¨ç°ç›¸åŒ")

        else:
            # å•ä¸ªæ¨¡å‹è¯„ä¼°
            model = lgbm_model if lgbm_model else gbdt_model
            model.load_model(args.model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[model.feature_columns].values
            y_test = test_df['Label'].values

            # ä½¿ç”¨æ¨¡å‹ç›´æ¥é¢„æµ‹
            y_pred = model.gbdt_model.predict(X_test)

            # è¯„ä¼°
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred))

            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred))

            print(f"\nå‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")

    elif args.mode == 'backtest':
            # å›æµ‹æ¨¡å¼
            print("=" * 50)
            print("å›æµ‹æ¨¡å¼")
            print("=" * 50)
            
            from backtest_evaluator import BacktestEvaluator
            
            # åŠ è½½æ¨¡å‹
            print("\nåŠ è½½æ¨¡å‹...")
            horizon_suffix = f'_{args.horizon}d'
            
            if args.model_type == 'lgbm':
                model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
                lgbm_model.load_model(model_path)
                model = lgbm_model.model
            else:
                model_path = args.model_path.replace('.pkl', f'_gbdt{horizon_suffix}.pkl')
                gbdt_model.load_model(model_path)
                model = gbdt_model.gbdt_model
            
            # å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆç”¨äºå›æµ‹ï¼‰
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            # å›æµ‹ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ï¼Œä¸åº”ç”¨é¢„æµ‹å‘¨æœŸçš„æ ‡ç­¾è¿‡æ»¤
            test_df = lgbm_model.prepare_data(WATCHLIST, for_backtest=True)
            test_df = test_df.dropna()
            
            # æŒ‰æ—¶é—´æ’åº
            test_df = test_df.sort_index()
            
            # è·å–ç‰¹å¾å’Œæ ‡ç­¾
            X_test = test_df[lgbm_model.feature_columns].values
            y_test = test_df['Label'].values
            
            # è·å–ä»·æ ¼æ•°æ®ï¼ˆç”¨äºå›æµ‹ï¼‰
            prices = test_df['Close']
            
            print(f"æµ‹è¯•æ•°æ®: {len(test_df)} æ¡")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•æ•°æ®
            if len(test_df) == 0:
                print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æµ‹è¯•æ•°æ®ï¼Œæ— æ³•è¿›è¡Œå›æµ‹")
                print("è¯·ç¡®ä¿æ•°æ®å‡†å¤‡æ­£ç¡®ï¼Œå¹¶ä¸”æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®")
                return
            
            print(f"æµ‹è¯•æ—¶é—´æ®µ: {test_df.index[0]} åˆ° {test_df.index[-1]}")
            
            # è¿è¡Œå›æµ‹
            print("\nå¼€å§‹å›æµ‹...")
            evaluator = BacktestEvaluator(initial_capital=100000)
            results = evaluator.backtest_model(
                model=model,
                test_data=pd.DataFrame(X_test, index=test_df.index),
                test_labels=pd.Series(y_test, index=test_df.index),
                test_prices=prices,
                confidence_threshold=0.55
            )
            
            # ç»˜åˆ¶å›æµ‹ç»“æœ
            output_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'output')
            os.makedirs(output_dir, exist_ok=True)
            plot_path = os.path.join(output_dir, f'backtest_results_{args.horizon}d_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png')
            evaluator.plot_backtest_results(results, save_path=plot_path)
            
            # ä¿å­˜å›æµ‹ç»“æœ
            result_path = os.path.join(output_dir, f'backtest_results_{args.horizon}d_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
            import json
            with open(result_path, 'w') as f:
                # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹
                results_for_json = {
                    k: float(v) if isinstance(v, (np.float64, np.float32, np.int64, np.int32)) else v
                    for k, v in results.items()
                    if k not in ['portfolio_values', 'benchmark_values', 'trades']
                }
                json.dump(results_for_json, f, indent=2)
            print(f"\nğŸ“Š å›æµ‹ç»“æœå·²ä¿å­˜åˆ°: {result_path}")


if __name__ == '__main__':
    main()
