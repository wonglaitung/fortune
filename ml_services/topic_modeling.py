#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LDAä¸»é¢˜å»ºæ¨¡æ¨¡å—
ç”¨äºå¯¹æ–°é—»æ–‡æœ¬è¿›è¡Œä¸»é¢˜å»ºæ¨¡ï¼Œç”Ÿæˆä¸»é¢˜åˆ†å¸ƒç‰¹å¾
"""

import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import pickle
import warnings
warnings.filterwarnings('ignore')

# ä¸­æ–‡åˆ†è¯
try:
    import jieba
    import jieba.analyse
    from jieba import posseg
except ImportError:
    print("âš ï¸  è­¦å‘Šï¼šæœªå®‰è£…jiebaï¼Œä½¿ç”¨è‹±æ–‡åˆ†è¯")
    jieba = None

# LDAä¸»é¢˜å»ºæ¨¡
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class TopicModeler:
    """LDAä¸»é¢˜å»ºæ¨¡å™¨"""
    
    def __init__(self, n_topics=10, language='mixed'):
        """
        åˆå§‹åŒ–ä¸»é¢˜å»ºæ¨¡å™¨
        
        Args:
            n_topics: ä¸»é¢˜æ•°é‡
            language: è¯­è¨€ç±»å‹ ('chinese', 'english', 'mixed')
        """
        self.n_topics = n_topics
        self.language = language
        self.lda_model = None
        self.vectorizer = None
        self.feature_names = None
        self.topic_names = self._generate_topic_names()
        
        # åˆå§‹åŒ–NLTKèµ„æº
        self._init_nltk()
        
        # ä¸­æ–‡åœç”¨è¯
        self.chinese_stopwords = self._get_chinese_stopwords()
        
        # è‹±æ–‡åœç”¨è¯
        self.english_stopwords = set(stopwords.words('english'))
        
    def _generate_topic_names(self):
        """ç”Ÿæˆä¸»é¢˜åç§°ï¼ˆå ä½ç¬¦ï¼Œéœ€è¦äººå·¥æ ‡æ³¨ï¼‰"""
        return [f"Topic_{i+1}" for i in range(self.n_topics)]
    
    def _init_nltk(self):
        """åˆå§‹åŒ–NLTKèµ„æº"""
        try:
            nltk.download('punkt', quiet=True)
            nltk.download('stopwords', quiet=True)
            nltk.download('wordnet', quiet=True)
        except:
            pass
    
    def _get_chinese_stopwords(self):
        """è·å–ä¸­æ–‡åœç”¨è¯"""
        # å¸¸ç”¨ä¸­æ–‡åœç”¨è¯
        stopwords_list = [
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€',
            'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰',
            'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'é‚£', 'äº›', 'é‡Œ', 'ä¹ˆ', 'ä¹‹', 'ä¸º', 'è€Œ', 'åŠ',
            'ä¸', 'æˆ–', 'ç­‰', 'ä½†', 'å…¶', 'å¯¹', 'å°†', 'æŠŠ', 'è¢«', 'ç»™', 'è®©', 'ä½¿',
            'ç”±äº', 'å› æ­¤', 'ä½†æ˜¯', 'è€Œä¸”', 'å¹¶ä¸”', 'æˆ–è€…', 'å¦‚æœ', 'è™½ç„¶', 'ç„¶è€Œ',
            'è¿™ä¸ª', 'é‚£ä¸ª', 'è¿™ç§', 'é‚£ç§', 'è¿™äº›', 'é‚£äº›', 'è¿™é‡Œ', 'é‚£é‡Œ', 'æ€ä¹ˆ',
            'ä»€ä¹ˆ', 'å“ª', 'å¦‚ä½•', 'ä¸ºä½•', 'ä½•æ—¶', 'ä½•åœ°', 'å¤šå°‘', 'å‡ ', 'è°', 'è°çš„',
            'å¯ä»¥', 'èƒ½å¤Ÿ', 'å¯èƒ½', 'åº”è¯¥', 'éœ€è¦', 'å¿…é¡»', 'ä¸€å®š', 'è‚¯å®š', 'å½“ç„¶',
            'å·²ç»', 'æ­£åœ¨', 'å°†è¦', 'å·²ç»', 'æ›¾ç»', 'ä¸€ç›´', 'æ€»æ˜¯', 'å¸¸å¸¸', 'å¾€å¾€',
            'æ¯”è¾ƒ', 'æ›´åŠ ', 'æœ€', 'æ›´', 'éå¸¸', 'ç‰¹åˆ«', 'ååˆ†', 'ç›¸å½“', 'æŒº', 'è¾ƒ',
            'å…¬å¸', 'é›†å›¢', 'æœ‰é™å…¬å¸', 'è‚¡ä»½', 'æ§è‚¡', 'ç§‘æŠ€', 'å‘å±•', 'è¡¨ç¤º', 'ç§°',
            'å‘å¸ƒ', 'å®£å¸ƒ', 'è¡¨ç¤º', 'ç§°', 'æ˜¾ç¤º', 'è¡¨æ˜', 'æŒ‡å‡º', 'è®¤ä¸º', 'è§‰å¾—',
            'ä¸­å›½', 'é¦™æ¸¯', 'ç¾å›½', 'å¸‚åœº', 'è‚¡å¸‚', 'æŠ•èµ„', 'æŠ•èµ„è€…', 'åˆ†æå¸ˆ', 'æŠ¥å‘Š',
            'å­£åº¦', 'å¹´åº¦', 'å¹´', 'æœˆ', 'æ—¥', 'æ˜ŸæœŸ', 'å‘¨', 'ä»Šå¤©', 'æ˜¨å¤©', 'æ˜å¤©'
        ]
        return set(stopwords_list)
    
    def preprocess_text(self, text):
        """
        æ–‡æœ¬é¢„å¤„ç†
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            str: é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if pd.isna(text) or text is None:
            return ""
        
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        text = str(text)
        
        # è½¬æ¢ä¸ºå°å†™
        text = text.lower()
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ•°å­—ï¼Œä¿ç•™å­—æ¯ã€ä¸­æ–‡å’Œç©ºæ ¼
        text = re.sub(r'[^\w\s\u4e00-\u9fff]', ' ', text)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def tokenize_chinese(self, text):
        """
        ä¸­æ–‡åˆ†è¯
        
        Args:
            text: ä¸­æ–‡æ–‡æœ¬
            
        Returns:
            list: åˆ†è¯åˆ—è¡¨
        """
        if jieba is None:
            # å¦‚æœæ²¡æœ‰jiebaï¼Œä½¿ç”¨ç®€å•åˆ†è¯
            return [char for char in text if char.strip()]
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text)
        
        # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
        tokens = [
            word for word in words 
            if word not in self.chinese_stopwords 
            and len(word) > 1
            and word.strip()
        ]
        
        return tokens
    
    def tokenize_english(self, text):
        """
        è‹±æ–‡åˆ†è¯
        
        Args:
            text: è‹±æ–‡æ–‡æœ¬
            
        Returns:
            list: åˆ†è¯åˆ—è¡¨
        """
        try:
            # ä½¿ç”¨NLTKåˆ†è¯
            tokens = word_tokenize(text)
            
            # è¯å½¢è¿˜åŸ
            lemmatizer = WordNetLemmatizer()
            tokens = [lemmatizer.lemmatize(token) for token in tokens]
            
            # è¿‡æ»¤åœç”¨è¯
            tokens = [
                token for token in tokens 
                if token not in self.english_stopwords 
                and len(token) > 2
                and token.isalpha()
            ]
            
            return tokens
        except:
            # ç®€å•åˆ†è¯
            tokens = text.split()
            tokens = [
                token for token in tokens 
                if token not in self.english_stopwords 
                and len(token) > 2
                and token.isalpha()
            ]
            return tokens
    
    def tokenize_mixed(self, text):
        """
        ä¸­è‹±æ–‡æ··åˆåˆ†è¯
        
        Args:
            text: ä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
            
        Returns:
            list: åˆ†è¯åˆ—è¡¨
        """
        # åˆ†ç¦»ä¸­æ–‡å’Œè‹±æ–‡
        chinese_text = re.sub(r'[a-zA-Z\s]', '', text)
        english_text = re.sub(r'[\u4e00-\u9fff\s]', '', text)
        
        # åˆ†åˆ«åˆ†è¯
        chinese_tokens = self.tokenize_chinese(chinese_text)
        english_tokens = self.tokenize_english(english_text)
        
        # åˆå¹¶ç»“æœ
        return chinese_tokens + english_tokens
    
    def load_news_data(self, filepath='data/all_stock_news_records.csv', days=30):
        """
        åŠ è½½æ–°é—»æ•°æ®
        
        Args:
            filepath: æ–°é—»æ•°æ®æ–‡ä»¶è·¯å¾„
            days: æœ€è¿‘å¤šå°‘å¤©çš„æ–°é—»
            
        Returns:
            DataFrame: æ–°é—»æ•°æ®
        """
        try:
            # è¯»å–æ–°é—»æ•°æ®
            df = pd.read_csv(filepath, encoding='utf-8-sig')
            
            # è½¬æ¢æ—¥æœŸåˆ—
            df['æ–°é—»æ—¶é—´'] = pd.to_datetime(df['æ–°é—»æ—¶é—´'])
            df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
            
            # ç­›é€‰æœ€è¿‘Nå¤©çš„æ–°é—»
            cutoff_date = datetime.now() - timedelta(days=days)
            df = df[df['æ—¥æœŸ'] >= cutoff_date]
            
            # åˆå¹¶æ ‡é¢˜å’Œå†…å®¹
            df['æ–‡æœ¬'] = df['æ–°é—»æ ‡é¢˜'].astype(str) + ' ' + df['ç®€è¦å†…å®¹'].astype(str)
            
            # ç§»é™¤ç©ºæ–‡æœ¬
            df = df[df['æ–‡æœ¬'].str.strip() != '']
            
            print(f"âœ… åŠ è½½äº† {len(df)} æ¡æ–°é—»æ•°æ®ï¼ˆæœ€è¿‘{days}å¤©ï¼‰")
            
            return df
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ–°é—»æ•°æ®å¤±è´¥: {e}")
            return None
    
    def train_model(self, texts, max_features=1000, max_df=0.95, min_df=2):
        """
        è®­ç»ƒLDAæ¨¡å‹
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            max_features: æœ€å¤§ç‰¹å¾æ•°
            max_df: æ–‡æ¡£é¢‘ç‡ä¸Šé™
            min_df: æ–‡æ¡£é¢‘ç‡ä¸‹é™
            
        Returns:
            bool: æ˜¯å¦è®­ç»ƒæˆåŠŸ
        """
        try:
            print(f"ğŸš€ å¼€å§‹è®­ç»ƒLDAä¸»é¢˜æ¨¡å‹ï¼ˆ{self.n_topics}ä¸ªä¸»é¢˜ï¼‰...")
            
            # æ–‡æœ¬é¢„å¤„ç†
            print("ğŸ“ æ–‡æœ¬é¢„å¤„ç†...")
            processed_texts = [self.preprocess_text(text) for text in texts]
            
            # åˆ†è¯
            print("ğŸ”¤ åˆ†è¯å¤„ç†...")
            if self.language == 'chinese':
                tokenized_texts = [self.tokenize_chinese(text) for text in processed_texts]
            elif self.language == 'english':
                tokenized_texts = [self.tokenize_english(text) for text in processed_texts]
            else:  # mixed
                tokenized_texts = [self.tokenize_mixed(text) for text in processed_texts]
            
            # è¿‡æ»¤ç©ºæ–‡æ¡£
            tokenized_texts = [tokens for tokens in tokenized_texts if tokens]
            
            if len(tokenized_texts) < self.n_topics * 2:
                print(f"âš ï¸  è­¦å‘Šï¼šæ–‡æ¡£æ•°é‡ä¸è¶³ï¼ˆ{len(tokenized_texts)}ï¼‰ï¼Œå»ºè®®è‡³å°‘{self.n_topics * 2}æ¡")
            
            # å°†è¯åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            text_strings = [' '.join(tokens) for tokens in tokenized_texts]
            
            # åˆ›å»ºæ–‡æ¡£-è¯çŸ©é˜µ
            print("ğŸ”¢ åˆ›å»ºæ–‡æ¡£-è¯çŸ©é˜µ...")
            self.vectorizer = CountVectorizer(
                max_features=max_features,
                max_df=max_df,
                min_df=min_df
            )
            doc_term_matrix = self.vectorizer.fit_transform(text_strings)
            
            # è®­ç»ƒLDAæ¨¡å‹
            print("ğŸ¤– è®­ç»ƒLDAæ¨¡å‹...")
            self.lda_model = LatentDirichletAllocation(
                n_components=self.n_topics,
                random_state=42,
                max_iter=20,
                learning_method='batch',
                n_jobs=-1
            )
            self.lda_model.fit(doc_term_matrix)
            
            # ä¿å­˜ç‰¹å¾åç§°
            self.feature_names = self.vectorizer.get_feature_names_out()
            
            print(f"âœ… LDAæ¨¡å‹è®­ç»ƒå®Œæˆï¼")
            print(f"   - ä¸»é¢˜æ•°é‡: {self.n_topics}")
            print(f"   - è¯æ±‡æ•°é‡: {len(self.feature_names)}")
            print(f"   - æ–‡æ¡£æ•°é‡: {len(text_strings)}")
            
            # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯ï¼ˆå·²ç¦ç”¨ä»¥å‡å°‘è¾“å‡ºï¼‰
            # self._print_topic_keywords()
            
            return True
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒLDAæ¨¡å‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _print_topic_keywords(self, n_words=10):
        """æ‰“å°æ¯ä¸ªä¸»é¢˜çš„å…³é”®è¯"""
        print("\nğŸ“Š ä¸»é¢˜å…³é”®è¯åˆ†æï¼š")
        print("=" * 80)
        
        for topic_idx, topic in enumerate(self.lda_model.components_):
            # è·å–æ¯ä¸ªä¸»é¢˜çš„å‰Nä¸ªå…³é”®è¯
            top_words_idx = topic.argsort()[:-n_words - 1:-1]
            top_words = [self.feature_names[i] for i in top_words_idx]
            
            print(f"\nä¸»é¢˜ {topic_idx + 1} ({self.topic_names[topic_idx]}):")
            print(f"   å…³é”®è¯: {', '.join(top_words)}")
        
        print("\n" + "=" * 80)
    
    def get_topic_distribution(self, text):
        """
        è·å–æ–‡æœ¬çš„ä¸»é¢˜åˆ†å¸ƒ
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            np.array: ä¸»é¢˜åˆ†å¸ƒï¼ˆé•¿åº¦ä¸ºn_topicsï¼‰
        """
        if self.lda_model is None or self.vectorizer is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_model()")
            return None
        
        try:
            # é¢„å¤„ç†
            processed_text = self.preprocess_text(text)
            
            # åˆ†è¯
            if self.language == 'chinese':
                tokens = self.tokenize_chinese(processed_text)
            elif self.language == 'english':
                tokens = self.tokenize_english(processed_text)
            else:  # mixed
                tokens = self.tokenize_mixed(processed_text)
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            text_string = ' '.join(tokens)
            
            # è½¬æ¢ä¸ºæ–‡æ¡£-è¯å‘é‡
            doc_vector = self.vectorizer.transform([text_string])
            
            # è·å–ä¸»é¢˜åˆ†å¸ƒ
            topic_dist = self.lda_model.transform(doc_vector)[0]
            
            return topic_dist
            
        except Exception as e:
            print(f"âŒ è·å–ä¸»é¢˜åˆ†å¸ƒå¤±è´¥: {e}")
            return np.zeros(self.n_topics)
    
    def save_model(self, filepath='data/lda_topic_model.pkl'):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            filepath: ä¿å­˜è·¯å¾„
        """
        try:
            model_data = {
                'lda_model': self.lda_model,
                'vectorizer': self.vectorizer,
                'n_topics': self.n_topics,
                'language': self.language,
                'topic_names': self.topic_names,
                'feature_names': self.feature_names,
                'saved_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            with open(filepath, 'wb') as f:
                pickle.dump(model_data, f)
            
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_model(self, filepath='data/lda_topic_model.pkl'):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            filepath: æ¨¡å‹æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åŠ è½½æˆåŠŸ
        """
        try:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
            
            self.lda_model = model_data['lda_model']
            self.vectorizer = model_data['vectorizer']
            self.n_topics = model_data['n_topics']
            self.language = model_data['language']
            self.topic_names = model_data['topic_names']
            self.feature_names = model_data['feature_names']
            
            # è°ƒè¯•ä¿¡æ¯å·²åˆ é™¤ä»¥å‡å°‘è¾“å‡º
            # print(f"âœ… æ¨¡å‹å·²ä» {filepath} åŠ è½½")
            # print(f"   - ä¿å­˜æ—¶é—´: {model_data['saved_at']}")
            # print(f"   - ä¸»é¢˜æ•°é‡: {self.n_topics}")
            
            # æ˜¾ç¤ºä¸»é¢˜å…³é”®è¯ï¼ˆå·²ç¦ç”¨ä»¥å‡å°‘è¾“å‡ºï¼‰
            # self._print_topic_keywords()
            
            return True
            
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def get_stock_topic_features(self, stock_code, df_news=None):
        """
        è·å–è‚¡ç¥¨çš„ä¸»é¢˜ç‰¹å¾
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            df_news: æ–°é—»æ•°æ®ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            dict: ä¸»é¢˜ç‰¹å¾ï¼ˆ10ä¸ªä¸»é¢˜çš„æ¦‚ç‡ï¼‰
        """
        if df_news is None:
            df_news = self.load_news_data()
        
        if df_news is None:
            return {f'Topic_{i+1}': 0.0 for i in range(self.n_topics)}
        
        # ç­›é€‰è¯¥è‚¡ç¥¨çš„æ–°é—»
        stock_news = df_news[df_news['è‚¡ç¥¨ä»£ç '] == stock_code]
        
        if len(stock_news) == 0:
            return {f'Topic_{i+1}': 0.0 for i in range(self.n_topics)}
        
        # è·å–æ‰€æœ‰æ–°é—»çš„ä¸»é¢˜åˆ†å¸ƒ
        topic_distributions = []
        for text in stock_news['æ–‡æœ¬']:
            topic_dist = self.get_topic_distribution(text)
            if topic_dist is not None:
                topic_distributions.append(topic_dist)
        
        if len(topic_distributions) == 0:
            return {f'Topic_{i+1}': 0.0 for i in range(self.n_topics)}
        
        # è®¡ç®—å¹³å‡ä¸»é¢˜åˆ†å¸ƒ
        avg_topic_dist = np.mean(topic_distributions, axis=0)
        
        # è½¬æ¢ä¸ºå­—å…¸
        topic_features = {f'Topic_{i+1}': float(avg_topic_dist[i]) for i in range(self.n_topics)}
        
        return topic_features


def main():
    """ä¸»å‡½æ•°ï¼šè®­ç»ƒLDAä¸»é¢˜æ¨¡å‹"""
    print("=" * 80)
    print("ğŸš€ LDAä¸»é¢˜å»ºæ¨¡è®­ç»ƒ")
    print("=" * 80)
    
    # åˆ›å»ºä¸»é¢˜å»ºæ¨¡å™¨
    topic_modeler = TopicModeler(n_topics=10, language='mixed')
    
    # åŠ è½½æ–°é—»æ•°æ®
    print("\nğŸ“Š åŠ è½½æ–°é—»æ•°æ®...")
    df_news = topic_modeler.load_news_data(days=30)
    
    if df_news is None or len(df_news) == 0:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„æ–°é—»æ•°æ®")
        return
    
    # è®­ç»ƒæ¨¡å‹
    texts = df_news['æ–‡æœ¬'].tolist()
    success = topic_modeler.train_model(texts)
    
    if success:
        # ä¿å­˜æ¨¡å‹
        topic_modeler.save_model()
        
        # æµ‹è¯•ä¸»é¢˜åˆ†å¸ƒ
        print("\nğŸ§ª æµ‹è¯•ä¸»é¢˜åˆ†å¸ƒ...")
        test_text = df_news['æ–‡æœ¬'].iloc[0]
        topic_dist = topic_modeler.get_topic_distribution(test_text)
        print(f"æµ‹è¯•æ–‡æœ¬çš„ä¸»é¢˜åˆ†å¸ƒ:")
        for i, prob in enumerate(topic_dist):
            print(f"   Topic_{i+1}: {prob:.4f}")
        
        # è·å–è‚¡ç¥¨ä¸»é¢˜ç‰¹å¾ç¤ºä¾‹
        print("\nğŸ“Š è·å–è‚¡ç¥¨ä¸»é¢˜ç‰¹å¾ç¤ºä¾‹...")
        stock_code = df_news['è‚¡ç¥¨ä»£ç '].iloc[0]
        topic_features = topic_modeler.get_stock_topic_features(stock_code, df_news)
        print(f"è‚¡ç¥¨ {stock_code} çš„ä¸»é¢˜ç‰¹å¾:")
        for topic, prob in topic_features.items():
            print(f"   {topic}: {prob:.4f}")
    
    print("\n" + "=" * 80)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()