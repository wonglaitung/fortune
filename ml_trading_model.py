#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹ - äºŒåˆ†ç±»æ¨¡å‹é¢„æµ‹æ¬¡æ—¥æ¶¨è·Œ
æ•´åˆæŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢ã€èµ„é‡‘æµå‘ç­‰ç‰¹å¾ï¼Œä½¿ç”¨LightGBMè¿›è¡Œè®­ç»ƒ
"""

import warnings
import os
import sys
import argparse
from datetime import datetime, timedelta
import pickle

warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, log_loss, roc_auc_score
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from tencent_finance import get_hk_stock_data_tencent, get_hsi_data_tencent
from technical_analysis import TechnicalAnalyzer
from fundamental_data import get_comprehensive_fundamental_data
from base.base_model_processor import BaseModelProcessor
from us_market_data import us_market_data

# è‡ªé€‰è‚¡åˆ—è¡¨
WATCHLIST = [
    "0005.HK", "0388.HK", "0700.HK", "0728.HK", "0883.HK",
    "0939.HK", "0941.HK", "0981.HK", "1088.HK", "1138.HK",
    "1211.HK", "1288.HK", "1299.HK", "1330.HK", "1347.HK",
    "1398.HK", "1810.HK", "2269.HK", "2533.HK", "3690.HK",
    "3968.HK", "6682.HK", "9660.HK", "9988.HK"
]

# è‚¡ç¥¨åç§°æ˜ å°„
STOCK_NAMES = {
    "0005.HK": "æ±‡ä¸°é“¶è¡Œ",
    "0388.HK": "é¦™æ¸¯äº¤æ˜“æ‰€",
    "0700.HK": "è…¾è®¯æ§è‚¡",
    "0728.HK": "ä¸­å›½ç”µä¿¡",
    "0883.HK": "ä¸­å›½æµ·æ´‹çŸ³æ²¹",
    "0939.HK": "å»ºè®¾é“¶è¡Œ",
    "0941.HK": "ä¸­å›½ç§»åŠ¨",
    "0981.HK": "ä¸­èŠ¯å›½é™…",
    "1088.HK": "ä¸­å›½ç¥å",
    "1138.HK": "ä¸­è¿œæµ·èƒ½",
    "1288.HK": "å†œä¸šé“¶è¡Œ",
    "1330.HK": "ç»¿è‰²åŠ¨åŠ›ç¯ä¿",
    "1347.HK": "åè™¹åŠå¯¼ä½“",
    "1398.HK": "å·¥å•†é“¶è¡Œ",
    "1810.HK": "å°ç±³é›†å›¢-W",
    "2269.HK": "è¯æ˜ç”Ÿç‰©",
    "2533.HK": "é»‘èŠéº»æ™ºèƒ½",
    "2800.HK": "ç›ˆå¯ŒåŸºé‡‘",
    "3690.HK": "ç¾å›¢-W",
    "3968.HK": "æ‹›å•†é“¶è¡Œ",
    "6682.HK": "ç¬¬å››èŒƒå¼",
    "9660.HK": "åœ°å¹³çº¿æœºå™¨äºº",
    "9988.HK": "é˜¿é‡Œå·´å·´-SW",
    "1211.HK": "æ¯”äºšè¿ªè‚¡ä»½",
    "1299.HK": "å‹é‚¦ä¿é™©"
}


class FeatureEngineer:
    """ç‰¹å¾å·¥ç¨‹ç±»"""

    def __init__(self):
        self.tech_analyzer = TechnicalAnalyzer()

    def calculate_technical_features(self, df):
        """è®¡ç®—æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾"""
        if df.empty or len(df) < 200:
            return df

        # ç§»åŠ¨å¹³å‡çº¿
        df = self.tech_analyzer.calculate_moving_averages(df, periods=[5, 10, 20, 50, 100, 200])

        # RSI
        df = self.tech_analyzer.calculate_rsi(df, period=14)

        # MACD
        df = self.tech_analyzer.calculate_macd(df)

        # å¸ƒæ—å¸¦
        df = self.tech_analyzer.calculate_bollinger_bands(df, period=20, std_dev=2)

        # ATR
        df = self.tech_analyzer.calculate_atr(df, period=14)

        # æˆäº¤é‡æ¯”ç‡
        df['Vol_MA20'] = df['Volume'].rolling(window=20).mean()
        df['Vol_Ratio'] = df['Volume'] / df['Vol_MA20']

        # ä»·æ ¼ä½ç½®ï¼ˆç›¸å¯¹äºå‡çº¿ï¼‰
        df['Price_Ratio_MA5'] = df['Close'] / df['MA5']
        df['Price_Ratio_MA20'] = df['Close'] / df['MA20']
        df['Price_Ratio_MA50'] = df['Close'] / df['MA50']

        # å¸ƒæ—å¸¦ä½ç½®
        df['BB_Position'] = (df['Close'] - df['BB_lower']) / (df['BB_upper'] - df['BB_lower'])

        # æ¶¨è·Œå¹…
        df['Return_1d'] = df['Close'].pct_change()
        df['Return_5d'] = df['Close'].pct_change(5)
        df['Return_10d'] = df['Close'].pct_change(10)
        df['Return_20d'] = df['Close'].pct_change(20)

        return df

    def create_fundamental_features(self, code):
        """åˆ›å»ºåŸºæœ¬é¢ç‰¹å¾"""
        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')
            
            fundamental_data = get_comprehensive_fundamental_data(stock_code)
            if fundamental_data and 'financial_indicator' in fundamental_data:
                fi = fundamental_data['financial_indicator']
                return {
                    'PE': fi.get('å¸‚ç›ˆç‡', np.nan),
                    'PB': fi.get('å¸‚å‡€ç‡', np.nan),
                    'ROE': fi.get('å‡€èµ„äº§æ”¶ç›Šç‡', np.nan) / 100 if fi.get('å‡€èµ„äº§æ”¶ç›Šç‡') else np.nan,
                    'ROA': fi.get('æ€»èµ„äº§æ”¶ç›Šç‡', np.nan) / 100 if fi.get('æ€»èµ„äº§æ”¶ç›Šç‡') else np.nan,
                    'Dividend_Yield': fi.get('è‚¡æ¯ç‡', np.nan) / 100 if fi.get('è‚¡æ¯ç‡') else np.nan,
                    'EPS': fi.get('æ¯è‚¡æ”¶ç›Š', np.nan),
                    'Net_Margin': fi.get('å‡€åˆ©ç‡', np.nan) / 100 if fi.get('å‡€åˆ©ç‡') else np.nan,
                    'Gross_Margin': fi.get('æ¯›åˆ©ç‡', np.nan) / 100 if fi.get('æ¯›åˆ©ç‡') else np.nan
                }
        except Exception as e:
            print(f"è·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥ {code}: {e}")
        return {}

    def create_smart_money_features(self, df):
        """åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾"""
        if df.empty or len(df) < 50:
            return df

        # ä»·æ ¼ç›¸å¯¹ä½ç½®
        df['Price_Pct_20d'] = df['Close'].rolling(window=20).apply(lambda x: (x.iloc[-1] - x.min()) / (x.max() - x.min()))

        # æ”¾é‡ä¸Šæ¶¨ä¿¡å·
        df['Strong_Volume_Up'] = (df['Close'] > df['Open']) & (df['Vol_Ratio'] > 1.5)

        # ç¼©é‡å›è°ƒä¿¡å·
        df['Prev_Close'] = df['Close'].shift(1)
        df['Weak_Volume_Down'] = (df['Close'] < df['Prev_Close']) & (df['Vol_Ratio'] < 1.0) & ((df['Prev_Close'] - df['Close']) / df['Prev_Close'] < 0.02)

        # åŠ¨é‡ä¿¡å·
        df['Momentum_5d'] = df['Close'] / df['Close'].shift(5) - 1
        df['Momentum_10d'] = df['Close'] / df['Close'].shift(10) - 1

        return df

    def create_market_environment_features(self, stock_df, hsi_df, us_market_df=None):
        """åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
        
        Args:
            stock_df: è‚¡ç¥¨æ•°æ®
            hsi_df: æ’ç”ŸæŒ‡æ•°æ•°æ®
            us_market_df: ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆå¯é€‰ï¼‰
        """
        if stock_df.empty or hsi_df.empty:
            return stock_df

        # è®¡ç®—æ’ç”ŸæŒ‡æ•°æ”¶ç›Šç‡
        hsi_df['HSI_Return'] = hsi_df['Close'].pct_change()
        hsi_df['HSI_Return_5d'] = hsi_df['Close'].pct_change(5)

        # åˆå¹¶æ’ç”ŸæŒ‡æ•°æ•°æ®
        stock_df = stock_df.merge(hsi_df[['HSI_Return', 'HSI_Return_5d']], left_index=True, right_index=True, how='left')

        # ç›¸å¯¹è¡¨ç°ï¼ˆç›¸å¯¹äºæ’ç”ŸæŒ‡æ•°ï¼‰
        stock_df['Relative_Return'] = stock_df['Return_5d'] - stock_df['HSI_Return_5d']

        # å¦‚æœæä¾›äº†ç¾è‚¡æ•°æ®ï¼Œåˆå¹¶ç¾è‚¡ç‰¹å¾
        if us_market_df is not None and not us_market_df.empty:
            # ç¾è‚¡ç‰¹å¾åˆ—
            us_features = [
                'SP500_Return', 'SP500_Return_5d', 'SP500_Return_20d',
                'NASDAQ_Return', 'NASDAQ_Return_5d', 'NASDAQ_Return_20d',
                'VIX_Change', 'VIX_Ratio_MA20',
                'US_10Y_Yield', 'US_10Y_Yield_Change'
            ]

            # åªåˆå¹¶å­˜åœ¨çš„ç‰¹å¾
            existing_us_features = [f for f in us_features if f in us_market_df.columns]
            if existing_us_features:
                stock_df = stock_df.merge(
                    us_market_df[existing_us_features],
                    left_index=True, right_index=True, how='left'
                )

        return stock_df

    def create_label(self, df, horizon=1):
        """åˆ›å»ºæ ‡ç­¾ï¼šæ¬¡æ—¥æ¶¨è·Œ"""
        if df.empty or len(df) < horizon + 1:
            return df

        # è®¡ç®—æœªæ¥æ”¶ç›Šç‡
        df['Future_Return'] = df['Close'].shift(-horizon) / df['Close'] - 1

        # äºŒåˆ†ç±»æ ‡ç­¾ï¼š1=ä¸Šæ¶¨ï¼Œ0=ä¸‹è·Œ
        df['Label'] = (df['Future_Return'] > 0).astype(int)

        return df


class MLTradingModel:
    """æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹"""

    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.processor = BaseModelProcessor()
        self.model = None
        self.scaler = StandardScaler()
        self.feature_columns = []
        self.horizon = 1  # é»˜è®¤é¢„æµ‹å‘¨æœŸ

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1):
        """å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
        """
        self.horizon = horizon
        all_data = []

        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        print("ğŸ“Š è·å–ç¾è‚¡å¸‚åœºæ•°æ®...")
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            print(f"âœ… æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            print("âš ï¸ æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        for code in codes:
            try:
                print(f"å¤„ç†è‚¡ç¥¨: {code}")

                # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€ï¼Œè…¾è®¯è´¢ç»æ¥å£ä¸éœ€è¦
                stock_code = code.replace('.HK', '')

                # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
                if stock_df is None or stock_df.empty:
                    continue

                # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                hsi_df = get_hsi_data_tencent(period_days=730)
                if hsi_df is None or hsi_df.empty:
                    continue

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df = pd.concat(all_data, ignore_index=True)

        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # è½¬æ¢è¿‡æ»¤æ—¥æœŸä¸º datetime ç±»å‹
        if start_date:
            start_date = pd.to_datetime(start_date)
            df = df[df.index >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date)
            df = df[df.index <= end_date]

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1):
        """è®­ç»ƒæ¨¡å‹
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
        """
        print("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        print(f"ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # æ—¶é—´åºåˆ—åˆ†å‰²
        tscv = TimeSeriesSplit(n_splits=5)

        # è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒLightGBMæ¨¡å‹...")
        self.model = lgb.LGBMClassifier(
            n_estimators=100,
            learning_rate=0.05,
            max_depth=6,
            num_leaves=31,
            min_child_samples=20,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            verbose=-1
        )

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        scores = []
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X[train_idx], X[val_idx]
            y_train, y_val = y[train_idx], y[val_idx]

            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_val)
            score = accuracy_score(y_val, y_pred)
            scores.append(score)
            print(f"éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.model.fit(X, y)

        print(f"\nå¹³å‡éªŒè¯å‡†ç¡®ç‡: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})")

        # ç‰¹å¾é‡è¦æ€§ï¼ˆä½¿ç”¨ BaseModelProcessor ç»Ÿä¸€æ ¼å¼ï¼‰
        feat_imp = self.processor.analyze_feature_importance(
            self.model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘ï¼ˆå¦‚æœå¯èƒ½ï¼‰
        try:
            contrib_values = self.model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )
        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        print("\nç‰¹å¾é‡è¦æ€§ Top 10:")
        print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(10))

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    print(f"âš ï¸ è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—ç‰¹å¾
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # è·å–æœ€æ–°æ•°æ®ï¼ˆæˆ–æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼‰
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            X = latest_data[self.feature_columns].values

            # é¢„æµ‹
            proba = self.model.predict_proba(X)[0]
            prediction = self.model.predict(X)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_columns': self.feature_columns
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_columns = model_data['feature_columns']
        print(f"æ¨¡å‹å·²ä» {filepath} åŠ è½½")


class GBDTLRModel:
    """GBDT + LR ä¸¤é˜¶æ®µæ¨¡å‹ - æé«˜å‡†ç¡®åº¦å’Œå¯è§£é‡Šæ€§"""

    def __init__(self):
        self.feature_engineer = FeatureEngineer()
        self.processor = BaseModelProcessor()
        self.gbdt_model = None
        self.lr_model = None
        self.feature_columns = []
        self.actual_n_estimators = 0
        self.gbdt_leaf_names = []
        self.horizon = 1  # é»˜è®¤é¢„æµ‹å‘¨æœŸ

    def prepare_data(self, codes, start_date=None, end_date=None, horizon=1):
        """å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
        """
        self.horizon = horizon
        all_data = []

        # è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼ˆåªè·å–ä¸€æ¬¡ï¼‰
        print("ğŸ“Š è·å–ç¾è‚¡å¸‚åœºæ•°æ®...")
        us_market_df = us_market_data.get_all_us_market_data(period_days=730)
        if us_market_df is not None:
            print(f"âœ… æˆåŠŸè·å– {len(us_market_df)} å¤©çš„ç¾è‚¡å¸‚åœºæ•°æ®")
        else:
            print("âš ï¸ æ— æ³•è·å–ç¾è‚¡å¸‚åœºæ•°æ®ï¼Œå°†åªä½¿ç”¨æ¸¯è‚¡ç‰¹å¾")

        for code in codes:
            try:
                print(f"å¤„ç†è‚¡ç¥¨: {code}")

                # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€ï¼Œè…¾è®¯è´¢ç»æ¥å£ä¸éœ€è¦
                stock_code = code.replace('.HK', '')

                # è·å–è‚¡ç¥¨æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
                if stock_df is None or stock_df.empty:
                    continue

                # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®ï¼ˆ2å¹´çº¦730å¤©ï¼‰
                hsi_df = get_hsi_data_tencent(period_days=730)
                if hsi_df is None or hsi_df.empty:
                    continue

                # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                stock_df = self.feature_engineer.calculate_technical_features(stock_df)

                # åˆ›å»ºèµ„é‡‘æµå‘ç‰¹å¾
                stock_df = self.feature_engineer.create_smart_money_features(stock_df)

                # åˆ›å»ºå¸‚åœºç¯å¢ƒç‰¹å¾ï¼ˆåŒ…å«æ¸¯è‚¡å’Œç¾è‚¡ï¼‰
                stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

                # åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨æŒ‡å®šçš„ horizonï¼‰
                stock_df = self.feature_engineer.create_label(stock_df, horizon=horizon)

                # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
                fundamental_features = self.feature_engineer.create_fundamental_features(code)
                for key, value in fundamental_features.items():
                    stock_df[key] = value

                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                stock_df['Code'] = code

                all_data.append(stock_df)

            except Exception as e:
                print(f"å¤„ç†è‚¡ç¥¨ {code} å¤±è´¥: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        df = pd.concat(all_data, ignore_index=True)

        # è¿‡æ»¤æ—¥æœŸèŒƒå›´
        # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
        if not isinstance(df.index, pd.DatetimeIndex):
            df.index = pd.to_datetime(df.index)

        # è½¬æ¢è¿‡æ»¤æ—¥æœŸä¸º datetime ç±»å‹
        if start_date:
            start_date = pd.to_datetime(start_date)
            df = df[df.index >= start_date]
        if end_date:
            end_date = pd.to_datetime(end_date)
            df = df[df.index <= end_date]

        return df

    def get_feature_columns(self, df):
        """è·å–ç‰¹å¾åˆ—"""
        # æ’é™¤éç‰¹å¾åˆ—
        exclude_columns = ['Code', 'Open', 'High', 'Low', 'Close', 'Volume',
                          'Future_Return', 'Label', 'Prev_Close',
                          'Vol_MA20', 'MA5', 'MA10', 'MA20', 'MA50', 'MA100', 'MA200',
                          'BB_upper', 'BB_lower', 'BB_middle']

        feature_columns = [col for col in df.columns if col not in exclude_columns]

        return feature_columns

    def train(self, codes, start_date=None, end_date=None, horizon=1):
        """è®­ç»ƒ GBDT + LR æ¨¡å‹
        
        Args:
            codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨
            start_date: è®­ç»ƒå¼€å§‹æ—¥æœŸ
            end_date: è®­ç»ƒç»“æŸæ—¥æœŸ
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
        """
        print("="*70)
        print("ğŸš€ å¼€å§‹è®­ç»ƒ GBDT + LR æ¨¡å‹")
        print("="*70)

        # å‡†å¤‡æ•°æ®
        print("ğŸ“Š å‡†å¤‡è®­ç»ƒæ•°æ®...")
        df = self.prepare_data(codes, start_date, end_date, horizon=horizon)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        df = df.dropna()

        if len(df) < 100:
            raise ValueError(f"æ•°æ®é‡ä¸è¶³ï¼Œåªæœ‰ {len(df)} æ¡è®°å½•")

        # è·å–ç‰¹å¾åˆ—
        self.feature_columns = self.get_feature_columns(df)
        print(f"âœ… ä½¿ç”¨ {len(self.feature_columns)} ä¸ªç‰¹å¾")

        # å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
        X = df[self.feature_columns].values
        y = df['Label'].values

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs('output', exist_ok=True)

        # ========== Step 1: è®­ç»ƒ GBDT ==========
        print("\n" + "="*70)
        print("ğŸŒ² Step 1: è®­ç»ƒ GBDT æ¨¡å‹ï¼ˆç‰¹å¾å·¥ç¨‹ï¼‰")
        print("="*70)

        n_estimators = 32
        num_leaves = 64

        self.gbdt_model = lgb.LGBMClassifier(
            objective='binary',
            boosting_type='gbdt',
            subsample=0.8,
            min_child_weight=0.1,
            min_child_samples=10,
            colsample_bytree=0.7,
            num_leaves=num_leaves,
            learning_rate=0.05,
            n_estimators=n_estimators,
            random_state=2020,
            n_jobs=-1,
            verbose=-1
        )

        # ä½¿ç”¨æ—¶é—´åºåˆ—äº¤å‰éªŒè¯
        tscv = TimeSeriesSplit(n_splits=5)
        gbdt_scores = []

        for fold, (train_idx, val_idx) in enumerate(tscv.split(X), 1):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            self.gbdt_model.fit(
                X_train_fold, y_train_fold,
                eval_set=[(X_val_fold, y_val_fold)],
                eval_metric='binary_logloss',
                callbacks=[
                    lgb.early_stopping(stopping_rounds=5, verbose=False)
                ]
            )

            y_pred_fold = self.gbdt_model.predict(X_val_fold)
            score = accuracy_score(y_val_fold, y_pred_fold)
            gbdt_scores.append(score)
            print(f"   Fold {fold} éªŒè¯å‡†ç¡®ç‡: {score:.4f}")

        # ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒ
        self.gbdt_model.fit(X, y)

        # è·å–å®é™…è®­ç»ƒçš„æ ‘æ•°é‡
        # æ³¨æ„ï¼šåœ¨ä½¿ç”¨å…¨éƒ¨æ•°æ®é‡æ–°è®­ç»ƒæ—¶ï¼Œå¦‚æœæ²¡æœ‰ä½¿ç”¨æ—©åœï¼Œbest_iteration_ å¯èƒ½ä¸º None
        # è¿™ç§æƒ…å†µä¸‹ä½¿ç”¨ n_estimators
        self.actual_n_estimators = self.gbdt_model.best_iteration_ if self.gbdt_model.best_iteration_ else n_estimators
        print(f"\nâœ… GBDT è®­ç»ƒå®Œæˆ")
        print(f"   å®é™…è®­ç»ƒæ ‘æ•°é‡: {self.actual_n_estimators} (åŸè®¡åˆ’: {n_estimators})")
        print(f"   å¹³å‡éªŒè¯å‡†ç¡®ç‡: {np.mean(gbdt_scores):.4f} (+/- {np.std(gbdt_scores):.4f})")

        # ========== Step 2: è¾“å‡º GBDT ç‰¹å¾é‡è¦æ€§ ==========
        print("\n" + "="*70)
        print("ğŸ“Š Step 2: åˆ†æ GBDT ç‰¹å¾é‡è¦æ€§")
        print("="*70)

        feat_imp = self.processor.analyze_feature_importance(
            self.gbdt_model.booster_,
            self.feature_columns
        )

        # è®¡ç®—ç‰¹å¾å½±å“æ–¹å‘
        try:
            contrib_values = self.gbdt_model.booster_.predict(X, pred_contrib=True)
            mean_contrib_values = np.mean(contrib_values[:, :-1], axis=0)
            feat_imp['Mean_Contrib_Value'] = mean_contrib_values
            feat_imp['Impact_Direction'] = feat_imp['Mean_Contrib_Value'].apply(
                lambda x: 'Positive' if x > 0 else 'Negative'
            )

            # ä¿å­˜ç‰¹å¾é‡è¦æ€§
            feat_imp.to_csv('output/gbdt_feature_importance.csv', index=False)
            print("âœ… å·²ä¿å­˜ç‰¹å¾é‡è¦æ€§è‡³ output/gbdt_feature_importance.csv")

            # æ˜¾ç¤ºå‰20ä¸ªé‡è¦ç‰¹å¾
            print("\nğŸ“Š GBDT Top 20 é‡è¦ç‰¹å¾ (å«å½±å“æ–¹å‘):")
            print(feat_imp[['Feature', 'Gain_Importance', 'Impact_Direction']].head(20))

        except Exception as e:
            print(f"âš ï¸ ç‰¹å¾è´¡çŒ®åˆ†æå¤±è´¥: {e}")
            feat_imp['Impact_Direction'] = 'Unknown'

        # ========== Step 3: è·å–å¶å­èŠ‚ç‚¹ç´¢å¼• ==========
        print("\n" + "="*70)
        print("ğŸƒ Step 3: ç”Ÿæˆå¶å­èŠ‚ç‚¹ç‰¹å¾")
        print("="*70)

        gbdt_leaf_features = self.gbdt_model.booster_.predict(X, pred_leaf=True)

        # è·å–å®é™…çš„æ ‘æ•°é‡ï¼ˆåŸºäºå¶å­èŠ‚ç‚¹ç‰¹å¾çš„å®é™…å½¢çŠ¶ï¼‰
        actual_trees = gbdt_leaf_features.shape[1]
        print(f"   å®é™…å¶å­èŠ‚ç‚¹ç‰¹å¾æ•°é‡: {actual_trees}")

        # ç”Ÿæˆå¶å­èŠ‚ç‚¹ç‰¹å¾åç§°
        self.gbdt_leaf_names = [f'gbdt_leaf_{i}' for i in range(actual_trees)]
        df_gbdt_leaf = pd.DataFrame(gbdt_leaf_features, columns=self.gbdt_leaf_names)

        # ========== Step 4: å¯¹å¶å­èŠ‚ç‚¹åš One-Hot ç¼–ç  ==========
        print("   å¯¹å¶å­èŠ‚ç‚¹è¿›è¡Œ One-Hot ç¼–ç ...")
        df_gbdt_onehot = pd.DataFrame()

        for col in self.gbdt_leaf_names:
            onehot_feats = pd.get_dummies(df_gbdt_leaf[col], prefix=col)
            df_gbdt_onehot = pd.concat([df_gbdt_onehot, onehot_feats], axis=1)

        print(f"   ç”Ÿæˆäº† {df_gbdt_onehot.shape[1]} ä¸ªå¶å­èŠ‚ç‚¹ç‰¹å¾")

        # ========== Step 5: è®­ç»ƒ LR æ¨¡å‹ ==========
        print("\n" + "="*70)
        print("ğŸ“ˆ Step 5: è®­ç»ƒ LR æ¨¡å‹ï¼ˆæœ€ç»ˆåˆ†ç±»å™¨ï¼‰")
        print("="*70)

        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        X_train_lr, X_val_lr, y_train_lr, y_val_lr = train_test_split(
            df_gbdt_onehot, y, test_size=0.2, random_state=2020, stratify=y
        )

        self.lr_model = LogisticRegression(
            penalty='l2',
            C=0.1,
            solver='liblinear',
            random_state=2020,
            max_iter=1000
        )
        self.lr_model.fit(X_train_lr, y_train_lr)

        # è¯„ä¼°
        tr_pred_prob = self.lr_model.predict_proba(X_train_lr)[:, 1]
        val_pred_prob = self.lr_model.predict_proba(X_val_lr)[:, 1]

        tr_logloss = log_loss(y_train_lr, tr_pred_prob)
        val_logloss = log_loss(y_val_lr, val_pred_prob)

        tr_ks = self.processor.calculate_ks_statistic(y_train_lr, tr_pred_prob)
        val_ks = self.processor.calculate_ks_statistic(y_val_lr, val_pred_prob)

        tr_auc = roc_auc_score(y_train_lr, tr_pred_prob)
        val_auc = roc_auc_score(y_val_lr, val_pred_prob)

        print(f"\nâœ… LR è®­ç»ƒå®Œæˆ")
        print(f"   Train LogLoss: {tr_logloss:.4f}")
        print(f"   Val LogLoss: {val_logloss:.4f}")
        print(f"   Train KS: {tr_ks:.4f}")
        print(f"   Val KS: {val_ks:.4f}")
        print(f"   Train AUC: {tr_auc:.4f}")
        print(f"   Val AUC: {val_auc:.4f}")

        # ç»˜åˆ¶ ROC æ›²çº¿
        self.processor.plot_roc_curve(y_val_lr, val_pred_prob, "output/roc_curve.png")

        # ========== Step 6: è¾“å‡º LR ç³»æ•° ==========
        print("\n" + "="*70)
        print("ğŸ” Step 6: åˆ†æ LR ç³»æ•°")
        print("="*70)

        lr_coef = pd.DataFrame({
            'Leaf_Feature': X_train_lr.columns,
            'Coefficient': self.lr_model.coef_[0]
        }).sort_values('Coefficient', key=abs, ascending=False)

        lr_coef.to_csv('output/lr_leaf_coefficients.csv', index=False)
        print("âœ… å·²ä¿å­˜ LR ç³»æ•°è‡³ output/lr_leaf_coefficients.csv")

        print("\nğŸ“Š LR Top 10 é‡è¦å¶å­ç‰¹å¾ï¼ˆæŒ‰ç³»æ•°ç»å¯¹å€¼æ’åºï¼‰:")
        print(lr_coef.head(10))

        # ========== Step 7: è§£æé«˜æƒé‡å¶å­è§„åˆ™ ==========
        print("\n" + "="*70)
        print("ğŸ§  Step 7: è§£æé«˜æƒé‡å¶å­èŠ‚ç‚¹è§„åˆ™")
        print("="*70)

        top_leaves = lr_coef.head(5)

        for idx, row in top_leaves.iterrows():
            leaf_feat = row['Leaf_Feature']
            coef = row['Coefficient']

            if leaf_feat.startswith('gbdt_leaf_'):
                parts = leaf_feat.split('_')
                if len(parts) >= 4:
                    tree_idx = int(parts[2])
                    leaf_idx = int(parts[3])

                    print(f"\nğŸ” è§£æ {leaf_feat} (LRç³»æ•°: {coef:.4f})")
                    try:
                        rule = self.processor.get_leaf_path_enhanced(
                            self.gbdt_model.booster_,
                            tree_index=tree_idx,
                            leaf_index=leaf_idx,
                            feature_names=self.feature_columns
                        )
                        if rule:
                            for i, r in enumerate(rule, 1):
                                print(f"   {i}. {r}")
                        else:
                            print("   âš ï¸ è·¯å¾„æœªæ‰¾åˆ°")
                    except Exception as e:
                        print(f"   âš ï¸ è§£æå¤±è´¥: {e}")

        print("\n" + "="*70)
        print("âœ… GBDT + LR æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*70)
        print("ğŸ“Š æ‰€æœ‰å¯è§£é‡Šæ€§æŠ¥å‘Šå·²ç”Ÿæˆåœ¨ output/ ç›®å½•ä¸‹ï¼š")
        print("   - gbdt_feature_importance.csv")
        print("   - lr_leaf_coefficients.csv")
        print("   - roc_curve.png")

        return feat_imp

    def predict(self, code, predict_date=None, horizon=None):
        """é¢„æµ‹å•åªè‚¡ç¥¨

        Args:
            code: è‚¡ç¥¨ä»£ç 
            predict_date: é¢„æµ‹æ—¥æœŸ (YYYY-MM-DD)ï¼ŒåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥
            horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰ï¼Œé»˜è®¤ä½¿ç”¨è®­ç»ƒæ—¶çš„å‘¨æœŸ
        """
        if horizon is None:
            horizon = self.horizon

        try:
            # ç§»é™¤ä»£ç ä¸­çš„.HKåç¼€
            stock_code = code.replace('.HK', '')

            # è·å–è‚¡ç¥¨æ•°æ®
            stock_df = get_hk_stock_data_tencent(stock_code, period_days=730)
            if stock_df is None or stock_df.empty:
                return None

            # è·å–æ’ç”ŸæŒ‡æ•°æ•°æ®
            hsi_df = get_hsi_data_tencent(period_days=730)
            if hsi_df is None or hsi_df.empty:
                return None

            # è·å–ç¾è‚¡å¸‚åœºæ•°æ®
            us_market_df = us_market_data.get_all_us_market_data(period_days=730)

            # å¦‚æœæŒ‡å®šäº†é¢„æµ‹æ—¥æœŸï¼Œè¿‡æ»¤æ•°æ®åˆ°è¯¥æ—¥æœŸ
            if predict_date:
                predict_date = pd.to_datetime(predict_date)
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼è¿›è¡Œæ¯”è¾ƒ
                predict_date_str = predict_date.strftime('%Y-%m-%d')

                # ç¡®ä¿ç´¢å¼•æ˜¯ datetime ç±»å‹
                if not isinstance(stock_df.index, pd.DatetimeIndex):
                    stock_df.index = pd.to_datetime(stock_df.index)
                if not isinstance(hsi_df.index, pd.DatetimeIndex):
                    hsi_df.index = pd.to_datetime(hsi_df.index)
                if us_market_df is not None and not isinstance(us_market_df.index, pd.DatetimeIndex):
                    us_market_df.index = pd.to_datetime(us_market_df.index)

                # ä½¿ç”¨å­—ç¬¦ä¸²æ¯”è¾ƒé¿å…æ—¶åŒºé—®é¢˜
                stock_df = stock_df[stock_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                hsi_df = hsi_df[hsi_df.index.strftime('%Y-%m-%d') <= predict_date_str]
                if us_market_df is not None:
                    us_market_df = us_market_df[us_market_df.index.strftime('%Y-%m-%d') <= predict_date_str]

                if stock_df.empty:
                    print(f"âš ï¸ è‚¡ç¥¨ {code} åœ¨æ—¥æœŸ {predict_date_str} ä¹‹å‰æ²¡æœ‰æ•°æ®")
                    return None

            # è®¡ç®—ç‰¹å¾
            stock_df = self.feature_engineer.calculate_technical_features(stock_df)
            stock_df = self.feature_engineer.create_smart_money_features(stock_df)
            stock_df = self.feature_engineer.create_market_environment_features(stock_df, hsi_df, us_market_df)

            # æ·»åŠ åŸºæœ¬é¢ç‰¹å¾
            fundamental_features = self.feature_engineer.create_fundamental_features(code)
            for key, value in fundamental_features.items():
                stock_df[key] = value

            # è·å–æœ€æ–°æ•°æ®
            latest_data = stock_df.iloc[-1:]

            # å‡†å¤‡ç‰¹å¾
            if len(self.feature_columns) == 0:
                raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train()æ–¹æ³•")

            X = latest_data[self.feature_columns].values

            # Step 1: ä½¿ç”¨ GBDT è·å–å¶å­èŠ‚ç‚¹
            gbdt_leaf = self.gbdt_model.booster_.predict(X, pred_leaf=True)[0]
            df_gbdt_leaf = pd.DataFrame([gbdt_leaf], columns=self.gbdt_leaf_names)

            # Step 2: One-Hot ç¼–ç 
            df_gbdt_onehot = pd.DataFrame()
            for col in self.gbdt_leaf_names:
                onehot_feats = pd.get_dummies(df_gbdt_leaf[col], prefix=col)
                df_gbdt_onehot = pd.concat([df_gbdt_onehot, onehot_feats], axis=1)

            # ç¡®ä¿ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶ä¸€è‡´
            for col in self.lr_model.feature_names_in_:
                if col not in df_gbdt_onehot.columns:
                    df_gbdt_onehot[col] = 0

            df_gbdt_onehot = df_gbdt_onehot[self.lr_model.feature_names_in_]

            # Step 3: ä½¿ç”¨ LR é¢„æµ‹
            proba = self.lr_model.predict_proba(df_gbdt_onehot)[0]
            prediction = self.lr_model.predict(df_gbdt_onehot)[0]

            return {
                'code': code,
                'name': STOCK_NAMES.get(code, code),
                'prediction': int(prediction),
                'probability': float(proba[1]),
                'current_price': float(latest_data['Close'].values[0]),
                'date': latest_data.index[0]
            }

        except Exception as e:
            print(f"é¢„æµ‹å¤±è´¥ {code}: {e}")
            import traceback
            traceback.print_exc()
            return None

    def save_model(self, filepath):
        """ä¿å­˜æ¨¡å‹"""
        model_data = {
            'gbdt_model': self.gbdt_model,
            'lr_model': self.lr_model,
            'feature_columns': self.feature_columns,
            'actual_n_estimators': self.actual_n_estimators,
            'gbdt_leaf_names': self.gbdt_leaf_names
        }
        with open(filepath, 'wb') as f:
            pickle.dump(model_data, f)
        print(f"GBDT + LR æ¨¡å‹å·²ä¿å­˜åˆ° {filepath}")

    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        with open(filepath, 'rb') as f:
            model_data = pickle.load(f)
        self.gbdt_model = model_data['gbdt_model']
        self.lr_model = model_data['lr_model']
        self.feature_columns = model_data['feature_columns']
        self.actual_n_estimators = model_data['actual_n_estimators']
        self.gbdt_leaf_names = model_data['gbdt_leaf_names']
        print(f"GBDT + LR æ¨¡å‹å·²ä» {filepath} åŠ è½½")


def main():
    parser = argparse.ArgumentParser(description='æœºå™¨å­¦ä¹ äº¤æ˜“æ¨¡å‹')
    parser.add_argument('--mode', type=str, default='train', choices=['train', 'predict', 'evaluate'],
                       help='è¿è¡Œæ¨¡å¼: train=è®­ç»ƒ, predict=é¢„æµ‹, evaluate=è¯„ä¼°')
    parser.add_argument('--model-type', type=str, default='both', choices=['lgbm', 'gbdt_lr', 'both'],
                       help='æ¨¡å‹ç±»å‹: lgbm=å•ä¸€LightGBMæ¨¡å‹, gbdt_lr=GBDT+LRä¸¤é˜¶æ®µæ¨¡å‹, both=åŒæ—¶è®­ç»ƒä¸¤ç§æ¨¡å‹ï¼ˆé»˜è®¤ï¼‰')
    parser.add_argument('--model-path', type=str, default='data/ml_trading_model.pkl',
                       help='æ¨¡å‹ä¿å­˜/åŠ è½½è·¯å¾„')
    parser.add_argument('--start-date', type=str, default=None,
                       help='è®­ç»ƒå¼€å§‹æ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='è®­ç»ƒç»“æŸæ—¥æœŸ (YYYY-MM-DD)')
    parser.add_argument('--predict-date', type=str, default=None,
                       help='é¢„æµ‹æ—¥æœŸï¼šåŸºäºè¯¥æ—¥æœŸçš„æ•°æ®é¢„æµ‹ä¸‹ä¸€ä¸ªäº¤æ˜“æ—¥ (YYYY-MM-DD)ï¼Œé»˜è®¤ä½¿ç”¨æœ€æ–°äº¤æ˜“æ—¥')
    parser.add_argument('--horizon', type=int, default=1, choices=[1, 5, 20],
                       help='é¢„æµ‹å‘¨æœŸ: 1=æ¬¡æ—¥ï¼ˆé»˜è®¤ï¼‰, 5=ä¸€å‘¨, 20=ä¸€ä¸ªæœˆ')

    args = parser.parse_args()

    # åˆ¤æ–­æ˜¯å¦åŒæ—¶è®­ç»ƒä¸¤ç§æ¨¡å‹
    train_both = args.model_type == 'both'

    if train_both:
        print("=" * 70)
        print("ğŸš€ åŒæ—¶è®­ç»ƒä¸¤ç§æ¨¡å‹è¿›è¡Œå¯¹æ¯”")
        print("=" * 70)
        lgbm_model = MLTradingModel()
        gbdt_lr_model = GBDTLRModel()
    elif args.model_type == 'gbdt_lr':
        print("=" * 70)
        print("ğŸš€ ä½¿ç”¨ GBDT + LR ä¸¤é˜¶æ®µæ¨¡å‹")
        print("=" * 70)
        lgbm_model = None
        gbdt_lr_model = GBDTLRModel()
    else:
        print("=" * 70)
        print("ğŸš€ ä½¿ç”¨å•ä¸€ LightGBM æ¨¡å‹")
        print("=" * 70)
        lgbm_model = MLTradingModel()
        gbdt_lr_model = None

    if args.mode == 'train':
        print("=" * 50)
        print("è®­ç»ƒæ¨¡å¼")
        print("=" * 50)

        if train_both:
            # è®­ç»ƒ LGBM æ¨¡å‹
            print("\n" + "="*70)
            print("ğŸŒ³ è®­ç»ƒ LightGBM æ¨¡å‹")
            print("="*70)
            lgbm_feature_importance = lgbm_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon)
            # æ·»åŠ å‘¨æœŸåç¼€ï¼š_1d, _5d, _20d
            horizon_suffix = f'_{args.horizon}d'
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            lgbm_model.save_model(lgbm_model_path)
            lgbm_importance_path = lgbm_model_path.replace('.pkl', '_importance.csv')
            lgbm_feature_importance.to_csv(lgbm_importance_path, index=False)
            print(f"\nLightGBM æ¨¡å‹å·²ä¿å­˜åˆ° {lgbm_model_path}")
            print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {lgbm_importance_path}")

            # è®­ç»ƒ GBDT + LR æ¨¡å‹
            print("\n" + "="*70)
            print("ğŸŒ² è®­ç»ƒ GBDT + LR æ¨¡å‹")
            print("="*70)
            gbdt_lr_feature_importance = gbdt_lr_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon)
            gbdt_lr_model_path = args.model_path.replace('.pkl', f'_gbdt_lr{horizon_suffix}.pkl')
            gbdt_lr_model.save_model(gbdt_lr_model_path)
            gbdt_lr_importance_path = gbdt_lr_model_path.replace('.pkl', '_importance.csv')
            gbdt_lr_feature_importance.to_csv(gbdt_lr_importance_path, index=False)
            print(f"\nGBDT + LR æ¨¡å‹å·²ä¿å­˜åˆ° {gbdt_lr_model_path}")
            print(f"ç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {gbdt_lr_importance_path}")

            # å¯¹æ¯”ç‰¹å¾é‡è¦æ€§
            print("\n" + "="*70)
            print("ğŸ“Š ç‰¹å¾é‡è¦æ€§å¯¹æ¯”")
            print("="*70)

            # ç¡®ä¿ Impact_Direction åˆ—å­˜åœ¨
            if 'Impact_Direction' not in lgbm_feature_importance.columns:
                lgbm_feature_importance['Impact_Direction'] = 'Unknown'
            if 'Impact_Direction' not in gbdt_lr_feature_importance.columns:
                gbdt_lr_feature_importance['Impact_Direction'] = 'Unknown'

            # åˆå¹¶ç‰¹å¾é‡è¦æ€§
            comparison = lgbm_feature_importance.merge(
                gbdt_lr_feature_importance[['Feature', 'Gain_Importance', 'Impact_Direction']],
                on='Feature',
                suffixes=('_LGBM', '_GBDT_LR')
            )

            # è®¡ç®—é‡è¦æ€§å·®å¼‚ï¼ˆä½¿ç”¨ Gain_Importanceï¼‰
            comparison['Importance_Diff'] = abs(comparison['Gain_Importance_LGBM'] - comparison['Gain_Importance_GBDT_LR'])
            comparison = comparison.sort_values('Importance_Diff', ascending=False)

            print("\nTop 10 ç‰¹å¾é‡è¦æ€§å·®å¼‚:")
            print(comparison[['Feature', 'Gain_Importance_LGBM', 'Gain_Importance_GBDT_LR', 'Impact_Direction_LGBM', 'Impact_Direction_GBDT_LR']].head(10))

        else:
            # è®­ç»ƒå•ä¸ªæ¨¡å‹
            horizon_suffix = f'_{args.horizon}d'
            if lgbm_model:
                feature_importance = lgbm_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon)
                lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
                lgbm_model.save_model(lgbm_model_path)
                importance_path = lgbm_model_path.replace('.pkl', '_importance.csv')
                feature_importance.to_csv(importance_path, index=False)
                print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")
            else:
                feature_importance = gbdt_lr_model.train(WATCHLIST, args.start_date, args.end_date, horizon=args.horizon)
                gbdt_lr_model_path = args.model_path.replace('.pkl', f'_gbdt_lr{horizon_suffix}.pkl')
                gbdt_lr_model.save_model(gbdt_lr_model_path)
                importance_path = gbdt_lr_model_path.replace('.pkl', '_importance.csv')
                feature_importance.to_csv(importance_path, index=False)
                print(f"\nç‰¹å¾é‡è¦æ€§å·²ä¿å­˜åˆ° {importance_path}")

    elif args.mode == 'predict':
        print("=" * 50)
        print("é¢„æµ‹æ¨¡å¼")
        print("=" * 50)

        # è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æŒ‡å®šäº¤æ˜“æ—¥åçš„ç›®æ ‡æ—¥æœŸ
        def get_target_date(date, horizon=1):
            """è®¡ç®—æŒ‡å®šäº¤æ˜“æ—¥åçš„ç›®æ ‡æ—¥æœŸï¼Œè·³è¿‡å‘¨æœ«
            
            Args:
                date: èµ·å§‹æ—¥æœŸ
                horizon: é¢„æµ‹å‘¨æœŸï¼ˆ1=æ¬¡æ—¥ï¼Œ5=ä¸€å‘¨ï¼Œ20=ä¸€ä¸ªæœˆï¼‰
            
            Returns:
                ç›®æ ‡æ—¥æœŸå­—ç¬¦ä¸² (YYYY-MM-DD)
            """
            target_day = date + pd.Timedelta(days=horizon)
            # è·³è¿‡å‘¨æœ«
            while target_day.weekday() >= 5:
                target_day += pd.Timedelta(days=1)
            return target_day.strftime('%Y-%m-%d')

        if train_both:
            # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
            print("\nåŠ è½½æ¨¡å‹...")
            # æ·»åŠ å‘¨æœŸåç¼€ï¼š_1d, _5d, _20d
            horizon_suffix = f'_{args.horizon}d'
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            gbdt_lr_model_path = args.model_path.replace('.pkl', f'_gbdt_lr{horizon_suffix}.pkl')
            
            lgbm_model.load_model(lgbm_model_path)
            gbdt_lr_model.load_model(gbdt_lr_model_path)

            # é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
            print("\nå¼€å§‹é¢„æµ‹...")
            if args.predict_date:
                print(f"åŸºäºæ—¥æœŸ: {args.predict_date}")
            lgbm_predictions = []
            gbdt_lr_predictions = []

            for code in WATCHLIST:
                lgbm_result = lgbm_model.predict(code, predict_date=args.predict_date)
                gbdt_lr_result = gbdt_lr_model.predict(code, predict_date=args.predict_date)
                
                if lgbm_result and gbdt_lr_result:
                    lgbm_predictions.append(lgbm_result)
                    gbdt_lr_predictions.append(gbdt_lr_result)

            # åˆå¹¶é¢„æµ‹ç»“æœ
            lgbm_pred_df = pd.DataFrame(lgbm_predictions)
            gbdt_lr_pred_df = pd.DataFrame(gbdt_lr_predictions)

            # æ·»åŠ æ•°æ®æ—¥æœŸå’Œç›®æ ‡æ—¥æœŸ
            lgbm_pred_df['data_date'] = lgbm_pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
            lgbm_pred_df['target_date'] = lgbm_pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))

            gbdt_lr_pred_df['data_date'] = gbdt_lr_pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
            gbdt_lr_pred_df['target_date'] = gbdt_lr_pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))

            # åˆå¹¶å¯¹æ¯”
            comparison = lgbm_pred_df.merge(
                gbdt_lr_pred_df,
                on='code',
                suffixes=('_LGBM', '_GBDT_LR')
            )

            # è®¡ç®—é¢„æµ‹ä¸€è‡´æ€§
            comparison['é¢„æµ‹ä¸€è‡´'] = comparison['prediction_LGBM'] == comparison['prediction_GBDT_LR']
            comparison['æ¦‚ç‡å·®å¼‚'] = abs(comparison['probability_LGBM'] - comparison['probability_GBDT_LR'])

            # æ˜¾ç¤ºå¯¹æ¯”ç»“æœ
            print("\n" + "=" * 140)
            print("ğŸ“Š ä¸¤ç§æ¨¡å‹é¢„æµ‹ç»“æœå¯¹æ¯”")
            print("=" * 140)
            print(f"\n{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'LGBMé¢„æµ‹':<10} {'LGBMæ¦‚ç‡':<10} {'GBDT+LRé¢„æµ‹':<12} {'GBDT+LRæ¦‚ç‡':<12} {'æ˜¯å¦ä¸€è‡´':<8} {'æ¦‚ç‡å·®å¼‚':<10} {'å½“å‰ä»·æ ¼':<10} {'é¢„æµ‹ç›®æ ‡':<12}")
            print("-" * 140)

            for _, row in comparison.iterrows():
                lgbm_pred_label = "ä¸Šæ¶¨" if row['prediction_LGBM'] == 1 else "ä¸‹è·Œ"
                gbdt_lr_pred_label = "ä¸Šæ¶¨" if row['prediction_GBDT_LR'] == 1 else "ä¸‹è·Œ"
                consistent = "âœ“" if row['é¢„æµ‹ä¸€è‡´'] else "âœ—"

                print(f"{row['code']:<10} {row['name_LGBM']:<12} {lgbm_pred_label:<10} {row['probability_LGBM']:<10.4f} {gbdt_lr_pred_label:<12} {row['probability_GBDT_LR']:<12.4f} {consistent:<8} {row['æ¦‚ç‡å·®å¼‚']:<10.4f} {row['current_price_LGBM']:<10.2f} {row['target_date_LGBM']:<12}")

            # ç»Ÿè®¡æ‘˜è¦
            print("\n" + "=" * 140)
            print("ğŸ“ˆ ç»Ÿè®¡æ‘˜è¦")
            print("=" * 140)

            consistent_count = comparison['é¢„æµ‹ä¸€è‡´'].sum()
            total_count = len(comparison)
            print(f"\né¢„æµ‹ä¸€è‡´æ€§: {consistent_count}/{total_count} ({consistent_count/total_count*100:.1f}%)")

            lgbm_up = (comparison['prediction_LGBM'] == 1).sum()
            lgbm_down = (comparison['prediction_LGBM'] == 0).sum()
            print(f"\nLGBM æ¨¡å‹: ä¸Šæ¶¨ {lgbm_up} åª, ä¸‹è·Œ {lgbm_down} åª")

            gbdt_lr_up = (comparison['prediction_GBDT_LR'] == 1).sum()
            gbdt_lr_down = (comparison['prediction_GBDT_LR'] == 0).sum()
            print(f"GBDT+LR æ¨¡å‹: ä¸Šæ¶¨ {gbdt_lr_up} åª, ä¸‹è·Œ {gbdt_lr_down} åª")

            avg_prob_diff = comparison['æ¦‚ç‡å·®å¼‚'].mean()
            print(f"\nå¹³å‡æ¦‚ç‡å·®å¼‚: {avg_prob_diff:.4f}")

            # æ˜¾ç¤ºä¸ä¸€è‡´çš„é¢„æµ‹
            inconsistent = comparison[~comparison['é¢„æµ‹ä¸€è‡´']]
            if len(inconsistent) > 0:
                print("\n" + "=" * 140)
                print("âš ï¸  é¢„æµ‹ä¸ä¸€è‡´çš„è‚¡ç¥¨")
                print("=" * 140)
                for _, row in inconsistent.iterrows():
                    lgbm_pred_label = "ä¸Šæ¶¨" if row['prediction_LGBM'] == 1 else "ä¸‹è·Œ"
                    gbdt_lr_pred_label = "ä¸Šæ¶¨" if row['prediction_GBDT_LR'] == 1 else "ä¸‹è·Œ"
                    print(f"{row['code']:<10} {row['name_LGBM']:<12} LGBM: {lgbm_pred_label} ({row['probability_LGBM']:.4f})  vs  GBDT+LR: {gbdt_lr_pred_label} ({row['probability_GBDT_LR']:.4f})")

            # ä¿å­˜å¯¹æ¯”ç»“æœ
            comparison_export = comparison[[
                'code', 'name_LGBM', 'prediction_LGBM', 'probability_LGBM',
                'prediction_GBDT_LR', 'probability_GBDT_LR', 'é¢„æµ‹ä¸€è‡´', 'æ¦‚ç‡å·®å¼‚',
                'current_price_LGBM', 'data_date_LGBM', 'target_date_LGBM'
            ]]
            comparison_export.columns = [
                'code', 'name', 'prediction_LGBM', 'probability_LGBM',
                'prediction_GBDT_LR', 'probability_GBDT_LR', 'consistent', 'probability_diff',
                'current_price', 'data_date', 'target_date'
            ]
            
            comparison_path = args.model_path.replace('.pkl', '_comparison.csv')
            comparison_export.to_csv(comparison_path, index=False)
            print(f"\nå¯¹æ¯”ç»“æœå·²ä¿å­˜åˆ° {comparison_path}")

            # ä¿å­˜å„è‡ªçš„é¢„æµ‹ç»“æœ
            horizon_suffix = f'_{args.horizon}d'
            lgbm_pred_path = args.model_path.replace('.pkl', f'_lgbm_predictions{horizon_suffix}.csv')
            lgbm_pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']].to_csv(lgbm_pred_path, index=False)
            print(f"LGBM é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {lgbm_pred_path}")

            gbdt_lr_pred_path = args.model_path.replace('.pkl', f'_gbdt_lr_predictions{horizon_suffix}.csv')
            gbdt_lr_pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']].to_csv(gbdt_lr_pred_path, index=False)
            print(f"GBDT+LR é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {gbdt_lr_pred_path}")

        else:
            # å•ä¸ªæ¨¡å‹é¢„æµ‹
            model = lgbm_model if lgbm_model else gbdt_lr_model
            horizon_suffix = f'_{args.horizon}d'
            if lgbm_model:
                model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            else:
                model_path = args.model_path.replace('.pkl', f'_gbdt_lr{horizon_suffix}.pkl')
            model.load_model(model_path)

            # é¢„æµ‹æ‰€æœ‰è‚¡ç¥¨
            predictions = []
            if args.predict_date:
                print(f"åŸºäºæ—¥æœŸ: {args.predict_date}")
            for code in WATCHLIST:
                result = model.predict(code, predict_date=args.predict_date)
                if result:
                    predictions.append(result)

            # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
            print("\né¢„æµ‹ç»“æœ:")
            horizon_text = {1: "æ¬¡æ—¥", 5: "ä¸€å‘¨", 20: "ä¸€ä¸ªæœˆ"}.get(args.horizon, f"{args.horizon}å¤©")
            if args.predict_date:
                print(f"è¯´æ˜: åŸºäº {args.predict_date} çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
            else:
                print(f"è¯´æ˜: åŸºäºæœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®é¢„æµ‹{horizon_text}åçš„æ¶¨è·Œ")
            print("-" * 100)
            print(f"{'ä»£ç ':<10} {'è‚¡ç¥¨åç§°':<12} {'é¢„æµ‹':<8} {'æ¦‚ç‡':<10} {'å½“å‰ä»·æ ¼':<12} {'æ•°æ®æ—¥æœŸ':<15} {'é¢„æµ‹ç›®æ ‡':<15}")
            print("-" * 100)

            for pred in predictions:
                pred_label = "ä¸Šæ¶¨" if pred['prediction'] == 1 else "ä¸‹è·Œ"
                data_date = pred['date'].strftime('%Y-%m-%d')
                target_date = get_target_date(pred['date'], horizon=args.horizon)

                print(f"{pred['code']:<10} {pred['name']:<12} {pred_label:<8} {pred['probability']:.4f}    {pred['current_price']:.2f}        {data_date:<15} {target_date:<15}")

            # ä¿å­˜é¢„æµ‹ç»“æœ
            pred_df = pd.DataFrame(predictions)
            pred_df['data_date'] = pred_df['date'].apply(lambda x: x.strftime('%Y-%m-%d'))
            pred_df['target_date'] = pred_df['date'].apply(lambda x: get_target_date(x, horizon=args.horizon))
            
            pred_df_export = pred_df[['code', 'name', 'prediction', 'probability', 'current_price', 'data_date', 'target_date']]
            
            horizon_suffix = f'_{args.horizon}d'
            pred_path = args.model_path.replace('.pkl', f'_predictions{horizon_suffix}.csv')
            pred_df_export.to_csv(pred_path, index=False)
            print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° {pred_path}")

    elif args.mode == 'evaluate':
        print("=" * 50)
        print("è¯„ä¼°æ¨¡å¼")
        print("=" * 50)

        if train_both:
            # åŠ è½½ä¸¤ä¸ªæ¨¡å‹
            print("\nåŠ è½½æ¨¡å‹...")
            horizon_suffix = f'_{args.horizon}d'
            lgbm_model_path = args.model_path.replace('.pkl', f'_lgbm{horizon_suffix}.pkl')
            gbdt_lr_model_path = args.model_path.replace('.pkl', f'_gbdt_lr{horizon_suffix}.pkl')
            
            lgbm_model.load_model(lgbm_model_path)
            gbdt_lr_model.load_model(gbdt_lr_model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = lgbm_model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[lgbm_model.feature_columns].values
            y_test = test_df['Label'].values

            # LGBM æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ³ LightGBM æ¨¡å‹è¯„ä¼°")
            print("="*70)
            y_pred_lgbm = lgbm_model.model.predict(X_test)
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_lgbm))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_lgbm))
            lgbm_accuracy = accuracy_score(y_test, y_pred_lgbm)
            print(f"\nå‡†ç¡®ç‡: {lgbm_accuracy:.4f}")

            # GBDT + LR æ¨¡å‹è¯„ä¼°
            print("\n" + "="*70)
            print("ğŸŒ² GBDT + LR æ¨¡å‹è¯„ä¼°")
            print("="*70)
            gbdt_leaf_test = gbdt_lr_model.gbdt_model.booster_.predict(X_test, pred_leaf=True)
            df_gbdt_leaf_test = pd.DataFrame(gbdt_leaf_test, columns=gbdt_lr_model.gbdt_leaf_names)

            df_gbdt_onehot_test = pd.DataFrame()
            for col in gbdt_lr_model.gbdt_leaf_names:
                onehot_feats = pd.get_dummies(df_gbdt_leaf_test[col], prefix=col)
                df_gbdt_onehot_test = pd.concat([df_gbdt_onehot_test, onehot_feats], axis=1)

            for col in gbdt_lr_model.lr_model.feature_names_in_:
                if col not in df_gbdt_onehot_test.columns:
                    df_gbdt_onehot_test[col] = 0

            df_gbdt_onehot_test = df_gbdt_onehot_test[gbdt_lr_model.lr_model.feature_names_in_]
            y_pred_gbdt_lr = gbdt_lr_model.lr_model.predict(df_gbdt_onehot_test)

            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred_gbdt_lr))
            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred_gbdt_lr))
            gbdt_lr_accuracy = accuracy_score(y_test, y_pred_gbdt_lr)
            print(f"\nå‡†ç¡®ç‡: {gbdt_lr_accuracy:.4f}")

            # å¯¹æ¯”ç»“æœ
            print("\n" + "="*70)
            print("ğŸ“Š æ¨¡å‹å¯¹æ¯”")
            print("="*70)
            print(f"LightGBM å‡†ç¡®ç‡: {lgbm_accuracy:.4f}")
            print(f"GBDT + LR å‡†ç¡®ç‡: {gbdt_lr_accuracy:.4f}")
            print(f"å‡†ç¡®ç‡å·®å¼‚: {abs(lgbm_accuracy - gbdt_lr_accuracy):.4f}")
            
            if gbdt_lr_accuracy > lgbm_accuracy:
                print(f"\nâœ… GBDT + LR æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {gbdt_lr_accuracy - lgbm_accuracy:.4f} ({(gbdt_lr_accuracy - lgbm_accuracy)/lgbm_accuracy*100:.2f}%)")
            elif lgbm_accuracy > gbdt_lr_accuracy:
                print(f"\nâœ… LightGBM æ¨¡å‹è¡¨ç°æ›´å¥½ï¼Œæå‡ {lgbm_accuracy - gbdt_lr_accuracy:.4f} ({(lgbm_accuracy - gbdt_lr_accuracy)/gbdt_lr_accuracy*100:.2f}%)")
            else:
                print(f"\nâš–ï¸  ä¸¤ç§æ¨¡å‹è¡¨ç°ç›¸åŒ")

        else:
            # å•ä¸ªæ¨¡å‹è¯„ä¼°
            model = lgbm_model if lgbm_model else gbdt_lr_model
            model.load_model(args.model_path)

            # å‡†å¤‡æµ‹è¯•æ•°æ®
            print("å‡†å¤‡æµ‹è¯•æ•°æ®...")
            test_df = model.prepare_data(WATCHLIST)
            test_df = test_df.dropna()

            X_test = test_df[model.feature_columns].values
            y_test = test_df['Label'].values

            # æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œé¢„æµ‹
            if gbdt_lr_model:
                # GBDT + LR æ¨¡å‹éœ€è¦å…ˆé€šè¿‡ GBDT è·å–å¶å­èŠ‚ç‚¹ç‰¹å¾
                gbdt_leaf_test = model.gbdt_model.booster_.predict(X_test, pred_leaf=True)
                df_gbdt_leaf_test = pd.DataFrame(gbdt_leaf_test, columns=model.gbdt_leaf_names)

                # One-Hot ç¼–ç 
                df_gbdt_onehot_test = pd.DataFrame()
                for col in model.gbdt_leaf_names:
                    onehot_feats = pd.get_dummies(df_gbdt_leaf_test[col], prefix=col)
                    df_gbdt_onehot_test = pd.concat([df_gbdt_onehot_test, onehot_feats], axis=1)

                # ç¡®ä¿ç‰¹å¾åˆ—ä¸è®­ç»ƒæ—¶ä¸€è‡´
                for col in model.lr_model.feature_names_in_:
                    if col not in df_gbdt_onehot_test.columns:
                        df_gbdt_onehot_test[col] = 0

                df_gbdt_onehot_test = df_gbdt_onehot_test[model.lr_model.feature_names_in_]

                # ä½¿ç”¨ LR é¢„æµ‹
                y_pred = model.lr_model.predict(df_gbdt_onehot_test)
            else:
                # å•ä¸€ LightGBM æ¨¡å‹
                y_pred = model.model.predict(X_test)

            # è¯„ä¼°
            print("\nåˆ†ç±»æŠ¥å‘Š:")
            print(classification_report(y_test, y_pred))

            print("\næ··æ·†çŸ©é˜µ:")
            print(confusion_matrix(y_test, y_pred))

            print(f"\nå‡†ç¡®ç‡: {accuracy_score(y_test, y_pred):.4f}")


if __name__ == '__main__':
    main()
